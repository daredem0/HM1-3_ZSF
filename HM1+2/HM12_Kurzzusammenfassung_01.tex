%%%%%Präambel%%%%%

\documentclass[12pt,a4paper]{report}%Schriftgröße, Papierformat einstellen
%\documentclass{scrbook}
\usepackage[top=30mm,bottom=30mm]{geometry}
\usepackage{lipsum}
\usepackage{csquotes}
%Pakete laden zur deutschen Rechtschreibung und für Umlaute
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc} %für Windows, Linux
%\usepackage[applemac]{inputenc} %für Mac
%\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[dvipsnames]{xcolor}
\usepackage{cancel}
\usepackage{titlesec}
\usepackage{cite}
\usepackage{filecontents}
\usepackage{tabularx}
\usepackage{harvard}
\usepackage{units}
\usepackage{longtable} 
\usepackage{multirow}
\usepackage{chngcntr}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{autobreak}
\usepackage{booktabs}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{hhline}
\let\harvardleftorig\harvardleft
%\usepackage[round]{natbib}
%\usepackage{hyperref}
\usepackage[nottoc]{tocbibind}

%Pakete laden zu mathematischen Symbolen etc.
\usepackage{calc} 
\usepackage{amsmath,amssymb,amsthm,amsopn}
\usepackage{scrpage2}
\pagestyle{scrheadings}
\clearscrheadfoot
\automark[chapter]{section}
\ofoot{\pagemark}
\ifoot{}
\chead{\headmark}
\setfootsepline{1pt}
\setheadsepline{1pt}
%\setheadsepline[\textwidth+20pt]{0.5pt}

\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\diverg}{div}
\DeclareMathOperator{\rot}{rot}
\DeclareMathOperator{\spur}{spur}
\DeclareMathOperator{\determ}{det}

%Inhaltsverzeichnis mit Links erstellen
\usepackage[colorlinks,
pdfpagelabels,
pdfstartview = FitH,
bookmarksopen = true,
bookmarksnumbered = true,
linkcolor = black,
plainpages = false,
hypertexnames = false,
citecolor = black] {hyperref}

% Umgebungen für Definitionen, Sätze, usw.
\newtheorem{satz}{Satz}[section]
\newtheorem{definition}[satz]{Definition}     
\newtheorem{lemma}[satz]{Lemma}	
\newtheorem{bem}{Bemerkung}[section]
% Es werden Sätze, Definitionen etc innerhalb einer Section mit
% 1.1, 1.2 etc durchnummeriert, ebenso die Gleichungen mit (1.1), (1.2) ..                  
\numberwithin{equation}{section}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

%neue Befehle definieren
\newcommand{\R}{\mathbb{R}} %zB \R als Abkürzung für das Symbol der reellen Zahlen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\diffp}{\partial}
\newcommand{\laplace}{\Delta}

\newcommand{\subsubsubsection}{\paragraph}
\newcommand\citevgl
{\def\harvardleft{(vgl.\ \global\let\harvardleft\harvardleftorig}%
 \cite
}
\newcommand\citeVgl
{\def\harvardleft{(Vgl.\ \global\let\harvardleft\harvardleftorig}%
 \cite
}

\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\def\ccite#1#2{\glqq #1\grqq\cite{#2}}

\newcolumntype{L}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%Makros
%Makro Color
%#1 Text
\def\colBord#1{\begingroup\color{Fuchsia}{#1}\endgroup}
\def\colRed#1{\begingroup\color{Red}{#1}\endgroup}
\def\colGreen#1{\begingroup\color{LimeGreen}{#1}\endgroup}
\def\colBlue#1{\begingroup\color{NavyBlue}{#1}\endgroup}

\def\usGreen#1#2{\underset{\colGreen{#1}}{#2}}
\def\usBord#1#2{\underset{\colBord{#1}}{#2}}

\def\ubGreen#1#2{\underbrace{#2}_{\colGreen{#1}}}

\def\defF{\textbf{Def.: }}
\def\mDef#1{
\begin{definition}
  #1
\end{definition}}

\def\vecT#1{\left(\begin{array}{c} #1 \end{array}\right)}
\def\dddot{\cdot \\ \cdot \\ \cdot}
\def\vecD#1{\vecT{#1_1 \\ \dddot \\ #1_d}}
\def\vecDt#1#2{\vecT{#1 \\ \dddot \\ #2}}
\def\vecN{\mathcal{O}}
\def\vspan#1{span \lbrace #1 \rbrace}
\def\vdim#1{dim \lbrace #1 \rbrace}
\def\vker#1{ker \lbrace #1 \rbrace}
\def\vrang#1{Rang \lbrace #1 \rbrace}
\def\mzxz#1#2#3#4{\left(\begin{array}{c c} #1 & #2 \\ #3 & #4 \\ \end{array}\right)}
\def\mdxd#1#2#3{\left(\begin{array}{c c c} #1 \\ #2 \\ #3 \end{array}\right)}
\def\dfp#1#2{\frac{\partial #1}{\partial #2}}
\def\diff#1#2{\frac{\mathrm{d}#1}{\mathrm{d}#2}}

\def\epsF{\pmb{\varepsilon}}

\def\multiTwo#1#2{\multicolumn{2}{>{\hsize=\dimexpr2\hsize+2\tabcolsep+\arrayrulewidth\relax}#1}{#2}}
\def\multiThree#1#2{\multicolumn{3}{>{\hsize=\dimexpr3\hsize+4\tabcolsep+2\arrayrulewidth\relax}#1}{#2}}

\def\inR#1{\qquad ,\; #1 \in \R}
\def\inRs{\in \R}
\def\bracks#1{\left[ #1 \right]}
\def\abs#1{\left| #1 \right|}
\def\brac#1{\left( #1 \right)}

%laziness
\def\fermi{Fermi-Dirac-Verteilung}

\newcolumntype{L}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}



\def\formTab#1#2{
\begin{equation}
  \begin{tabularx}{12cm}{R{3cm} l l}
    #1 &: &$#2$
  \end{tabularx}
\end{equation}
}
\newcommand{\formTabL}[3]{
\begin{equation}
  \begin{tabularx}{12cm}{R{3cm} l l}
    #1 &: &$#2$ 
  \end{tabularx}
  \label{eq:#3}
\end{equation}}
\def\formTn{$ \\ $\;$ & $\;$ & $}
\def\formTnQ{$ \\ $\;$ & $\;$ & $\qquad}
\def\formTnQQ{$ \\ $\;$ & $\;$ & $\qquad \qquad}
\def\formTnQQQ{$ \\ $\;$ & $\;$ & $\qquad \qquad \qquad}

\renewcommand{\theequation}{\arabic{section}.\arabic{subsection}
.\arabic{equation}}
%Setzt den equation-Zaehler nach jeder Seite zurueck
\numberwithin{equation}{subsection}	


%\setlength\abovedisplayskip{0pt}

% Auf der Seite http://detexify.kirelabs.org/classify.html können Sie mathematische Symbole, Pfeile usw per Maus eingeben und bekommen den Latex-Befehl dafür angezeigt.
% detexify gibt es auch als App...

%jetzt beginnt das eigentliche Dokument
\begin{document}

\bibliographystyle{agsm}

\author{}
\title{\underline{HM1-2 Kurzzusammenfassung} \\ $\;$ \\ $\;$ \\ Florian Leuze}
\date{}

\maketitle % erzeugt den Kopf
\newpage

\tableofcontents

  \subsection*{Versionierung}
  \begin{tabular}{|p{2cm}|p{1cm}|p{1.5cm}|p{8.5cm}|}\hline
    Datum & Vers. & Kürzel & Änderung \\ \hline
    19.04.2018 & 0.1 & FL & Erzeugung Dokument; Erzeugung Inhaltsverzeichnis; Erzeugung Versionierung; Erzeugung 2.1 - 2.7.4 \\ \hline
    19.04.2018 & 0.2 & FL & Korrekturen 2.6.1 - 2.6.9 u. 2.7.1 - 2.7.2 Titel\\ \hline
    20.04.2018 & 0.2.1 & FL & Erzeugung 2.7.1.1 - 2.7.1.4; Korrektur Riemannsche Untersumme; Erzeugung Literaturverzeichnis \\ \hline
    01.05.2018 & 0.2.2 & FL & Neustrukturierung; Erzeugung Allgemeines; Erzeugung Zahlen \\ \hline
    23.06.2018 & 0.2.3 & FL & Neustrukturierung; Löschung HM1 Stoff; Erzeugung HM2 Stoff \\ \hline
    23.06.2018 & 0.2.4 & FL & Kleinere Korrekturen \\ \hline
    27.06.2018 & 0.2.5 & FL & Hinzugefügt: Bem. 2.8, 3.1, 3.2, 3.3, 3.8, 3.9.1 Schritt 5, 3.9.2 (Quadriken im $\R^2$; Korrekturen: 3.6.1.1 Fehler in Formel korrigiert, 3.6.1.3 Tippfehler korrigiert \\ \hline
    27.06.2018 & 0.2.6 & FL &  Bem. 3.6 korrigiert \\ \hline
    01.08.2018 & 0.3.0 & FL & Überarbeitung Trigonometrie \\ \hline
    04.08.2018 & 0.3.1 & FL & Erzeugung lin. skal. Diffgleichungen, LGS \\ \hline
    05.08.2018 & 0.3.2 & FL & Erzeugung Analytische Geometrie; Erzeugung Logik und Beweise \\ \hline
    06.08.2018 & 0.3.3 & FL & Erzeugung Mengen, Relationen und Abbildungen, Erzeugung konvergente Folgen \\ \hline
    07.08.2018 & 0.3.4 & FL & Erzeugung el. realw. Funktionen, Ableitung; Fertigstellung konvergente Folgen \\ \hline
    08.08.2018 & 0.3.5 & FL & Erzeugung Ableitung, Taylorpolynome und Reihen. \\ \hline
    09.08.2018 & 0.4.0 & FL & Umordnung von Reihen, Stetigkeit, Extremalprobleme, Funktionenfolgen; Korrekturen am Layout \\ \hline
    09.08.2018 & 0.4.2 & FL & Korrekturen am Layout \\ \hline
    09.08.2018 & 0.4.3 & FL & Korrektur Def.7.8 \\ \hline
    13.08.2018 & 0.3 & FL & Erzeugung Mehrd. Extremw., Satz über impl. Funkt., Extremwertaufgaben mit Nebenbe., Kurven/Bogenl., Wegintegrale \\ \hline
    13.08.2018 & 0.3.1 & FL & Kleinere Korrekturen \\ \hline
    13.08.2018 & 0.3.2 & FL & Korrektur Layout \\ \hline
   15.12.2018 & 0.3.3 & FL & Kleinere Korrekturen \\ \hline
  \end{tabular}
  
\newpage
\listoffigures

\newpage
\chapter{Allgemeines}
	\section{Trigonometrie}
	  \subsection{Winkelfunktionen}
	  \begin{align}
	    sin(\alpha) = \frac{Gegenkathete}{Hypothenuse}\\
	    cos(\alpha) = \frac{Ankathete}{Hypothenuse}\\
	    tan(\alpha) = \frac{Gegenkathete}{Ankathete} \label{eq:trigo_Winkelf}
	  \end{align}
	  
	  \subsubsection{Wichtige Werte}
	  \renewcommand{\arraystretch}{1.5}
	  \begin{tabular}{|p{3.2cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}\hline
	  $\alpha$ in Gradmaß & $0^{\circ}$ & $30^{\circ}$ & $45^{\circ}$ & $60^{\circ}$ & $90^{\circ}$ \\ \hline
	  $\alpha$ in Bogenmaß & $0$ & $\frac{\Pi}{6}$ & $\frac{\pi}{4}$ & $\frac{\pi}{3}$ & $\frac{\pi}{2}$ \\ \hline
	  $sin\alpha$ & $\frac{1}{2}\sqrt{0}$ & $\frac{1}{2}\sqrt{1}$ & $\frac{1}{2} \sqrt{2}$ & $\frac{1}{2}\sqrt{3}$ & $1$ \\ \hline
	  $cos\alpha$ & $1$ & $\frac{1}{2}\sqrt{3}$ & $\frac{1}{2}\sqrt{2}$ & $\frac{1}{2}\sqrt{1}$ & $\frac{1}{2}\sqrt{0}$ \\ \hline
	  $tan\alpha$ & $0$ & $\frac{1}{3}\sqrt{3}$ & $1$ & $\sqrt{3}$ & n.d. \\ \hline
	  \end{tabular}
	  \renewcommand{\arraystretch}{1}
	  
	  \subsection{Sinussatz}
	  \begin{equation}
	    \frac{a}{sin\alpha} = \frac{b}{sin\beta} = \frac{c}{sin\gamma} = 2r = \frac{abc}{2F} \label{eq:allg_sinussatz}  
	  \end{equation}
	  
	  \subsection{Cosinussatz}
	  \begin{align}   
	    a^2 = b^2 + c^2 - 2bc cos\alpha\\
	    b^2 = c^2 + a^2 - 2ca cos\beta\\
	    c^2 = a^2 + b^2 - 2ab cos\gamma \label{eq:trigo_cosinussatz}
	  \end{align}
	  
	  \subsection{Tangenssatz}
	  \begin{align}
	    \frac{b + c}{b - c} = \frac{tan\left(\frac{\beta + \gamma}{2}\right)}{tan\left(\frac{\beta - \gamma}{2}\right)} 
	    = \frac{cot\left(\frac{\alpha}{2} \right)}{tan\left(\frac{\beta - \gamma}{2}\right)}
	  \end{align}
	  Analog für $\frac{a + b}{a - b}$ und $\frac{a + c}{a - c}$.\label{eq:trigo_tangenssatz}
	  
	  \subsection{Umwandlung}
	  \begin{align}
	    tan\alpha = \frac{sin\alpha}{cos\alpha}\\
	    sin^2(\alpha) + cos^2(\alpha) = 1\\
	    1 + tan^2(\alpha) = \frac{1}{cos^2(\alpha)} = sec^2(\alpha)\\
	    1 + cot^2(\alpha) = \frac{1}{sin^2(\alpha)} = csc^2(\alpha)\label{eq:trigo_umwandlung}
	  \end{align}
	  
	  \subsection{Additionstheoreme}
	  \begin{align}
	    sin(x \pm y) &= sin(x) cos(y) \pm cos(x) sin(y)\\
	    cos(x \pm y) &= cos(x) cos(y) \mp sin(x) sin(y)\\\\
	    tan(x \pm y) &= \frac{tan(x) \pm tan(y)}{1 \mp tan(x) tan(y)} = \frac{sin(x \pm y)}{cos(x \pm y)}\\
	    cot(x \pm y) &= \frac{cot(x) cot(y) \mp 1}{cot(y) \pm cot(x)} = \frac{cos(x \pm y)}{sin(x \pm y)}\\\\
	    sin(x + y) \cdot sin(x - y) &= cos^2(y) - cos^2(x) = sin^2(x) - sin^2(y)\\
	    cos(x + y) \cdot cos(x - y) &= cos^2(y) - sin^2(x) = cos^2(x) - sin^2(y)\label{eq:trigo_addtheo}
	  \end{align}
	  
	  \subsection{Folgerungen aus den Additionstheoremen}
	  \begin{align}
	  cos^2(\frac{x}{2}) + sin^2(\frac{x}{2}) &\;\;= cos(\frac{x}{2}) cos(\frac{x}{2}) + sin(\frac{x}{2}) sin(\frac{x}{2})\\ 
	  &\overset{\eqref{eq:trigo_addtheo}}{=} cos(\frac{x}{2}-\frac{x}{2}) = cos(0) = 1\\
	  \\
	  2sin(\frac{x}{2})cos(\frac{x}{2}) &\;\;= sin(\frac{x}{2})cos(\frac{x}{2}) + sin(\frac{x}{2})cos(\frac{x}{2})\\
	  &\overset{\eqref{eq:trigo_addtheo}}{=} sin(\frac{x}{2}+ \frac{x}{2}) = sin(x)\\
	  \\
	  sin(2x) = sin(x+x) &\overset{\eqref{eq:trigo_addtheo}}{=} sin(x)cos(x) + sin(x)cos(x) \\
	  &\;\;=2sin(x)cos(x) \label{eq:trigo:addtheo_folg}
	  \end{align}
	  \newpage
\chapter{HM1}

\section{Zahlen}
  \subsection{Zahlbereiche}
    \subsubsection{Natürliche Zahlen}
    \begin{itemize}
      \item Direkt vom Zählen abgeleitet.
      \item $\N = \{1, 2, 3, 4, ...\}$
      \item Gleichungen wie $n+x = m$ i.A. in $\N$ nicht lösbar. 
    \end{itemize}

    \subsubsection{Ganze Zahlen}
    \begin{itemize}
      \item $\Z = \{..., -3, -2, -1, 0, 1, 2, 3, ...\}$
      \item Gleichungen wie $n \cdot x = m$ i.A. in $\Z$ nicht lösbar.
    \end{itemize}

    \subsubsection{Rationale Zahlen}
		\begin{itemize} 
			\item $\Q = \left\lbrace \frac{m}{n}: n\in \N, m \in \Z\text{, m, n teilerfremd} \right\rbrace$
			\item Gleichungen wie $x^2 = 2$ in $\Q$ nicht lösbar.
		\end{itemize}

		\subsubsection{Reelle Zahlen}
		\begin{itemize}
			\item $\R = \left\lbrace x = \sum_{j = - \infty}^n x_j 10^j : x_j \in \{0, 1, ..., 9\}\; \text{für ein n} \in \Z \right\rbrace $
			\item Gleichungen wie $x^2 = -1$ in $\R$ nicht lösbar.
		\end{itemize}

		\subsubsection{Komplexe Zahlen}
		\begin{itemize}
			\item $\C = \left\lbrace z = x + iy:\; x,\;y \in \R \right\rbrace$
			\ $i$ heißt imaginäre Einheit, es gilt $\R \subset \C$.
		\end{itemize}
	
	\subsection{Algebraische Strukturen}
		\subsubsection{Gruppe}
		\begin{definition} 
		  \glqq Eine Menge $M$ mit der Verknüpfung $+:M\times M \rightarrow M$, deren Elemente die Eigenschaften $(G1) - (G3)$ erfüllen, heißt Gruppe. Gilt zusätzlich $(G4)$, so heißt $(M,+)$ eine kommutative Gruppe.\grqq \cite{HM12}
		In der Literatur findet man als alternative Bezeichnung kommutativ auch abelsch. \citeVgl{LinAF}
		\end{definition}
		
			\subsubsubsection{Gruppenaxiome}
		  \begin{equation}
			  \begin{tabularx}{14.7cm}{l l l}
					(\textbf{G1}) & $x+(y+z) = (x+y)+z$ & \colBlue{(Assoziativgesetz)}\\
					(\textbf{G2}) & $x+0 = x = 0+x$ & \colBlue{(0 ist das neutrale Element)}\\
					(\textbf{G3}) & $x+(-x) = (-x)+x = 0$ & \colBlue{(-x ist das inverse Element zu x)}\\
					(\textbf{G4}) & $x+y = y+x$ & \colBlue{(Kommutativität also Vertauschbarkeit)}\\
			  \end{tabularx}
			  \label{ax:groupaxioms}
			\end{equation}
			
			\begin{definition} 
			  \glqq Sei eine Gruppe eine Verknüpfung mit $\cdot$ und $G' \subset G$ eine nichtleere Teilmenge. $G'$ heißt eine Untergruppe, wenn für $a,\;b \in G'$ auch $a\cdot b \in G'$ und $a^{-1} \in G'$. \cite{LinAF}
			  \newline
			  Im Bezug auf Gruppen und Ringe spielen die Begrifflichkeiten Isomorphismus und Homomorphismus eine Rolle. Beide Begriffe leiten sich von Morphismus (Struktur bzw. Form), Homo (gleich im Sinne von ähnlich) und Iso (gleich im Sinne von identisch) ab. 
		  \end{definition}
		  
			\begin{definition} 
			  \glqq Sind $G$ und $H$ Gruppen mit Verknüpfungen $\cdot$ und $\times$, so heißt eine Abbildung $\varphi : G \rightarrow H$ Homomorphismus (von Gruppen), wenn
			\end{definition}
			
			\begin{equation}
			  \varphi (a \cdot b) = \varphi (a) \times \varphi (b) \qquad  \forall a,\;b \in G\text{. \cite{LinAF}}
			\end{equation}
			\newline
			Ein Homomorphismus heißt Isomorphismus wenn er bijektiv ist. \cite{LinAF}
		
		  \subsubsection{Ring}
		  \begin{definition}
		    Erfüllt eine Menge die Eigenschaften einer Gruppe, hat jedoch zwei Verknüpfungen (z.B. $(+,-)$) spricht man von einem Ring.
		\end{definition}
		
		  \subsubsection{Körper}
		  \begin{definition}
		    Erfüllt eine Menge die Eigenschaften eines Ringes und ist zusätzlich kommutativ spricht man von einem Körper. Ein Beispiel für einen Körper ist die algebraische Struktur der rationalen und reellen Zahlen. 
		  \end{definition}
		
	    \subsubsubsection{Körperaxiome}
		  \begin{equation}
				\begin{tabularx}{14.7cm}{l l l}
					(\textbf{K1}) & $x\cdot(y\cdot z) = (x\cdot y)\cdot z$ & \colBlue{(Assoziativgesetz)}\\
					(\textbf{K2}) & $x\cdot 1 = 1 \cdot x = x$ & \colBlue{(1 ist das neutrale Element)}\\
					(\textbf{K3}) & $x\cdot(\frac{1}{x}) = (\frac{1}{x}) \cdot x = 1$ & \colBlue{($\frac{1}{x}$ ist das inverse Element zu x)}\\
					(\textbf{K4}) & $x\cdot y = y\cdot x$ & \colBlue{(Kommutativität also Vertauschbarkeit)} \\
					(\textbf{D}) & $x \cdot (y+z) = x \cdot y + x \cdot z$ & \colBlue{(Kommutativität also Vertauschbarkeit)}\\
				\end{tabularx}
		    \label{ax:körperaxiome}
		  \end{equation}
		  \newline
		
      \begin{definition} 
        \glqq Eine Menge $M$ mit den Verknüpfungen $+:M\times M  \rightarrow M$ und $\cdot: M\times M \rightarrow M$, deren Elemente die Gesetze $(G1) - (G4)$, $(M1) - M4)$ und $(D)$ erfüllt, heißt Körper.\grqq \cite{HM12}
      \end{definition}
		
	\subsection{Komplexe Zahlen}
	\begin{align}
		\C = \{ z &= x + iy:\; x,\;y \in \R \}\\
		i^2 &= 1
	\end{align}
	Den Realteil einer komplexen Zahl $z$ bezeichnet man i.A. mit $x$, den Imaginärteil mit $y$.
	\begin{equation}
	  Re\{z\} = x \qquad Im\{z\} = y
	\end{equation}
	
	\begin{definition} 
	  \glqq Zwei komplexe Zahlen sind gleich, wenn ihre Real- und Imaginärteile gleich sind. \grqq \cite{HM12}
	\end{definition}
	
		\subsubsection{Betrag der komplexen Zahl}
		Da sich komplexe Zahlen geometrisch im $\R^2$ interpretieren lassen, kann aus beiden Teilen ein Betragspfeil gebildet werden.
		\begin{equation}
		  |z| = \sqrt{x^2 + y^2}
		\end{equation}
		
		\subsubsubsection{Eigenschaften}
  	\begin{align}
		  |z| &= 0\\
			|z| = 0 &\Leftrightarrow z = 0\\
			|z_1 \cdot z_2| &= |z_1| |z_2|\\
			|z_1 + z_2| &\leq |z_1| + |z_2| \quad \text{\colBlue{(Dreiecksungleichung)}}
		\end{align}
		
		\subsubsection{Komplex konjugierte Zahl}
		\begin{definition} 
		  \glqq $\overline{z} = x -iy$ heißt die zu $z = x + iy$ konjugiert komplexe Zahl. \grqq \cite{HM12}
		\end{definition}
		
		\subsubsubsection{Eigenschaften}
		\begin{align}
			\overline{\overline{z}} &= z \\
			\overline{z_1 + z_2} = \overline{z_1} + \overline{z_2},&\qquad \overline{z_1 \cdot z_2} = \overline{z_1} \cdot \overline{z_2} \\
			Re\{z\} = \frac{1}{2} /z + \overline{z}), &\qquad Im\{z\} = \frac{1}{2i} /z - \overline{z})\\
			|z| = \sqrt{z \overline{z}}, &\qquad z\overline{z} = x^2 + y^2
		\end{align}
			
		\subsubsection{Polarkoordinatendarstellung}
		\begin{align}
			z = x + iy &= |z|(cos\varphi + isin\varphi)\\
			\Rightarrow x = |z| cos\varphi, &\qquad \rightarrow y = |z| sin\varphi) \nonumber\\
			\Rightarrow tan \varphi &= \frac{sin \varphi}{cos \varphi} = \frac{x}{y}
		\end{align}
		
		Achtung. Die Umkehrfunktion von Tanges ist nicht eindeutig. Es gilt:
		
		\begin{align}
		\varphi = 
		\begin{cases} 
		    arctan(\frac{x}{y}) \qquad &,\; x > 0,\; y\geq 0\\
		    \frac{\pi}{2}  &,\;x = 0,\; y > 0\\
		    \pi + arctan(\frac{y}{x}) &,\; x < 0\\
		    \frac{3\pi}{2} &,\; x=0,\;y< 0\\
		    2\pi + arctan(\frac{y}{x}) &,\; x> 0,\; y < 0
		\end{cases}
		\end{align}
	
	  \subsubsection{Multiplikation}
	  \begin{align}
	  z_1 \cdot z_2 &\;\; = |z_1|(cos\varphi_1 + isin\varphi_1) \cdot |z_2| (cos\varphi_2 + isin\varphi_2)\\
	  &\overset{\eqref{eq:trigo_addtheo}}{=} |z_1| |z_2| (cos(\varphi_1 + \varphi_2) + isin(\varphi_1 + \varphi_2))\\
	  &\;\; \Rightarrow |z_1 \cdot z_2| = |z_1| |z_2| \qquad ,\; Winkel = \varphi_1 + \varphi_2 \label{eq:zahl_kompl_mult}
	  \end{align}
	  
	  \subsubsection{Formel von de Moivre}
	  Setzt man in \eqref{eq:zahl_kompl_mult} $z_1 = z_2 = z$ ein erhält man die Formel von de Moivre.
	  \begin{align}
	    z \cdot z &= z^2 = |z|^2 (cos(2\varphi) + isin(2\varphi)) &,\; bzw.&&\nonumber\\
	    z^n &= |z|^n(cos(n\varphi)  + isin(n \varphi)) &,\; bzw.&&\nonumber\\
	    z^n& = |z|^n(cos\varphi + isin\varphi)^n\nonumber\\
	    &\Rightarrow (cos\varphi + isin\varphi)^n = cos(n\varphi) + isin(n\varphi)
	  \end{align}
	
	  Mit der Eulerschen Formel erhält man $e^{i\varphi} := cos\varphi + isin\varphi$. Daraus folgt:
	  \begin{align}
	  (e^{i\varphi})^n &= e^{in\varphi} \\
	  e^{i(\varphi_1 + \varphi_2} &= e^{i\varphi_1} e^{i\varphi_2}\\
	  e^{-i\varphi} &= \frac{1}{e^{i\varphi}}\\
	  z_x = |z_x|e^{i\varphi_x} &\Rightarrow z_1 z_2 = |z_1| |z_2| e^{i(\varphi_1 + \varphi_2)}
	  \end{align}

  \subsection{Polynome}
  Ein komplexes Polynom hat die Form
  \begin{align}
    p(z) = \sum_{k=0}^n a_k z^k = a_n z^n + a_{n-1} z^{n-1} + ... + a_0 \\
    a_k \in \C \nonumber
  \end{align}
  Falls $a_n \neq 0$ gibt $n$ den Grad des Polynoms an.
  \newline
  \begin{definition} 
    \glqq Ist $p(z)$ ein reelles Polynom, d.h. $a_k \in \R$, dann ist mit $z\in \C$ auch 
  $\overline{z} \in \C$ eine Nullstelle, d.h. aus $p(z) = 0$ folgt $p(\overline{z}) = 0$, d.h. die
  Nullstellen sind konjugiert komplex zueinander. \grqq \cite{HM12}
  \end{definition}

	\subsubsection{Fundamentalsatz der Algebra}
	\begin{satz}
	  Jedes Polynom vom Grad $\geq$ 1 besitzt in $\C$ mindestens eine Nullstelle.     
	  \label{satz:fund_alg}
	\end{satz}
	  
  \subsubsection{Polynomdivision}
    Über Polynomdivision lassen sich Polynome in Linearfaktoren zerlege. 
    \begin{align}
      Bsp.:  \nonumber\\
      (2z^3 - 3z^2 -6z +6) : (z-2) = (2z^2 +z -4)\quad ,\text{ Rest: } -2 \nonumber\\
      \underset{\text{\rule{3cm}{0.4pt}}}{-(2z^3 - 4z^2)} \nonumber \\
      0 + z^2 - 6z + 6 \nonumber \\
      \qquad \underset{\text{\rule[5mm]{3cm}{0.4pt}}}{z^2-2z} \nonumber \\
    \end{align}
    
    Aus \ref{satz:fund_alg} folgt:
    \begin{satz}
      Jedes Polynom p vom Grad $n\geq 1$ lässt sich über $\C$ in Linearfaktoren zerlegen, d.h. es gibt Zahlen $z_K$, die Nullstellen von p sind, sodass
      \begin{align*}
        p(z) = a_n(z-z_1)(z-z_2).../z-z_n)
      \end{align*}
    \end{satz}
    
    \begin{satz} $\;$
      \begin{itemize}
        \item[a)]
          Besitzt ein Polynom p vom n-ten Grad n+1 Nullstellen, so ist p = 0
        \item[b)]
          Stimmen zwei Polynome p und q jeweils vom Grad n an n+1 Stellen überein, so ist p = q.
      \end{itemize}
    \end{satz}
    
  \subsection{Einheitswurzeln}
  Wurzeln der Form $z^n = 1$ können allgemein einfach bestimmt werden. Mit $|z^n| = |z|^n = 1$ folgt $|z| = 1$. Mit der eulerschen Identität folgt aus
  \begin{align*}
    z &= e^{i\varphi} = cos \varphi + i \; sin\varphi = 1 = 1 + 0 \cdot i\\
    &\Rightarrow n\varphi = 2 \pi k \quad ,k \in \Z\\
    &\Rightarrow \varphi = \frac{2\pi k}{n}\\
    &\Rightarrow z_0 = 1 e^{i\cdot 0},\quad z_1 = e^{i\frac{2\pi}{n}}, \quad ... z_n = e^{i\frac{k\pi}{n}}
  \end{align*}    
  Allgemein gilt mit $z^n = a$ und $\varphi = arccos\left(\frac{|z|}{Re\{z\}}\right)$:
  \begin{align}
    z_k = a^{\frac{1}{n}} \cdot \left(cos\left(\frac{\varphi + 2k\pi}{n}\right) + i\cdot sin\left(\frac{\varphi + 2k\pi}{n}\right)\right)
  \end{align}
  Bei reellen Polynomen sind dabei die Nullstellen komplex konjugiert zueinander.
  \newpage
  
\section{Lineare skalare Differentialgleichungen mit konstanten Koeffizienzen}
  \begin{definition}
    Lineare skalare Diff.gleichungen mit konst. Koeffizienten sind Gleichungen der bauart:
    \begin{align*}
      Ly(x) = a_n y^{(n)}(x)+a_{n-1}y^{(n-1)}(x)+...+a_1y'(x) + a_o y(x) = f(x)
    \end{align*}
  \end{definition}
  \subsection{Ansätze}
  \begin{itemize}
    \item[a)] Homogene Diff.gleichung:
    Mit $\lambda$ als einfache NST:
    \begin{equation}
      y(x) = e^{\displaystyle\lambda x} \label{eq:dgl_Ansatz_a}
    \end{equation}
    Mit $\lambda$ als $n$-fache NST:
    \begin{equation}
      f(x) = e^{\displaystyle\lambda x}, xe^{\displaystyle\lambda x}, ..., x^{n-1}e^{\displaystyle\lambda x} 
    \end{equation}
    Mit $\lambda = \alpha + \beta i$ (komplexe Nullstellen):
    \begin{equation}
      e^{\alpha x}cos(\beta x),\quad e^{\alpha x} sin(\beta x)
    \end{equation}
    \item[b)] Inhomogenität der Form $f(x) = p_k(x) e^{sx}$ mit $p_k(x)$ als ein Polynom $k$-ten Grades.
    \begin{equation}
      y_p(x) = R_k(x)e^{sx}x^q
    \end{equation}
    Mit $R_k(x) = a_kx^k+...+a_0$ und $q$ als Vielfachheit der Nullstelle $s$ (ist $s$ keine Nullstelle so ist $q = 0$ und damit $x^q = 1$).
    \item[c)] Inhomogenität der Form $cos(kx)$ oder $sin(kx)$.
    \begin{equation}
      yp(x) = (a cos(kx) + b sin(kx)) x^q
    \end{equation}
    Beispiel Umformung:
    \begin{align}
      f(x) &= sin(4x)\nonumber\\
      Mit \quad
      (1)\;\;\; e^{i\varphi} &= cos \varphi + i sin\varphi\nonumber \\
      und \quad
      (2)\; e^{-i\varphi} &= cos \varphi - i sin \varphi\nonumber\\
      folgt \;mit \;(1) + (2) \;&bzw.\; (1)-(2)\nonumber\\
      e^{i\varphi} + e^{-i\varphi} = 2\;cos\varphi \quad &bzw. \quad e^{i\varphi}-e^{-i\varphi} = 2 \; sin\varphi\nonumber\\
      \Rightarrow cos \varphi = \frac{1}{2} (e^{i\varphi}+e^{-i\varphi}) &\quad \Rightarrow sin \varphi = \frac{1}{2i} (e^{i\varphi}-e^{-i\varphi}),\quad \forall \varphi \in \R
    \end{align}
    $q$ im Ansatz gibt die Vielfachheit der NST $i \varphi$ im charackteristischen Polynom des homogenen Teils der Gleichung an.
    \item[d)] Inhomogenität der Form $q_k(x)e^{\alpha x} cos(\beta x)$ oder $q_k(x)e^{\alpha x} sin(\beta x)$
    \begin{equation}
      yp(x) = R_k(x)x^qe^{\alpha x}cos(\beta x) + \tilde{R}_k(x) x^qe^{\alpha x}cos{\beta x}
    \end{equation}
  \end{itemize}
  \subsection{Vorgehensweise}
    \begin{flalign*}
      &\textbf{Schritt 1: } \text{Ansatz wählen}&
    \end{flalign*}
    \begin{flalign*}
      &\textbf{Schritt 2: } \text{Ansatz einsetzen und charackteristisches Polynom bilden}& \\
      &\text{Beispiel:}&
    \end{flalign*}
    \begin{align*}
      &y'' + 3y'+2y = 0 \\
      &\overset{\eqref{eq:dgl_Ansatz_a}}{\Rightarrow} \left(e^{\lambda x}\right)'' + 3\left(e^{\lambda x}\right)' + 2e^{\lambda x} = 0 \\
      \left(e^{\lambda x}\right)' &\;\;= \lambda e^{\lambda x} \Rightarrow \left(e^{\lambda x}\right)'' = \lambda^2 e^{\lambda x} \\
      &\;\;\Rightarrow \lambda^2 + 3 \lambda + 2 = 0
    \end{align*}
    \begin{flalign*}
      &\textbf{Schritt 3.1: } \text{Nullstellen des char. Polynoms suchen und in Ansatz einsetzen}&\\
      &Beispiel:&
    \end{flalign*}
    \begin{align*}
      \lambda^2 + 3 \lambda + 2 = 0 \Rightarrow \lambda_1 = -2,\quad \lambda_2 = -1
    \end{align*}
    \begin{flalign*}
      &\textbf{Schritt 3.2: } \text{Gegebenenfalls komplexe NST in reale umwandeln}&\\
      &Beispiel:&
    \end{flalign*}
    \begin{align*}
      y(x) &= C_1 e^{-x+ix} + C_2 e^{-x-ix} \\
      \Rightarrow y(x) &= \tilde{C}_1e^{-x}cosx + \tilde{C}_2 e^{-x}sinx \qquad, \tilde{C}_1, \tilde{C}_2 \in \R
    \end{align*}
    \begin{flalign*}
      &\textbf{Schritt 4: } \text{Allgemeine Lösung aufstellen}&
    \end{flalign*}
    \begin{align*}
      \Rightarrow y(x) = C_1 e^{-2x} + C_2 e^{-x}\quad, C_1,C_2\in \R
    \end{align*}
    
    \newpage
    \subsubsection{Anfangswertproblem}
    Falls Anfangswerte vorhanden sind können an dieser Stelle die Konstanten Koeffizienten explizit bestimmt werden. \newline
    \begin{flalign*}
    &Beispiel:&
    \end{flalign*}
    \begin{align*}
	    y(0) &= 1,\;y'(0) = 0\\
	    y(t) &= C_1 e^{5t} + C_2 e^{-2t}\\
	    &\Rightarrow y(0) = C_1 e^{5\cdot 0} + C_2 e^{-2\cdot 0} = 1\\ 
      &\Rightarrow C_1 = 1-C_2\\
      y'(t)& = 5 C_1 e^{5t} -2 C_2 e^{-2t} \Rightarrow y'(0) = 5C_1 - 2C_2 = 0\\
      &\Rightarrow y'(0) = 0 = 5(1-C_2)-2C_2 \Rightarrow C_2 = \frac{5}{7}\\
      &\Rightarrow C_1 = 1-C_2 = 1- \frac{5}{7} = \frac{2}{7}\\
      \; \\
      &\Rightarrow y(t) = \frac{2}{7} e^{5t}+\frac{5}{7}e^{-2t}
    \end{align*}
    
    \subsubsection{Inhomogenität}
      \begin{flalign*}
        &\textbf{Schritt 1: } \text{Ansatz wählen}&
      \end{flalign*}
      \begin{flalign*}
        &\textbf{Schritt 2: } \text{Ansatz gegebenenfalls ableiten und in homogenen Teil einsetzen}&
      \end{flalign*}    
      \begin{flalign*}
        &\textbf{Schritt 3: } \text{Über Koeffizientenvergleich Vorfaktoren bestimmen}&
      \end{flalign*}    
      \begin{flalign*}
        &\textbf{Schritt 4: } \text{Allgemeine Lösung bilden}&
      \end{flalign*}    
      \begin{equation}
        y(x) = y_{hom}(x) + y_p(x)
      \end{equation}
      \begin{flalign*}
        &\textbf{Schritt 5: } \text{Gegebenenfalls Anfangswertproblem lösen}&
      \end{flalign*}    
      \newpage
    
\section{Lineare Gleichungssysteme}
  \subsection{Gauss Algorithmus}
  Siehe Kurzzusammenfassung HM2
  \begin{align}
	     \begin{array}{c c c c c c c c l}
	       \tilde{a}_{11} x_1 & + & \tilde{a}_{12} x_2 & + & \dots & + & \tilde{a}_{1n} x_n & = &  \tilde{b}_1 \\
	       \ddots &\;& \ddots &\;&\;&\;&\;&\;&\vdots\\
	       \; & \; & a^{(r)}_{rr}x_r &+& \dots &+& a^{(r)}_{rn}x_n &=& b_r\\
	       &\;&\;&\;&\;&\;& 0 & = & b^{(r)}_{r+1}\\
	       &\;&\;&\;&\;&\;& \; & \vdots & \;\\
	       &\;&\;&\;&\;&\;& 0 & = & b^{(r)}_{m}
	     \end{array}
	\end{align}
	\begin{definition}
	  Die Zahl $r$ heißt der Rang der Matrix A. Die Zahl $a_{11}$ heißt Pivotelement (Tendenziell eher die Zahlen $a_{11}$ bis $a_{rr}$ heißen Pivotelemente, aber so wie oben stehts im Script). 
	\end{definition}
	
	\subsubsection{Lößbarkeit}
	\begin{itemize}
	  \item[1. Fall: ]
	  Damit Lösungen existieren können, muss $b^{(r)}_{r+1} = ... = b^{(r)}_{m} = 0$ gelten, sonst existieren keine Lösungen. Das heißt es darf keine Zeile allgemein der Form $a = 0$ mit $-\infty < a < 0 \lor 0 < a < \infty$ existieren.
	  \item[2. Fall: ]
	    Ist $b^{(r)}_{r+1} = ... = b^{(r)}_{m} = 0$ und ist $r = n$, so existiert eine eindeutige Lösung. Das heißt es existieren gleich viele Unbekannte wie Zeilen und Fall 1 ist ausgeschlossen.
    \item[3. Fall: ]
    Ist $b^{(r)}_{r+1} = ... = b^{(r)}_{m} = 0$ und ist $r < n$, so existiert eine Schar von Lösungen. D.h. es kann eine Variable frei gewählt werden.
  \end{itemize}
	\newpage
	
\section{Analytische Geometrie}
  \subsection{Dreidimensionaler Raum}
  \begin{definition}
    Der dreidimensionale Raum ist durch $\R^3 = \R \times \R \times \R = \lbrace p_1, p_2, p_3: p_1, p_2, p_3 \in \R \rbrace$. Jeder Punkt $p\in \R^3$ ist eindeutig durch die Angabe seiner Koordinaten $p_1, p_2, p_3)$ bestimmt.
  \end{definition}
  \subsubsection{Abstand}
  \formTab{Euklidischer Abstand zweier Punkte}{d(P,Q)=\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2 + (p_3 - q_3)^2}} 
  \subsubsection{Vektoren} 
  \formTab{Norm eines Vektors}{||x||_{\R^3} = \sqrt{x_1^2 + x_2^2 + x_3^2}}
  Vektoren können skalar multipliziert oder miteinander addiert werden:
  \begin{align}
    \lambda \vecT{x_1 \\ x_2 \\ x_3} &= \vecT{\lambda x_1 \\ \lambda x_2 \\ \lambda x_3}\\
    \vecT{x_1 \\ x_2 \\ x_3} + \vecT{y_1 \\ y_2 \\ y_3} &= \vecT{x_1 + y_1 \\ x_2 + y_2 \\ x_3 + y_3}
  \end{align}   
  \begin{definition}
    Die Koordinaten $P = (p_1, p_2, p_3)$ eines Punktes $p$ definieren den Ortsvektor $p$. Sind zwei Punkte $P,Q$ mit Ortsvektoren $p,q$ gegeben, so gilt 
    \begin{equation}
      d(P,Q) = ||p-q||
    \end{equation}   
  \end{definition}  
    
  \subsubsection{Skalarprodukt}
  Das Skalarprodukt projeziert einen Vektor $b$ auf einen Vektor $a$. Es ist im $\R^3$ definiert als:
  \begin{align}
    (a,b) = <a,b> &= a_1 b_1 + a_2 b_2 + a_3 b_3 \\
    &bzw. \nonumber \\
    (a,b) &= ||a|| \; ||b|| cos \phi
  \end{align}
  Weiterhin gilt:
  \begin{equation}
  ||x|| = \sqrt{x_1^2 + x_2^2 + x_3^2} = \left(\left<\vecT{x_1 \\ x_2 \\ x_3},\vecT{x_1 \\ x_2 \\ x_3}\right>\right)^{\frac{1}{2}} = (x,x)^{\frac{1}{2}}
  \end{equation}
  \begin{definition}
	  $a$ und $b$ heißen orthogonal, falls 
	  \begin{equation*}
	    (a,b) = 0 
	  \end{equation*}
  \end{definition}
  
  \subsubsection{Vektor- bzw. Kreuzprodukt}
  \begin{definition}
    Der Betrag des Vektorproduktes $a\times b$ ist gleich dem Flächeninhalt des von $a$ und $b$ aufgespannten Parallelogramms. Dies folgt direkt aus der Trigonometrie des Dreiecks.
    \begin{equation}
      A = ||a \times b||
    \end{equation}
    Es gilt:
    \begin{equation}
      ||a \times b|| = ||a|| \; ||b|| sin\phi
    \end{equation}    
    Das Vektorprodukt $a \times b$ steht senkrecht auf $a$ und $b$. Die Vektoren $a,b,a \times b$ bilden ein Rechtssystem. $a$: Daumen, $b$: Zeigefinger, $a \times b$: Mittelfinger der rechten Hand.
  Es gelten folgende Eigenschaften:
  \begin{itemize}
    \item[i)   ] $a \times a = 0$
    \item[ii)  ] $a \times b = -b \times a$
    \item[iii) ] $(\lambda a) \times b = \lambda (a \times b)$
    \item[iv)  ] $a \times (b+c) = a \times b + a \times c$
    \item[v)   ]
    \begin{align*}
      \text{Seien }e_1, e_2, e_3\text{ die Einheitsvektoren (siehe HM2) dann gilt:}\\
      e_1 \times e_2 = e_3, \quad e_2 \times e_3 = e_1, \quad e_3 \times e_1 = e_2
    \end{align*}
    \item[vi)   ]
    \begin{equation}
      a := \vecT{a_1 \\ a_2 \\ a_3},\; b := \vecT{b_1 \\ b_2 \\ b_3} \Rightarrow a \times b =  \vecT{a_2 b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1} 
    \end{equation}
  \end{itemize}
  \end{definition}
  Es gilt die Cauchy-Schwarzsche Ungleichung:
  \begin{align}
    x,y \in \R^n_{\backslash \lbrace 0 \rbrace} &\nonumber \\
    |x \cdot y| &\leq ||x|| \; ||y|| \nonumber \\
    |x \cdot y| &= ||x|| \; ||y|| \Leftrightarrow x = \lambda y \quad, \lambda \in \R
  \end{align}
  
  \subsubsection{Spatprodukt}
  \begin{definition}
    Drei Vektoren $a,b,c$ spannen ein Volumen auf.
    \begin{equation}
      [a,b,c] = (a \times b, c)
    \end{equation}
  \end{definition}
  
  \subsection{Geraden im $\R^2$}
	  \subsubsection{Parameterdarstellung}
	  Die Parameterdarstellung einer Gerade im $\R^2$ ist gegeben mit:
	  \begin{align}
	    x &= a + \lambda(b-a)\quad, \lambda \in \R \nonumber \\
	    &\Rightarrow x = a + \lambda u
	  \end{align}
	  Wobei $u$ die Richtung der Gerade angibt und Richtungsvektor heißt. $a$ heißt Stützvektor.
	  \begin{figure}[htbp] 
		  \centering
		  \includegraphics[width=0.7\textwidth]{Geraden.png}
		  \caption{Gerade im $\R^2$\protect\cite{HM12}}
		  \label{fig:gerade_R2}
	  \end{figure}

    \subsubsection{Darstellung in Gleichungs- bzw. Normalform}
    Zur Darstellung in der Normalform muss zunächst ein Normalenvektor $n$ der orthogonal auf $u$ steht gewählt werden, d.h. es muss gelten $(n,u) = 0$.
    \begin{align}
      (n,x) = (n,a + \lambda u) = (n,a) + \lambda \underbrace{(n,u)}_{=0} = (n,a) =: p
    \end{align}
    Die Gleichungsdarstellung ist dann:
    \begin{equation}
      (n,x)-p = 0 
    \end{equation}
    \subsubsection{Hessesche Normalform}
    Zur Bildung der Hesseschen Normalform wird der Normalevektor normiert und es muss $p \geq 0)$ gelten. 
    \begin{equation}
      \left(\frac{n}{||n||},x\right)-p = (n^*,x)-p = 0
    \end{equation}
    \subsubsection{Minimaler Abstand}
    Der Punkt mit minimalem Abstand zum Ursprung $x^*$ auf der Gerade ist der Punkt, an dem der Ortsvektor orthogonal auf der Gerade steht. Das einsetzen dieses Punktes in die Hessesche Normalform liefert also den Abstand zum Ursprung mit:
    \begin{equation}
    (n^*,x^*)=||x^*|| = p = Abstand
    \end{equation}
    \subsubsection{Abstand eines Punktes zur Gerade}
    $\tilde{x}$ sei ein beliebiger Punkt, dann ist
    \begin{equation}
      d = (n^*, \tilde{x})-p
    \end{equation}
    Der Abstand des Punktes zur Geraden.
  \subsection{Geraden und Ebenen im $\R^3$}
    \subsubsection{Parameterdarstellung einer Geraden im $\R^3$}
    \begin{align}
      &g: x = a + \lambda u \quad ,bzw. \nonumber \\
      &g: \vecT{x_1 \\ x_2 \\ x_3} = \vecT{a_1 \\ a_2 \\ a_3} + \lambda \vecT{u_1 \\ u_2 \\ u_3}
    \end{align}
    \subsubsection{Koordinatendarstellung einer Geraden im $\R^3$}
    Eine Gerade lässt sich im $\R^3$ auch als Schnittgerade zweier Ebenengleichungen ausdrücken. Ein Beispiel dazu:
    \begin{align*}
      g: &= \vecT{x_1 \\ x_2 \\ x_3} = \vecT{1 \\ 0 \\ 2} + \lambda \vecT{1\\2\\1} \\
      &\Rightarrow
      \begin{array}{l l l}
        x_1 = 1 + \lambda \\
        x_2 = 2\lambda \\
        x_3 = 2 + \lambda
      \end{array}
      \Rightarrow \lambda = \frac{x_2}{2}\\
      &\Rightarrow x_1 = 1 + \frac{x_2}{2},\quad x3 = 2 + \frac{x_2}{2}
    \end{align*}
    \subsubsection{Parameterdarstellung einer Ebene im $\R^3$}
    \begin{align}
      &E: x = a + \lambda u + \mu v \quad ,bzw. \nonumber \\
      &E: \vecT{x_1 \\ x_2 \\ x_3} = \vecT{a_1 \\ a_2 \\ a_3} + \lambda \vecT{u_1 \\ u_2 \\ u_3} + \mu \vecT{v_1 \\ v_2 \\ v_3}
    \end{align}        
    \subsubsection{Gleichungsdarstellung bzw. Hessesche Normalform einer Ebene im $\R^3$}
    \begin{align}
    &(n^*,x) = (n^*,a) + \lambda \underbrace{(n,u)}_{=0} + \mu \underbrace{(n,v)}_{=0} \nonumber \\
    &\Rightarrow (n^*,x) - (n^*,a) = 0 \quad mit \; (n,a) =: p \nonumber \\
    &\Rightarrow (n^*,x) - p = 0
    \end{align}
    $p$ gibt den Abstand der Gerade zum Ursprung an. $n$ muss auf beiden Richtungsvektoren  senkrecht stehen und lässt sich durch
    \begin{equation}
      n = \frac{u \times v}{|| u \times v||}
    \end{equation}
    bestimmen.
  
    \begin{figure}[htbp] 
		  \centering
		  \includegraphics[width=0.7\textwidth]{Ebene_R3.png}
		  \caption{Ebenen im $\R^3$\protect\cite{HM12}}
		  \label{fig:ebene_R3}
	  \end{figure}
	  
	\subsection{Schnitte}
	  \subsubsection{Schnitt zweier Ebenen}
	  Sind die Normalenvektoren nicht parallel existiert eine Schnittgerade. Sie kann durch die entsprechenden Ebenengleichungen beschrieben werden.
	  \begin{align*}
	    E_1 &= \lbrace x\in \R^3:x_1 + 4x_2 = 2 \rbrace \\
	    E_2 &= \lbrace x \in \R^3: x_2 - x_3 = 1\rbrace \\
	    &\Rightarrow n_1 = \vecT{1\\4\\0},\quad n_2 = \vecT{0\\2\\-1} \\ 
	    &\Rightarrow n_1 \; und \; n_2 \;\text{nicht parallel} \Rightarrow \text{Schnittgerade ex.}\\
	    &\Rightarrow g=\lbrace x\in \R^3: x_1 + 4x_2 = 2, x_2 - x_3 = 1\rbrace
	  \end{align*}
	  Der Schnittwinkel zweier Ebenen kann über die jeweiligen Normalenvektoren bestimmt werden.
	  \begin{figure}[htbp] 
		  \centering
		  \includegraphics[width=0.67\textwidth]{schnitt_ebene.png}
		  \caption{Schnittwinkel zweier Ebenen\protect\cite{HM1Vortragsubung}}
		  \label{fig:ebene_schnittwinkel}
	  \end{figure} 
	  
	  \subsubsection{Schnitt einer Gerade mit einer Ebene}
	  Dort wo sich Ebene und Gerade schneiden muss der Punkt beide Gleichungen erfüllen. Um dies zu prüfen, setzt man dei Parameterdarstellung der Gerade in die Ebene ein. Erhält man einen Punkt, so ist dies der Punkt an dem sich beide schneiden.
	  \subsubsection{Schnitt zweier Geraden im $\R^3$}
	  Es seien $g_1$ und $g_2$ zwei Geraden mit
	  \begin{align*}
	    g_1: x = a + \lambda u \quad, \lambda \in \R\\
	    g_2: y = b + \mu v \quad, \mu \in \R
	  \end{align*}
	  Es existieren grundsätzlich drei Fälle:
	  \begin{itemize}
	    \item Es existiert ein Schnittpunkt \\
	      Schnittbedingung: $x = y$
	    \item Beide Geraden sind parallel \\
	      Bedingung: $u = \theta v \quad, \theta \in \R$
	    \item Die Geraden schneiden sich nicht und $u$ ist nicht parallel zu $v$ (windschief)
	  \end{itemize}
	  Für windschiefe Geraden ist der minimale Abstand durch die Bedingung $(x-y) \bot u,\; (x-y) \bot v$ gegeben.
	  \subsubsection{Abstand zweier Geraden im $\R^3$}
	  Hierzu stellt man Hilfsebenen auf.
	  \begin{align*}
	    g_1 \Rightarrow E_1: x = a + \lambda u + \mu v \\
	    g_2 \Rightarrow E_2: y = b + \lambda u + \mu v
	  \end{align*}
	  \begin{figure}[H] 
		  \centering
		  \includegraphics[width=0.5\textwidth]{Abstand_Geraden.png}
		  \caption{Abstand zweier Geraden $\R^3$}
		  \label{fig:abstand_geraden}
	  \end{figure}
	  Zur Bestimmung kann man den Abstandes vom Punkt $a$ (oder jeder andere Punkt auf $g_1$ zur Ebene $E_2$ bestimmen. Hierzu wird die hessesche Normalform von $E_2$ gebildet und $a$ eingesetzt.
\newpage

\section{Logik und Beweise}
  \subsection{Wahrheitswerte}
  \begin{table}[!htpb]
    \caption{Wahrheitswerte}
    \label{wahrheitswerte}
	  \begin{tabular}{|c|c|c|c|c|c|c|c|} \noalign{\hrule height 1.5pt}
		  $w(A$) & $w(B)$ & $\;$ & $w(\neg A)$ & $w(A \land B)$ & $w(A\lor B)$ & $w(A \Rightarrow B$ & $w(A \Leftrightarrow B$ \\ \noalign{\hrule height 1.5pt}
		  $1$  & $1$ & $\;$ & $0$ & $1$ & $1$ & $1$ & $1$\\ \hline
		  $1$  & $0$ & $\;$ & $0$ & $0$ & $1$ & $0$ & $0$\\ \hline
		  $0$  & $1$ & $\;$ & $1$ & $0$ & $1$ & $1$ & $0$\\ \hline
		  $0$  & $0$ & $\;$ & $1$ & $0$ & $0$ & $1$ & $1$\\ \noalign{\hrule height 1.5pt}
	  \end{tabular}
  \end{table}
  \begin{bem}
    Wenn $A \Rightarrow B$ wahr ist, dann heißt $A$ hinreichend für $B$ und $B$ heißt notwendig für $A$.
  \end{bem}
  \subsubsection{Tautologien}
  Tautologien sind Aussagen die immer wahr sind.
  \begin{itemize}
    \item $(A \Rightarrow B) \Leftrightarrow (\neg B \Rightarrow \neg A$
    \item $(A\Rightarrow B) \Leftrightarrow \neg(\neg B \land \neg A)$
    \item $A \lor \neg A$
    \item $\neg(A \land \neg A)$
    \item $\neg(A \land B) \Leftrightarrow \neg A \lor \neg B$ (De Morgansche Regel)
    \item $\neg(A \lor B) = \neg A \land \neg B$ (De Morgansche Regel)
    \item $(A \Rightarrow B) \land (B\Rightarrow C) \Rightarrow (A \Rightarrow C)$
    \item $(A \land (B\lor C)) \Leftrightarrow ((A \land B) \lor (A\land C))$ (Distributivgesetz)
    \item $(A\lor (B\land C)) \Leftrightarrow ((A \lor B) \land (A \lor C))$ (Distributivgesetz)
  \end{itemize} 
  
  \subsubsection{Umformungen}
  \begin{table}[H]
    \caption{Umformungen}
    \label{umformungen}
	  \begin{tabular}{|p{4cm}|p{10cm}|} \noalign{\hrule height 1.5pt}
      \textbf{Form der Negation} & \textbf{umgeformte Aussage} \\ \noalign{\hrule height 1.5pt}
      $\neg(\neg A)$ & $A$ \\ \hline
      $\neg(A \land B)$ & $(\neg A) \lor (\neg B)$ (De Morgansche Regel) \\ \hline
      $\neg(A \lor B)$ & $(\neg A)  \land (\neg B)$ (De Morgansche Regel) \\ \hline
      $\neg(A \Rightarrow B)$ & $A \land (\neg B)$ da $(A \Rightarrow B) \Leftrightarrow \neg A \lor \neg B$ \\ \hline
      $\neg(A \Leftrightarrow)$ & $A  \Leftrightarrow (\neg B)$\\ \hline
      $\neg (\forall x \in M: A(x))$ & $\exists x \in M: \neg A(x)$\\ \hline
      $\neg (\exists x \in M: A(x))$ & $\forall x \in M: \neg A(x)$\\ \noalign{\hrule height 1.5pt}
    \end{tabular}
  \end{table}
  \subsection{Vollständige Induktion}
    \subsubsection{Vorgehen}
    \begin{flalign*}
      &\textbf{Schritt 1: } \text{Induktionsannahme (also was zu zeigen ist)}&
    \end{flalign*}
    \vspace{-0.5cm}
    \begin{flalign*}
      &\textbf{Schritt 2: } \text{Induktionsanfang (meistens $n=1$, ist aber frei wählbar}& \\
    \end{flalign*}
    \vspace{-1cm}
    \begin{flalign*}
      &\textbf{Schritt 3: } \text{Induktionsvorraussetzung (eher optional)}&\\
    \end{flalign*}
    \vspace{-1cm}
    \begin{flalign*}
      &\textbf{Schritt 4: } \text{Induktionsbehauptung}&\\
      &\text{(die Behauptung, dass es für alle n gilt. Bei Summen hier in der Regel $n+1$ einfügen)}&\\
    \end{flalign*}
    \vspace{-1cm}
    \begin{flalign*}
      &\textbf{Schritt 5: } \text{Induktionsschritt (der eigentliche Beweis))}&\\
    \end{flalign*}
    \subsubsection{Beispiel 1}
    \begin{flalign*}
      &\underline{Induktionsannahme:}&\\ 
      &\qquad \qquad z.Z.:\; \forall n \in \N: 11^n - 4^n \text{ ist ein Vielfaches von 7}& \\
    \end{flalign*}
    \vspace{-1.7cm}
    \begin{flalign*}
      &\underline{Induktionsanfang:}&\\
      & n = 1:&\\
      & \; &11^1 - 4^1 &\overset{!}{=} x \cdot 7 \quad , x \in \N && \; &&\\
      &\; &11-4 &= 7 = x \cdot 7&&\; &&\\
      &\; &x &= 1 \checkmark &&\; &&\\
    \end{flalign*}
    \vspace{-1.7cm}
    \begin{flalign*}
      &\underline{Induktionsbehauptung:}&\\
      &\qquad \qquad 11^{n+1}-4^{n+1} = a\cdot 7  \quad , a \in \N \\
    \end{flalign*}
    \vspace{-1.7cm}
    \begin{flalign*}
    &\underline{Induktionsschluss:}&\\
    &\;& &11^{n+1}-4^{n+1} = 11^n \cdot 11-4^n \cdot 4 && \;&&\\
    &\;& &= 11^n (7+4)-4^n \cdot 4  && \;&&\\
    &\;& &= 7 \cdot 11^n + 4 \cdot 11^n + 4 \cdot 4^n  && \;&&\\
    &\;& &= \underbrace{7 \cdot 11^n}_{=(*)} + \underbrace{4(11^n - 4^n)}_{=(**)} && \;&&\\
    \end{flalign*}
    $(*)$ ist duch $7$ teilbar da $7$ ein Faktor ist, $(**)$ ist ebenfalls durch $7$ teilbar, da $(**)$ ein Vielfaches der Induktionsannahme ist, also gilt:
    \begin{flalign*}
    &\;& &7 \cdot 11^n + 4(11^n - 4^n) = b \cdot 7 \quad, b \in \R && \;&&\\
    &\;& &\Rightarrow b = 11^n + \frac{4 \cdot x\cdot 7}{7} = 11^n + 4 \cdot x&& \;&&\\
    \end{flalign*}
    Somit ist $b \cdot 7$ ein Vielfaches von $7$ was zu zeigen war.
  \subsection{Binomialkoeffizienten}
  \begin{satz}
    Es gibt $n! = 1 \cdot 2 \cdot 3 \cdot ...< \cdot (n-1) \cdot n$ Permutationen des n-Tupels $(1,...,n)$. 
  \end{satz}
  \begin{bem}
    Bezeichnung: $n!$ heißt $n$ Fakultät.
  \end{bem}
  Die Verallgemeinerung der binomischen Formel ist gegeben durch:
  \begin{equation}
    (a+b)^n = \sum_{k=0}^n \binom{n}{k} a^k \;0 b^{n-k}
  \end{equation}
  wobei 
  \begin{equation}
    \binom{n}{k} = \frac{n!}{(n-k)!k!}
  \end{equation}
  Binomialkoeffizient heißt und $0! = 1$ gilt.
  \begin{align}
    \begin{array}{c c c c c c c c c c c c}
      \; & \; &  \; & \; & \; &  1 & \; & \; & \; & \; & \;\\
      \; & \; &  \; & \; & 1  & \; &  1 & \; & \; & \; & \;\\
      \; & \; &  \; & 1  & \; & 2  & \; & 1  & \; & \; & \;\\
      \; & \; &  1  & \; & 3  & \; & 3  & \; & 1  & \; & \;\\
      \; & 1  &  \; & 4  & \; &  6 & \; & 4  & \; & 1  & \;\\
      1  & \; &  5  & \; & 10 & \; & 10 & \; & 5  & \; & 1 \\
      \; & \; &  \; & \; & \; &\vdots& \; & \; & \; & \; & \;\\
    \end{array}
    \begin{array}{c}
      \colBlue{(a+b)} \\
      \colBlue{(a+b)^2} \\
      \colBlue{(a+b)^3} \\
      \colBlue{(a+b)^4} \\
      \colBlue{(a+b)^5} \\
    \end{array}
  \end{align}
  bzw.
  \begin{align}
    \begin{array}{c c c c c c c c c c c}
      \; & \; &  \; & \; & \; &  \binom{0}{0} & \; & \; & \; & \; & \;\\
      \; & \; &  \; & \; & \binom{1}{0}  & \; &  \binom{1}{1} & \; & \; & \; & \;\\
      \; & \; &  \; & \binom{2}{0}  & \; & \binom{2}{1}  & \; & \binom{2}{2}  & \; & \; & \;\\
      \; & \; &  \binom{3}{0}  & \; & \binom{3}{1}  & \; & \binom{3}{2}  & \; & \binom{3}{3}  & \; & \;\\
      \; & \binom{4}{0}  &  \; & \binom{4}{1}  & \; &  \binom{4}{2} & \; & \binom{4}{3}  & \; & \binom{4}{4}  & \;\\
      \binom{5}{0}  & \; &  \binom{5}{1}  & \; & \binom{5}{2} & \; & \binom{5}{3} & \; & \binom{5}{4}  & \; & \binom{5}{5} \\
      \; & \; &  \; & \; & \; &\vdots& \; & \; & \; & \; & \;\\
    \end{array}
  \end{align}
  \newpage
  
\section{Mengen, Relationen und Abbildungen}
  \begin{definition}
    Mengen sind Ansammlungen von Elementen.
  \end{definition}
  \subsection{Arten von Mengen}
	  \begin{figure}[htbp] 
		  \centering
		  \includegraphics[width=0.9\textwidth]{mengen_arten.png}
		  \caption{Arten von Mengen\protect\cite{HM12}}
		  \label{fig:mengen_arten}
		\end{figure}
	  \begin{definition}
	    Ist $N \subset M$ dann heißt $N^c = M/N$ das Komplement von $N$. $M$ und $N$ heißen disjunkt, falls $M \cap N = \mathcal{O}$, wobei $\mathcal{O} = \lbrace \rbrace$ die leere Menge ist.
	  \end{definition}
	  \begin{definition}
	    Kartesisches Produkt: $M \times N = \lbrace (a,b): a \in M \land b \in N\rbrace$
	  \end{definition}
	  \begin{definition}
	    Potenzmenge: $P(M) = \lbrace X:X\subset M\rbrace$
	  \end{definition}
	  \subsubsection{Abkürzungen}
	  \begin{itemize}
	    \item $\bigcup\limits_{k = 1}^n A_k = A_1 \cup ... \cup A_n$
	    \item $\bigcap\limits_{k=1}^n A_k = A_1 \cap ... \cap A_n$
	    \item $\prod\limits_{k=1}^n A_k = A_1 \times ... \times A_n = \lbrace \underbrace{(a_1, ..., a_n)}_{n-Tupel}:\forall k: a_k \in A_k \rbrace$
	  \end{itemize}
	\subsection{Relationen}
	\begin{definition}
	  Relationen setzen Elemente zweier Mengen in Verbindung. Eine Relatiopn ist eine Teilmenge $R\subset M \times N$ mit $M$ und $N$ als Mengen. Man schreibt:
	  \begin{equation*}
	    xRy \text{ genau dann wenn } (x,y) \in R\subset M\times N
	  \end{equation*}
	  Jede Funktion $f \rightarrow f(x)$ ist eine Relation $R=\lbrace (x, f(x)): x\in \R \rbrace \subset \R \times \R$.
	\end{definition}
	\subsubsection{Äquivalenzrelationen}
	  Eigenschaften:
	  \begin{itemize}
	    \item $aRa$, d.h. $(a,a) \in R$ \colBlue{(Reflexivität)}
	    \item $aRb \Leftrightarrow bRa$, d.h. mit $(a,b)\in R$ ist auch $(b,a) \in R$ \colBlue{(Symmetrie)}
	    \item $aRb \land bRc \Rightarrow aRc$, d.h. mit $(a,b) \in R$ und $(b,c) \in R$ ist auch $(a,c) \in R$ \colBlue{(Transitivität)}
	  \end{itemize}
	  \begin{satz}
	    Ist $R$ eine Äquivalenzrelation, d.h. eine Relation mit obigen Eigenschaften, so zerfällt die Menge $M$ in disjunkte Teilmengen. \newline
	    Beispiel:
	    \begin{align*}
	      M &= \lbrace 1,2,3,4\rbrace \\
	      R &= \lbrace(1,3),(3,1),(1,1),(3,3),(2,4),(4,2),(2,2),(4,4)\rbrace\\
	      &\text{$R$ zerlegt $M$ in zwei Klassen (Teilmengen:}\\
	      \lbrace 2,4\rbrace &= [2]_R = [4]_R\\
	      \lbrace 1,3\rbrace &= \underbrace{[1]_R = [3]_R}_{\text{\parbox{2cm}{\centering% 
Repräsentanten \\[-1ex]der Klasse}}}
	    \end{align*}
	    bzw. allgemein: Äquivalenzklasse zu einem Element $a$ bezüglioch einer Äquivalenzrelation $R$
	    \begin{equation*}
	      [a]_R = \lbrace b \in M : aRb\rbrace
	    \end{equation*}
	    Die Menge aller Äquivalenzklassen ist: $M/_R = \lbrace[a]_R: a\in M\rbrace$. Es gilt entweder $[a]_R = [b]_R$ oder $[a]_R \cap [b]_R = \mathcal{O}$.
	  \end{satz}
	  \begin{bem}
	    Es wird häufig anstatt $R$ das Zeichen $\sim$ verwendet.
	  \end{bem}
	\subsubsection{Ordnungsrelationen}
	\begin{definition}
	  Eine Halbordnung $R\subset M \times M$ auf einer Menge $M$ erfüllt
	  \begin{itemize}
	    \item[(i)]   $xRx$
	    \item[(ii)]  $xRy \land yRx \Rightarrow y=x$
	    \item[(iii)] $xRy \land <Rz \Rightarrow xRz$ \colBlue{(Transitivität)}
	  \end{itemize}
	\end{definition}
	\begin{definition}
	  Ordnungsaxiome im $\R$
	  \begin{itemize}
	    \item[(i)]   $x \leq x$
	    \item[(ii)]  $x \leq y \land y \leq x \Rightarrow y = x$
	    \item[(iii)] $x \leq y \land y \leq z \Rightarrow x \leq z$
	    \item[(iv)] $x \leq y \lor y \leq x$ \colBlue{(Totalordnung)}
	    \item[(v)] $x \Rightarrow x+z \leq y+z$ \colBlue{(Zusammenhang mit Addition)}
	    \item[(vi)] $x\leq y \land z \geq 0 \Rightarrow x \cdot z \leq y \cdot z$ \colBlue{(Zusammenhang mit Multiplikation)}
	  \end{itemize}
	\end{definition}
	\begin{definition}
		Zu $a \in \R$ heißt
		\begin{align}
		  |a|:= 
		  \begin{cases} 
		    &a\quad ,\;falls\;a\geq 0 \\ -&a\quad ,\;falls \; a < 0 
		  \end{cases}
		\end{align}
		der Betrag von $a$.
	\end{definition}
	\subsection{Maximum, Minimum, Supremum, Infimum}
	Im Folgenden sei $M$ eine Teilmenge von $\R$.
	\begin{definition}
	  $x \in \R$ heißt eine obere Schranke von $M$, falls $y \leq x$ für alle $y \in M$ gilt. 
	\end{definition}
	\begin{definition}
	  $x \in \R$ heißt eine untere Schranke von $M$, falls $y \geq x$ für alle $y \in M$ gilt. 
	\end{definition}
	\begin{definition}
	  $M$ heißt beschränkt, falls $M$ nach oben und unten beschränkt ist, d.h. eine obere und eine untere Schranke existieren.
	\end{definition}
	\begin{definition}
	  $s = \sup M \in \R$ heißt das Supremum von $M$ und ist die kleinste obere Schranke, d.h. falls $s$ eine obere Schranke von $M$ ist und ferner jede andere obere Schranke $x$ von $M$ die Ungleichung $x\geq s$ erfüllt.
	\end{definition}
	\begin{definition}$\;$\newline
    \vspace{-0.7cm}
	  \begin{itemize}
	    \item[] Infimum: $\inf M \in \R$ ist die größte untere Schranke
	    \item[] Maximum: $s = \max M \in \R$ heißt das Maximum von $M$, falls $s \in M$ und für alle $x \in M$ gilt: $x\leq s$.
	    \item[] Minimum: $s = \min M \in \R$ heißt das Minimum von $M$, falls $s \in M$ und für alle $x \in M$ gilt: $s \leq x$.
	  \end{itemize}
	\end{definition}
	\begin{bem}
	  Für jede endliche Menge existiert das Maximum und Minimum.
	\end{bem}
	\begin{satz}
	  Jede nicht leere, nach oben beschränkte Menge $M \subset \R$ besitzt ein Supremum. (Analog jede nach unten beschränkte Menge ein Infimum)
	\end{satz}
	\begin{bem}
	  \begin{align}
	    \sup(T) \in T &\Rightarrow \sup(T) = \max(T) \\
	    \inf(T) \in T &\Rightarrow \inf(T) = \min(T)
	  \end{align}
	\end{bem}
	\subsection{Abbildungen}
	Abbildungen sind spezielle Relationen. Eine Abbildung $f$ von einer Menge $M$ in eine Menge $N$ ist eine Vorschrift, die jedem $x \in M$ genau ein $y \in N$ zuordnet. $M$ heißt der Definitionsbereich und $N$ heißt der Bildbereich. 
	\begin{definition}
	  $f: M\rightarrow N$ heißt surjektiv, falls die Gleichung $f(x) = y$ mindestens eine Lösung besitzt und zwar für alle $y \in N$.\newline
	  $f:M\rightarrow N$ heißt injektiv, falls für alle $x_1, x_2 \in M: f(x_1) = f(x_2) \Rightarrow x_1 = x_2$.\newline
	  $f$ heißt bijektiv, falls $f$ sowohl injektiv, als auch surjektiv ist.
	\end{definition}
	\subsection{Unendlichkeit}
	\begin{definition}
	  Eine Menge $M$ heißt endlich, falls es ein $n \in N$ und eine bijektive Abbildung $\Phi: \lbrace 1,...,n\rbrace \rightarrow M$ gibt. Eine Menge $M$ heißt abzählbar, falls es eine bijektive Abbildung $\Phi: \N \rightarrow M$ gibt. Sonst heißt die Menge überabzählbar.
	\end{definition}
	\begin{satz}
	  Die Menge der rationalen Zahlen $\Q$ ist abzählbar.
	\end{satz}
	\begin{satz}
	  Die reelen Zahlen sind überabzählbar.
	\end{satz}
	
	\subsection{Elementare realwertige Funktionen}
	  \subsubsection{Algebraische Funktionen}
	  Durch arithmetische Operationen aufgebaut (+,-,$\cdot$,$\backslash$), z.B.: \newline $f(x) = x^2 + 5x$, $f(x) = \sqrt[3]{x}$
    \subsubsubsection{Polynome (+,-,$\cdot$)}
    \begin{equation}
      y = f(x) = a_n x^n + a_{n-1} x^{n-1} + ... + a_0,\quad a_j \in \R
    \end{equation}  
		\vspace{-0.7cm}  	  
	  \begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.7\linewidth]{funktionen_quadratisch.png}
		  \caption{$y = x^2\quad (vgl.\; x^4, x^6, ...)$}
		  \label{fig:funkt_quadr}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.7\linewidth]{funktionen_kubisch.png}
		  \caption{$y = x^3 \quad (vgl.\;x^5, x^7, ...)$}
		  \label{fig:funkt_kub}
		\end{minipage}
		\end{figure}
		\vspace{-0.5cm}
		\begin{figure}[H]
		  \centering
		  \includegraphics[width=0.3\linewidth]{funktionen_linear.png}
		  \caption{$y = ax+b$}
		  \label{fig:funkt_lin}
		\end{figure}
		
		\vspace{-0.5cm}
		\newpage
		\subsubsubsection{Rationale Funktionen (+,-,$\cdot$, $\backslash$)}
		\begin{equation}
		  y = f(x) = \frac{p(x)}{q(x)} \qquad \text{mit } p,q \text{ sind Polynome}
		\end{equation}
		\begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.6\linewidth]{funktionen_pole_1.png}
		  \caption{$y = \frac{1}{x}$}
		  \label{fig:funkt_pole_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.65\linewidth]{funktionen_pole_2.png}
		  \caption{$y = \frac{1}{x^2}$}
		  \label{fig:funkt_pole_2}
		\end{minipage}
		\end{figure}
		
		\subsubsubsection{n-te Wurzel}
		\begin{equation}
		  y = f(x) = \sqrt[n]{x}
		\end{equation}
		\begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.6\linewidth]{funktionen_wurzel_2.png}
		  \caption{Quadratwurzel}
		  \label{fig:funkt_root_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.6\linewidth]{funktionen_wurzel_3.png}
		  \caption{Kubische Wurzel}
		  \label{fig:funkt_root_2}
		\end{minipage}
		\end{figure}
		Wie man an den Graphiken erkennt ist die Wurzel gerade die Spiegelung an der Ursprungsgeraden des korrelierenden Polynoms. 
		
		\subsubsection{Transzendentale Funktionen}
		\subsubsubsection{Exponentialfunktionen}
		\begin{equation}
		  y =? f(x) = e^x \qquad, e = 2,7182...
		\end{equation}
		Streng monoton wachsende Funktion. Es gilt:
		\begin{equation}
		  \lim_{x\rightarrow \infty} e^x = \infty \qquad \lim_{x\rightarrow -\infty} e^x = 0
		\end{equation}
		
		\subsubsubsection{Unendliche Summen}
		Zum Beispiel Taylorreihen, Potenzreihen
		
		\subsubsubsection{(Natürlicher) Logarithmus}
		\begin{equation}
		  y = f(x) = ln(x)
		\end{equation}
		Bildet das Inverse der e-Funktion. 
		\begin{figure}[H]
		  \centering
		  \includegraphics[width=0.35\linewidth]{funktionen_e_ln.png}
		  \caption{Grün: $y = e^x$; Rot: $y = ln(x)$}
		  \label{fig:funkt_e_ln}
		\end{figure}
		Zum Wechsel der Basis können die Logarithmusgesetze angewandt werden:
		\begin{align}
		  h(x) &= a^x = e^{x \cdot ln(x)} \text{, da } ln(a^x) = x\;ln(a) \nonumber \\
		  k(x) &= log_a(x) = \frac{ln(x)}{ln(a)}
		\end{align}
		
		\subsubsubsection{Trigonometrische Funktionen}
		\begin{figure}[H]
		  \centering
		  \includegraphics[width=0.35\linewidth]{funktionen_einheitskreis_sin_cos.png}
		  \caption{Grün: $1$; Rot: $sin(x)$; Blau: $cos(x)$}
		  \label{fig:funkt_e_ln}
		\end{figure}
		Aus dem Einheitskreis geht die Identität
		\begin{equation}
		  sin^2(x) + cos^2(x) = 1
		\end{equation}
		direkt hervor.
		\begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.9\linewidth]{funktionen_sin.png}
		  \caption{Sinusfunktion}
		  \label{fig:funkt_sin}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.9\linewidth]{funktionen_cos.png}
		  \caption{Cosinusfunktion}
		  \label{fig:funkt_cos}
		\end{minipage}
		\end{figure}
		Es gelten folgende Zusammenhänge:\newline
		$\;$\newline
			\begin{tabular}{l | l}
				\tabitem $cos(x) = cos(-x)$ & 
				  \tabitem $e^z = e^{Re\lbrace z \rbrace} \left(cos(Im(z))+isin(Im(z))\right) \quad, z \in \C$\\
				\tabitem $-sin(x) = sin(-x)$ & \tabitem $sin(x) = \frac{1}{2i}\left(e^{ix}-e^{-ix}\right) \quad, x \in \R$\\
				\tabitem $cos(x+2 \pi) = cos(x)$ &  \tabitem $cos(x) = \frac{1}{2}\left(e^{ix}+e^{-ix}\right) \quad, x \in \R$\\
				\tabitem $sin(x+2\pi) = sin(x)$ & $\;$
		\end{tabular} \newline
		$\;$ \newline
		Weiterhin gilt:
		\begin{figure}[H]
		  \centering
		  \includegraphics[width=0.35\linewidth]{ableitungskreis.png}
		  \caption{Ableitungsregel}
		  \label{fig:funkt_trigo_abl_1}
		\end{figure}
		\subsubsubsection{Weitere trigonometrische Funktionen}
		\begin{equation}
		  tan(x) = \frac{sin(x)}{cos(x)}, \qquad  csc(x) = \frac{1}{sin(x)}, \qquad  cot(x),\; \dots
		\end{equation}
		\vspace{-0.5cm}
		\begin{figure}[H]
		  \centering
		  \includegraphics[width=0.5\linewidth]{funktionen_tan.png}
		  \caption{$tan(x)$}
		  \label{fig:funkt_tan}
		\end{figure}
		\subsubsubsection{Hyperbolische Funktionen}
		\begin{align}
		  cosh(x) &= \frac{1}{2}\left( e^x + e^{-x}\right)\nonumber \\
		  sinh(x) &= \frac{1}{2}\left( e^x - e^{-x}\right)\nonumber \\
		  tanh(x) &= \frac{sinh(x)}{cosh(x)}\nonumber \\
		  sech,\;coth&,\; \dots ,\; etc.
		\end{align}
		\vspace{-0.7cm}  	  
	  \begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.8\linewidth]{funktionen_cosh.png}
		  \caption{$y = cosh(x)$}
		  \label{fig:funkt_cosh}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.8\linewidth]{funktionen_sinh.png}
		  \caption{$y = sinh(x)$}
		  \label{fig:funkt_sinh}
		\end{minipage}
		\end{figure}
		\vspace{-0.5cm}
		\begin{figure}[H]
		  \centering
		  \includegraphics[width=0.4\linewidth]{funktionen_tanh.png}
		  \caption{$y = tanh(x)$}
		  \label{fig:funkt_tanh}
		\end{figure}
		Es gelten folgende Identitäten:
		\begin{align}
		  \big(cosh(x)\big)' &= sinh(x) \\
		  \big(sinh(x)\big)' &= cosh(x) \\
		  cosh^2(x) - sinh^2(x) &= 1 \\
		  cos(z) &= \frac{1}{2}\left(e^{iz}+e^{iz}\right) = cosh(iz) \quad , z \in \C \\
		  sin(z) &= \frac{1}{2i}\left( e^{iz} - e^{iz}\right) = -isinh(/iz) \quad , z \in \C
		\end{align}
\newpage

\section{Konvergente Folgen}
  \begin{definition}
    Eine Folge in einer Menge $M$ ist eine Abbildung $\N \rightarrow M,\; n\rightarrow a_n$. Wir schreiben $(a_n)_{n \in \N}$. Für $n_j \in \N$ mit $1\leq n_1 < n_2 < n_3 < ...$ heißt $(a_{n_j})_{j\in \N}$ eine Teilfolge von $(a_n)_{n \in \N}$.
  \end{definition}
  Beispiele:
  \begin{align*}
    (a_n)_{n \in \N}\; mit\; a_n = \frac{1}{n} \qquad , 1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4},...\\
    Teilfolge\;a_1, a_5, a_8, a_{17},... \qquad  ,1 \frac{1}{5}, \frac{1}{8}, \frac{1}{17}, ...
  \end{align*}
  \subsection{Vergleich expliziter und rekursiver Darstellung}
  \begin{tabular}{l c l}
    $a_n = (-1)^n ,\quad n \in \N$ & $\rightarrow$ & $a_{n+1} = -a_n,\quad a_1 = -1$\\
    $a_n = n! quad n \in \N$ & $\leftarrow$ & $ a_n = n \cdot a_{n-1}, \quad n \geq 2, \quad a_1 = 1$\\
    $\;$ & $\;$ & $a_1 = 1$ \\
    $\;$ & $\;$ & $a_2 = 2\cdot a_1 = 2 \cdot 1$ \\
    $\;$ & $\;$ & $a_3 = 3\cdot a_2 = 3 \cdot 2 \cdot 1$ \\
  \end{tabular}    

  
  \subsection{Definitionen $\varepsilon-N$-Kriterium/Konvergenzkriterium}
  \begin{definition}
    Es sei $(a_n)_{n \in \N}$ eine Folge in $\R$.
    \begin{itemize}
      \item[a)] Eine Folge $(a_n)_{n \in \N}$ heißt beschränkt, falls es ein $C > 0$ gibt, mit $|a_n| \leq C$ für alle $n \in \N$.
      \item[b)] Eine Folge $(a_n)_{n \in \N}$ heißt konvergent mit Grenzwert (Limes) $a$, falls für alle $\varepsilon > 0$ ein $N\in \N$ existiert, so dass für alle $n \in \N$ mit $n \geq N$ gilt: $|a_n - a| < \varepsilon$. Eine nicht kovergente Folge heißt divergent.
      \begin{equation}
        Konvergenzkriterium:\; \forall \varepsilon > 0 \quad \exists N(\varepsilon) \in \N \quad \forall n \geq N(\varepsilon): |a_n -a| < \varepsilon \label{eq:folge_konvergenz}
      \end{equation}
      \item[c)] Die möglichen Grenzwerte von Teilfolgen heißen Häufungspunkte.
    \end{itemize}\label{def:folge_konvergenz}
  \end{definition}
  \begin{figure}[htbp] 
	  \centering
	  \includegraphics[width=0.9\textwidth]{folge_epsil_krit.png}
	  \caption{Epsilon Kriterium\protect\cite{HM12}}
	  \label{fig:folge_epsilon}
	\end{figure}
  \begin{bem}
    Konvergiert $(a_n)_{n \in \N}$ gegen $a$ schreibt man
    \begin{align}
      &\lim_{n \rightarrow \infty} a_n = a\\
      &bzw. \nonumber \\
      &a_n \underset{n \rightarrow \infty}{\rightarrow} a \nonumber
    \end{align}
  \end{bem}
  \begin{bem}
    Die Schranke $N$ ist i.A. von $\varepsilon$ abhängig. Daher schreibt man $N(\varepsilon)$.
  \end{bem}
  \begin{bem}
    Der Grenzwert einer Folge ist eindeutig.
  \end{bem}
  
  \subsection{Ausdrücke mit Brüchen}
  $(a_n)$ mit $a_n = \frac{P(n)}{Q(n)}$ mit $P,Q$ sind Polynome.
  \begin{align*}
    \textbf{(1)}\quad grad(P) > grad(Q) &\Rightarrow (a_n) \text{ divergiert} \\
    \textbf{(2)}\quad grad(P) < grad(Q) &\Rightarrow (a_n) \text{ konvergiert gegen } 0\\
    \textbf{(3)}\quad grad(P) = grad(Q) &\Rightarrow (a_n) \text{ konvergiert gegen Quotient der Leitkoeffizienten}
  \end{align*}   
  
  \subsection{Dreiecksungleichung}
  \begin{align}
    |x+y| &\leq |x| + |y| \qquad ,\; x,y \in \R \\
    \text{bzw. in umgekehrter Form} \nonumber \\
    \big| |x| - |y| \big| &\leq |x-y|
  \end{align}
  
  \subsection{Beschränktheit}
  \begin{satz}
    Eine konvergente reelle Folge ist beschränkt.
  \end{satz}
  \begin{figure}[H] 
	  \centering
	  \includegraphics[width=0.3\textwidth]{beschraenkt.jpg}
	  \caption{derp derp derp}
	  \label{fig:folge_epsilon}
	\end{figure}
  
  \subsection{Grenzwertsätze}
  \begin{satz}
    Seien $(a_n)$, $(b_n)$ konvergente Folgen. Dann konvergieren auch die Folgen $\big(|a_n|\big)$, $(a_n + b_n)$ und $(\lambda a_n)$ für $\lambda \in \R$, und es gilt:
    \begin{itemize}
      \item[i) ] ~\\[-28pt]
      \begin{equation}
        \lim|a_n| = |\lim a_n|
      \end{equation}
      \item[ii) ] ~\\[-28pt]
      \begin{equation}
        \lim (a_n \pm b_n) = \lim a_n \pm \lim b_n
      \end{equation}       
      \item[iii) ] ~\\[-28pt]
      \begin{equation}
        lim (\lambda a_n) = \lambda \lim a_n
      \end{equation}
    \end{itemize}
  \end{satz}
  \begin{satz}
    Seien $(a_n)$, $(b_n)$ konvergente Folgen.
    \begin{itemize}
      \item[iv) ] Dann konvergiert auch $(a_n b_n)$ und es gilt\newline
      \begin{equation}
        lim (a_n b_n) = (\lim a_n)(\lim b_n)
      \end{equation}
      \item[v) ] Ist $b_n \neq 0$ für alle $n \in \N$ und $b \neq 0$, so konvergiert auch $\left( \frac{a_n}{b_n} \right)$ mit
      \begin{equation}
        \lim \left( \frac{a_n}{b_n} \right) = \frac{\lim a_n}{\lim b_n}
      \end{equation}
      \item[vi) ] Sei $m \in \N$. Ist $a_n \geq 0$ für alle $n \in \N$, dann konvergiert auch $\big( \sqrt[m]{a_n}\big)$ und es gilt
      \begin{equation}
        \lim \sqrt[m]{a_n} = \sqrt[m]{\lim a_n}
      \end{equation}
    \end{itemize}
  \end{satz}
  \begin{bem}
    Bei rationalen Ausdrücken mit der höchsten Potenz durchkürzen. Beispiel:
    \begin{equation*}
      \frac{n}{n^2+4n+8} = \frac{\frac{1}{n}}{1+\frac{4}{n}+\frac{8}{n^2}} \rightarrow \frac{0}{1}=0
    \end{equation*}
  \end{bem}
  
	  \subsubsection{Beispiele wichtiger Grenzwerte}
	  \begin{itemize}
	    \item[1) ] $\lim \frac{1}{n} = 0$
	    \item[2) ] Sei $q \in \R$ mit $|q| < 1$:\newline
	      $\lim q^n = 0$
	    \item[3) ] Sei $q \in \R$ mit $|q| < 1$ und $p \in \Z$:\newline
	      $\lim n^p q^n = 0$
	    \item[4) ] Sei $a \in \R$:\newline
	      $\lim \frac{a^n}{n!} = 0$
	    \item[5) ] $\;$\newline
	      $\lim\limits_{n \rightarrow \infty} \frac{n^k}{a^n} = 0$
	    \item[6) ] $\;$\newline
	      $\lim\limits_{n \rightarrow \infty} \sqrt[n]{c} = 1$
	    \item[7) ] $\;$\newline
	      $\lim\limits_{n \rightarrow \infty} \sqrt[n]{n} = 1$
	  \end{itemize}
	  
	\subsection{Konvergenzsätze}
	\begin{satz} $\;$\newline
	  \begin{itemize}
	    \item[1) ] Der Grenzwert einer konvergenten Folge ist eindeutig.
	    \item[2) ] Jede konvergente Folge ist beschränkt.
	    \item[3) ] Jede monotone und beschränkte Folge ist konvergent.
	      \begin{itemize}
	        \item[a) ]$(a_n)$ ist monoton wachsend und nach oben beschränkt 
	        \begin{equation*}
	          \Rightarrow \lim\limits_{n \rightarrow \infty} a_n = \sup\limits_{n \in \N} (a_n)
	        \end{equation*}
	        \item[b) ]$(a_n)$ ist monoton fallend und nach unten beschränkt 
	        \begin{equation*}
	          \Rightarrow \lim\limits_{n \rightarrow \infty} a_n = \inf\limits_{n \in \N} (a_n)
	        \end{equation*}
	      \end{itemize}
	  \end{itemize}
  \end{satz}  
  
  \subsection{Monotene Folgen}
  \begin{definition}
    Eine reelle Folge $(a_n)_{n \in \N}$ heißt monoton wachsend, falls $a_n \leq a_{n+1}$ für alle $n \in \N$. Sie heißt streng monoton wachsend, falls $a_n < a_{n+1}$ für alle $n \in \N$. Analog monoton fallend und streng monoton fallend.
  \end{definition}
  \begin{satz}
    Ist die Folge $(a_n)_{n \in \N}$ monoton wachsend und nach oben beschränkt, so konvergiert die Folge und es gilt
    \begin{equation}
      \lim_{n\rightarrow \infty} a_n = \sup \lbrace a_n : n\in \N \rbrace
    \end{equation}
  \end{satz}
  \begin{figure}[htbp] 
	  \centering
	  \includegraphics[width=0.9\textwidth]{folge_monoton_beschraenkt.png}
	  \caption{Konvergenz mw u. beschr. Folgen\protect\cite{HM12}}
	  \label{fig:folge_mw_beschr}
	\end{figure}
	
		\subsubsection{Intervallschachtelung}
		Es seien $(a_n)_{n \in \N}$ und $(b_n)_{n \in \N}$ reelle Folgen.
		$(a_n)_{n \in \N}$ sei monoton wachsend und $(b_n)_{n \in \N}$ sei monoton fallend. Es gelte $(a_n)_{n \in \N} \leq (b_n)_{n \in \N}$ für alle $n \in \N$.
    Dann gilt 
    \begin{equation*}
      \lim_{n \rightarrow \infty} a_n,\quad \lim_{n \rightarrow \infty} b_n \quad ex.
    \end{equation*}
    und
    \begin{equation}
      \lim_{n \rightarrow \infty} (b_n - a_n) = 0 \Rightarrow \lim_{n \rightarrow \infty} a_n = \lim_{n \rightarrow \infty} b_n
    \end{equation}
\newpage

\section{Ableitung}
  \subsection{Differenzenquotient/Differentialquotient}
  \begin{definition}
    Die Änderung einer Funktion heißt Differenzenquotient:
    \begin{equation}
      \frac{f(x_n)-f()x_0)}{x_n - x_o}
    \end{equation}
  \end{definition}
  \begin{definition}
    Gegeben sei $f: I\rightarrow \R$ und ein Punkt $x_o \in I$. Die Funktion $f$ heißt differenzierbar in $x_0$, falls der Grenzwert
    \begin{equation}
      \lim_{x\rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0} = \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h}
    \end{equation}
    existiert. Der Grenzwert heißt der Differentialquotient bzw. die Ableitung von $f$ in $x_0$ und wird mit $f'(x_0)$ bzw. $\diff{f}{x}(x_0)$ bezeichnet.
  \end{definition}
  \begin{bem}
    Die Ableitung $f'(x_0)$ gibt die Steigung der Tangente an $f$ in $x_0$ an. Die Tangentengleichung ist:
    \begin{equation}
      l(x) = f(x_0) + f'(x_0)(x-x_0)
    \end{equation}
    Es gilt:
    \begin{align}
      &l(x_0) = f(x_0) + f'(x_0)(x_0 - x_0) = f(x_0) \\
      und\nonumber \\
      &l'(x_0) = f'(x_0)
    \end{align}
    d.h. der Funktionswert und die Ableitung von $f$ und $l$ stimmen in $x_0$ überein.
  \end{bem}
  \subsection{Stetigkeit}
  \begin{definition}
    Eine Funktion $f$ heißt stetig in $x_0$, falls 
    \begin{equation}
      \lim_{x \rightarrow x_0} f(x) = f(x_0)
    \end{equation}
  \end{definition}
  \begin{satz}
    Jede in $x_0$ differenztierbare Funktion ist auch stetig in $x_0$  
  \end{satz}
  \begin{bem}
    Merkregel:\newline
    Stetig: $f$ kann durchgezeichnet werden.\newline
    Diffbar: $f$ hat keinen Knick
  \end{bem}
  Beispiel:\newline
  $f(x) = |x|$ ist in $x_0 = 0$ stetig aber nicht diffbar.
  \begin{bem}
    Bezeichnungen:
    \begin{itemize}
      \item $C^0$: Menge aller stetigen Funktionen
      \item $C^1$: Menge aller diffbaren Funktionen mit $f,f'$ stetig
      \item $C^n$: Menge der n-mal stetig diffbaren Funktionen, d.h. $f, f', \dots, f^{(n)}$
      \item $C^{\infty}$: Menge aller unendlich oft stetig diffbaren Funktionen
    \end{itemize}
  \end{bem}
  \subsection{Wichtige Ableitungen}
  \begin{equation}
    f(x) = x^n \rightarrow f'(x) = nx^{n-1}
  \end{equation}
  \begin{equation}
    f(x) = e^x \Rightarrow f'(x) = e^x
  \end{equation}
  \begin{equation}
    f(x) = ln(x) \Rightarrow f'(x) = \frac{1}{x}
  \end{equation}
  \begin{equation}
    f(x) = arctan(x) \Rightarrow f'(x) = \frac{1}{(tan(y))'}
  \end{equation}
  \begin{equation}
    f(x) = arcsin(x) \Rightarrow f'(x) = \frac{1}{(sin(y))'}
  \end{equation}

  \subsection{Ableitungsregeln}
  \begin{satz} Produkt-/Quotientenregel\newline
    \vspace{-0.5cm}
    \begin{itemize}
      \item[a) ] Seien $\alpha ,\beta \in \R$, dann gilt
        \begin{equation}
          (\alpha f + \beta g)'(x) = \alpha f'(x) + \beta g'(x) \text{\colBlue{(Linearität der Ableitung)}}
        \end{equation}
      \item[b) ] ~\\[-28pt]
        \begin{equation}
          (fg)'(x) = f'(x)\cdot g(x)+f(x) \cdot g'(x)
        \end{equation}
      \item[c) ] ~\\[-28pt]
        \begin{equation}
          (\frac{f}{g})'(x) = \frac{f'(x)g(x)-g'(x)f(x)}{\big(g(x)\big)^2}
        \end{equation}
    \end{itemize}
  \end{satz}
  \begin{satz} Kettenregel\newline
    \vspace{-0.5cm}
    \begin{itemize}
      \item[a) ]Es gilt:
      \begin{equation}
        (g \circ f)'(x) = g' \big(f(x)\big) \cdot f'(x)
      \end{equation}
      \item[b) ] Es sei $f: [a,b] \rightarrow \R$ streng monoton wachsend mit $f'(x_0) \neq 0$ und diffbar. Dann existiert die Umkehrfunktion $f^{-1}:[f(a), f(b)] \rightarrow \R$ und besitzt die Ableitung
      \begin{equation}
        \left( f^{-1}\right)'(y_0) = \frac{1}{f'(x_0)}
      \end{equation}
      mit $y_0 = f(x_0)$.
    \end{itemize}
  \end{satz}
  \newpage
  
  \subsection{Mittelwertsätze}  
  \begin{definition}
    Es sei $f:(a,b)\rightarrow \R$ und $x_0 \in (a,b)$.
    \begin{itemize}
      \item[a) ]$f(x)$ hat in $x_0$ ein lokales Maximum, falls es ein $\varepsilon > 0$ gibt mit $f(x) \leq f(x_0)$ für alle $x \in (a,b)$ mit $(x-x-0) < \varepsilon$.
      \item[b) ] Analog für lokales Minimum. 
    \end{itemize}
  \end{definition}
  \vspace{-0.5cm}
	  \begin{figure}[H]
	  \centering
	  \includegraphics[width=0.3\textwidth]{mws_def.png}
	  \caption{Lokales Minimum/Maximum\protect\cite{HM12}}
	  \label{fig:mws_min_max}
	\end{figure}
  \begin{satz}
    Besitzt eine stetig diffbare Funktion $f: (a,b) \rightarrow \R$ in einer Stelle $x_0 \in (a,b)$ ein lokales Maximum oder Minimum, so gilt notwendigerweise $f'(x_0) = 0$.
  \end{satz}
  \begin{satz}
    Seien $f,g:[a,b] \rightarrow \R$ stetig und auf $(a,b)$ diffbar mit $g'(x) > 0$ bzw. $g'(x) <0$ jeweils für alöle $x \in (a,b)$. 
    \begin{itemize}
      \item[a) ] \textbf{Satz von Rolle:} Gilt $f(a) = f(b)$, so existiert ein $\xi \in (a,b)$ mit $f'(\xi) = 0$.
      \item[b) ] \textbf{1. Mittelwertsatz:} Es gibt ein $\xi \in (a,b)$ mit $f'(\xi) = \frac{f(b) - f(a)}{b-a}$.
      \item[c) ] \textbf{2. Mittelwertsatz:} Es gibt ein $\xi \in (a,b)$ mit $\frac{f'(\xi)}{g'(\xi)} = \frac{f(b) - f(a)}{g(b) - g(a)}$.
    \end{itemize}
  \end{satz}
  \begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.8\linewidth]{mws_rolle.png}
		  \caption{Zu a) \protect\cite{HM12}}
		  \label{fig:mws_rolle}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.8\linewidth]{mws_1.png}
		  \caption{Zu b) \protect\cite{HM12}}
		  \label{fig:funkt_sinh}
		\end{minipage}
  \end{figure}
		
  \subsection{Monotonie}	
  \begin{satz}$\;$ \newline
  \vspace{-0.5cm}
    \begin{itemize}
      \item[a) ] Ist $f:(a,b)\rightarrow \R$ differenzierbar und ist $f:[a,b]\rightarrow \R$ stetig und es gelte $f'(x_0) = 0 \quad \forall x \in (a,b)$, so ist $f$ konstant.
      \item[b) ] 
      \begin{tabular}{l c l} 
        $f'(x) \geq 0 \quad \forall x \in (a,b)$ & $\Leftrightarrow$ & $f$ ist monoton wachsend \\
        $f'(x) > 0 \quad \forall x \in (a,b)$ & $\Rightarrow$ & $f$ ist streng monoton wachsend \\
        $f'(x) \leq 0 \quad \forall x \in (a,b)$ & $\Leftrightarrow$ & $f$ ist monoton fallend \\
        $f'(x) < 0 \quad \forall x \in (a,b)$ & $\Rightarrow$ & $f$ ist streng monoton fallend \\
      \end{tabular}
    \end{itemize}
  \end{satz}	
  
  \subsection{Das Prinzip von l'Hospital}
  \begin{satz}
    Gilt $\lim\limits_{b\rightarrow a} f(b) = 0$, $\lim\limits_{b\rightarrow a} g(b) = 0$ und existiert $\lim\limits_{b\rightarrow a} \frac{f'(b)}{g'(b)}$, so existiert auch $\lim\limits_{b\rightarrow a} \frac{f(b)}{g(b)}$ und er ist gleich $\lim\limits_{b\rightarrow a} \frac{f'(b)}{g'(b)}$. Analog $\lim\limits_{b\rightarrow a} f(b) = \infty$ und $\lim\limits_{b\rightarrow a} g(b) = \infty$.
    \begin{equation}
      \Rightarrow \lim\limits_{b\rightarrow a} f(b) = 0 \land \lim\limits_{b\rightarrow a} g(b) = 0 \land \lim\limits_{b\rightarrow a} \frac{f'(b)}{g'(b)} \Rightarrow \lim\limits_{b\rightarrow a} \frac{f'(b)}{g'(b)} = \lim\limits_{b\rightarrow a} \frac{f(b)}{g(b)}
    \end{equation}
  \end{satz}
  \begin{bem}
    Zur Berechnung muss der Ausdruck gegebenenfalls auf $"\frac{0}{0}"$ zurückgeführt werden. Entspricht zum Beispiel $\frac{f(b)}{g(b)}$ der Form $"\frac{\infty}{\infty}"$ kann durch Umformung zu $\frac{\frac{1}{f(b)}}{\frac{1}{g(b)}}$ l'Hospital angewandt werden.\newline
    Liegt ein Ausdruck in der Form $"\infty \cdot 0"$ wie zum Beispiel $\lim\limits_{x\rightarrow \infty} \underbrace{x}_{\rightarrow \infty} \underbrace{ln\left(\frac{x+1}{x-1}\right)}_{\rightarrow 0 }$ vor, kann durch Umformung zu $\lim\limits_{x\rightarrow \infty} \frac{ln\left(\frac{x+1}{x-1}\right)}{\left(\frac{1}{x}\right)}$ wieder in die Form $"\frac{0}{0}"$ gebracht werden.
  \end{bem}
  \newpage
  
\section{Taylorpolynome}
  \begin{satz}
  Sei $f:I\rightarrow \R$ eine $C{n+1}$ Funktion und sei $x_0 \in I$ und heißt Entwicklungspunkt. Dann ist
  \begin{itemize}
    \item[a) ] das Taylorpolynom n-ten Grades zum Entwicklungspunkt $x_0$ gegebn durch
    \begin{equation}
    T_n(f, x, x^*) = \sum\limits_{k= 0}^n \frac{f^{(k)}(x^*)}{k!} (x-x^*)^k
    \end{equation}
    und stimmt mit $f$ in $x_0$ bis zur n-ten Ableitung überein.
    \item[b) ] Der Fehler 
    \begin{equation}
      R_n(x,x_0) = f(x) - T_n(x,x_0)
    \end{equation}
    ist gegeben durch die Restgliedformel nach Lagrange
    \begin{equation}
      R_n(x,x_0) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}
    \end{equation}
    wobei $\xi = x_0 + \Theta (x-x_0)$ mit $\Theta \in (0,1)$, d.h. $\xi$ ist eine Zahl zwischen $x$ und $x_0$. Es gilt dann
    \begin{equation}
      \big|R_n(x,x_0)\big| = \frac{\sup\limits_{\xi \in (x_0,x)} \left|f^{(n+1)}(\xi)\right|}{(n+1)!}|x-x_0|^{(n+1)}
    \end{equation}
  \end{itemize}
  \end{satz}  
\newpage

\section{Reihen}
  \subsection{Satz von Bolzano-Weierstraß (Kompaktheitssatz)}
  \begin{satz}
    Jede beschränkte reelle Folge $(a_n)_{n \in \N}$ besitzt eine konvergente Teilfolge.
  \end{satz}
  \begin{bem}
    Im Prinzip unterteilt man das Intervall jeweils mittig und erhält so eine Folge von Teilintervalle mit je unendlich vielen Folgengliedern. Daraus konstruiert man eine konvergente Teilfolge
  \end{bem}  
  \begin{bem}
    Der Satz gilt auch im $\R^d$ aber nicht im $\R^{\infty}$.
  \end{bem}
  \begin{definition}
    $x$ heißt Häufungspunkt einer Folge $(a_n)_{n \in \N}$, wenn eine Teilfolge existiert, die gegen $x$ konvergiert.
  \end{definition}
  Formuliert man den Satz von Bolzano Weierstraß dahingehen um, lautet er
  \begin{bem}
    Jede beschränkte reelle Folge besitzt mindestens einen Häufungspunkt.
  \end{bem}
  \begin{definition}$\;$\newline 
    limes inferior$\;$: $\liminf\limits_{n\rightarrow \infty} a_n \;= \inf\lbrace x:\; x$ ist ein Häufungspunkt von $(a_n)_{n \in \N}\rbrace$ \newline
    limes superior: $\limsup\limits_{n\rightarrow \infty} a_n = \sup\lbrace x:\; x$ ist ein Häufungspunkt von $(a_n)_{n \in \N}\rbrace$
  \end{definition}
  
  \subsection{Häufungspunkte}
  \begin{bem}
    Ein Häufungspunkt $a$ einer Folge $(a_n)$ ist der Grenzwert einer Teilfolge $(a_{n_k})$.
    \begin{equation*}
      \lim\limits_{k \rightarrow \infty} a_{n_k} = a
    \end{equation*}
    \begin{tabular}{l c l}
      Größter HP&: & $\limsup_{n \rightarrow \infty} a_n = \overline{\lim\limits_{n \rightarrow \infty}} a_n$ \\
      Kleinster HP&: & $\liminf_{n \rightarrow \infty} a_n = \underline{\lim\limits_{n \rightarrow \infty}} a_n$
    \end{tabular} \newline
    Es gilt:
    \begin{align*}
    \textbf{(1)}\quad \overline{\lim\limits_{n \rightarrow \infty}} a_n &= \underline{\lim\limits_{n \rightarrow \infty}} a_n \Rightarrow a_n \text{ konvergiert} \\
    \textbf{(2)}\quad \lim\limits_{n \rightarrow \infty} a_n  &\rightarrow \text{ jede Teilfolge } a_{n_k} \text{ konvergiert.}
    \end{align*}
  \end{bem}  
  
  \subsection{Cauchy-Folgen}
  \begin{definition}
    Eine Folge $(a_n)_{n \in \N}$ heißt Cauchy-Folge, falls für alle $\varepsilon > 0$ ein $N \in \N$ \colRed{(im Script steht $n \in \N$, aber irgendwie unlogisch)} existiert, sodass für alle $m,n \in \N$ gilt: $|a_n - a_m|<\varepsilon$.
    \begin{equation}
      \forall \varepsilon > 0:\; \exists N\quad \forall n,m \geq N: |a_n -a_m| < \varepsilon 
    \end{equation}
    (vgl. Konvergenzkriterium \eqref{def:folge_konvergenz} bzw. \eqref{eq:folge_konvergenz}).
  \end{definition}
  \begin{satz}
    Jede konvergente Folge ist auch eine Cauchy-Folge.
  \end{satz}
  \begin{satz}
    In den reellen Zahlen gilt: Jede Cauchy-Folge inst konvergent.
  \end{satz}
  \begin{bem}
    Jede Cauchy-Folge ist beschränkt (somit ist Bolzano Weierstraß anwendbar).
  \end{bem}
  Die Konsequenzen aus dem Beweis der obigen Aussagen sind:
  \begin{bem} $\;$\newline \vspace{-0.5cm}
    \begin{itemize}
      \item Um Konvergenz nachzuweisen reicht der Nachweis einer Cauchy-Folge (Cauchy-Kriterium)
      \item Supremumsvollständigkeit $\Rightarrow$ Cauchyfolgenvollständigkeit
    \end{itemize}
  \end{bem}
  
  \subsection{Reihen reeller Zahlen}
  \begin{definition}
    Bildet man zu einer gegebenen Folge $(a_n)_{n \in \N}$ mit $a_n \in \R$ eine neue Folge $(s_n)_{n \in \N}$ mit 
    \begin{equation}
      s_n = \sum\limits_{k=0}^n a_k
    \end{equation}
    so wird die Folge $(s_n)_{n \in \N}$ eine Reihe genannt. Die einzelnen $s_n$ heißen Partialsummen der Reihe. Ist $(s_n)_{n \in \N}$ konvergent, so wird der Grenzwert $s = \lim\limits_{n\rightarrow \infty} \sum\limits_{k = 0}^n a_k$ mit $\sum\limits_{k=0}^{\infty} a_k$ bezeichnet. Oft wird $\sum\limits_{k=0}^{\infty} a_k$ als Abkürzung für $(s_n)_{n \in \N}$ verwendet.
  \end{definition}
  
  \begin{satz} $\;$\newline \vspace{-0.5cm}
  \begin{itemize}
  \item[a) ] Es gilt das Cauchysche Konvergenzkriterium.
    \begin{equation}
      \sum\limits_{k=0}^{\infty} a_k \; konvergent \; \Leftrightarrow \forall \varepsilon > 0 \quad \exists N \quad \forall n,m \geq N: \quad \left| \sum\limits_{k = n+1}^m a_k \right| < \varepsilon
    \end{equation}
    \item[b) ] Notwendige aber nicht hinreichende Bedingung für die Konvergenz von $\sum\limits_{k=0}^{\infty} a_k$ ist, dass $a_k$ eine Nullfolge ist, also das gilt: $\lim\limits_{k \rightarrow \infty} a_k = 0$
    \item[c) ] Sind 
    \begin{equation*}
      \sum\limits_{k=0}^{\infty} a_k, \qquad \sum\limits_{k=0}^{\infty} b_k
    \end{equation*}
    konvergente Reihen, so konvergieren auch
    \begin{align*}
    \sum\limits_{k=0}^{\infty} \left( a_k + b_k\right) &= \left(\sum\limits_{k=0}^{\infty} a_k\right) + \left(\sum\limits_{k=0}^{\infty} b_k\right) \\
    \sum\limits_{k=0}^{\infty} \left( \lambda a_k\right) &= \lambda \sum\limits_{k=0}^{\infty} a_k
    \end{align*}
  \end{itemize}
  \end{satz}
  
	  \subsubsection{Wichtige Reihen}
	  \begin{itemize}
	    \item Exponentialreihe:
	    \begin{equation}
	      e^x = \sum\limits_{k=0}^\infty \frac{x^k}{k!} \qquad \forall x \in \R
	    \end{equation}
	    \item Geometrische Reihe
	    \begin{equation}
	      \frac{1}{1-q} = \sum\limits_{k = 0}^\infty q^k \qquad |q| < 1
	    \end{equation}
	    \item Sinusreihe
	    \begin{equation}
	      sin(y) = \sum\limits_{k = 0}^\infty \frac{(-1)^k y^{2k+1}}{(2k+1)!} = y -\frac{1}{3!}y^3 + \frac{1}{5!}y^5 + ...
	    \end{equation}
	    \item Cosinusreihe
	    \begin{equation}
	      cos(y) = \sum\limits_{k = 0}^\infty = (-1)^k \frac{y^2k}{(2k)!} = 1 - \frac{1}{2}y^2 + \frac{1}{4!}y^4 + ...
	    \end{equation}
	  \end{itemize}
	  
	  \subsubsection{Konvergenzen und Divergenzen ausgewählter Reihen}
	  \begin{itemize}
	    \item Geometrische Reihe \newline
	    Konvergenz kann nur für $|q| < 1$ vorliegen da $a_k$ eine Nullfolge sein muss. Sonst divergent.
	    \item Harmonische Reihe \newline
	    \begin{equation}
	      \lim\limits_{n \rightarrow \infty} \sum\limits_{k = 0}^n \frac{1}{k} = \infty
	    \end{equation}
	  \end{itemize}
	  
	  \subsubsection{Leibnizkriterium für alternierende Reihen}
	  \begin{satz}
	    Ist $a_k \geq 0$ und ist $(a_k)_{k \in \N}$ monoton fallend, oder anders ausgedrückt ist $(a_k)_{k \in \N}$ eine monoton fallende Nullfolge, so ist
	    \begin{equation*}
	      \sum\limits_{k = 1}^{\infty} (-1)^k a_k
	    \end{equation*}
	    konvergent.
	  \end{satz}
  
  \subsection{Absolut konvergente Reihen}
  \begin{definition}
    Eine Reihe $\sum\limits_{k=0}^\infty$ heißt absolut konvergent, falls die Reihe $\sum\limits_{k=0}^\infty |a_k|$ konvergiert.
  \end{definition}
  \begin{satz}
    Jede absolut konvergente Reihe ist konvergent.
  \end{satz}
	  \subsubsection{Konvergenzkriterien}
	  \begin{satz}
	    \begin{itemize}
	      \item[a) ] Die absolute Konvergenz von $\sum\limits_{k=0}^\infty a_k$ ist äquivalent zur Beschränktheit von $\left(\sum\limits_{k=0}^\infty a_k \right)_{k\in \N}$.
	      \item[b) ] Majorantenkriterium \newline
	      Sei $|a_k| \leq b_k$ und $\sum\limits_{k=0}^\infty b_k$ konvergent, so folgt die absolute Konvergenz von $\sum\limits_{k=0}^\infty a_k$. Die Reihe $\sum\limits_{k=0}^\infty b_k$ heißt konvergente Majorante. 
	      \item[c) ] Divergenzkriterium \newline
	      Sei $0 \leq a_k \leq b_k$ und $\sum\limits_{k=0}^\infty a_k$ divergent (d.h. = $\infty$). Dann ist auch $\sum\limits_{k=0}^\infty b_k$ divergent und $\sum\limits_{k=0}^\infty a_k$ heißt divergente Minorante.
	    \end{itemize} \label{ax:reihe_konv_a}
	  \end{satz}
	  \begin{bem}
	    Zu \eqref{ax:reihe_konv_a} b): Häufig wird die geometrische Reihe als konvergente Majorante genutzt.
	  \end{bem}
	  \begin{satz}
	    Vergleichsreihe:
	    \begin{equation}
	      \sum\limits_{k = 1}^\infty \frac{1}{k^\alpha} 
	      \begin{cases}
	        \text{konvergiert für }\alpha > 1 \\
	        \text{divergiert für  }\;\;\alpha \leq 1
	      \end{cases}
	    \end{equation}
	  \end{satz}
	  \begin{satz}
	    Wurzelkriterium: Gegeben sei die Reihe $\sum\limits_{k=1}^\infty a_k$. Man setzt $\alpha = \limsup_{k \rightarrow \infty} |a_k|^{\frac{1}{k}}$.
	    \begin{itemize}
	      \item[a) ] Ist $\alpha < 1$, so konvergiert $\sum\limits_{k=1}^\infty a_k$ absolut.
	      \item[b) ] Ist $\alpha > 1$, so divergiert $\sum\limits_{k=1}^\infty a_k$.
	      \item[c) ] Ist $\alpha = 1$, so ist keine Aussage möglich.
	    \end{itemize}\label{ax:folgen_konv_wurzel}
	  \end{satz}
	  \begin{satz}
	    Quotientenkriterium: Gegeben sei $\sum\limits_{k=1}^\infty a_k$ mit $a_k \neq 0$ für $k \in \N$. Wir setzen $\underline{\alpha} = \liminf\limits_{k \rightarrow \infty} \left| \frac{a_{k+1}}{a_k}\right|$ und $\overline{\alpha} = \limsup\limits_{k \rightarrow \infty} \left| \frac{a_{k+1}}{a_k}\right|$.
	    \begin{itemize}
	      \item[a) ] Ist $\overline{\alpha} < 1$, so konvergiert $\sum\limits_{k=1}^\infty a_k$ absolut.
	      \item[b) ] Ist $\underline{\alpha} > 1$, so divergiert $\sum\limits_{k=1}^\infty a_k$.
	    \end{itemize}\label{ax:folgen_konv_quot}
	  \end{satz}
    \begin{bem}
      Das Wurzelkriterium \eqref{ax:folgen_konv_wurzel} wird häufig bei n-ten Wurzeln angewandt, das Quotientenkriterium \eqref{ax:folgen_konv_quot} häufig bei Fakultäten.
    \end{bem}
  \subsection{Umordnung von Reihen}
  \begin{satz}
    Umordnung absolut konvergenter Reihen: Sei $\sigma: \N_0 \rightarrow \N_0$ eine bijektive Abbildung (erzeugt Umnummerierung der Folgenglieder). Ist $\sum\limits_{k=0}^\infty a_k$ absolut konvergent, so ist auch jede umgeordnete Reihe $\sum\limits_{k=0}^\infty a_{\sigma_k}$ absolut konvergent und es gilt
    \begin{equation}
      \sum\limits_{k=0}^\infty a_k = \sum\limits_{k=0}^\infty a_{\sigma_k}
    \end{equation}
  \end{satz}
  \begin{satz}
    Seien $\sum\limits_{k=0}^\infty a_k$ und $\sum\limits_{k=0}^\infty b_m$ absolut konvergent. Dann ist die Reihe $\sum\limits_{k=0}^\infty a_{\sigma_k} b_{\mu_k}$ für jede Nummerierung: $(\sigma,\mu):(\N_0 \rightarrow \N_0^2$ mit $k \rightarrow (\sigma_k, \mu_k)$ absolut konvergent und es gilt
    \begin{equation}
      \sum\limits_{k=0}^\infty a_{\sigma_k} b_{\mu_k} = \left(\sum\limits_{k=0}^\infty a_k \right) =  \left(\sum\limits_{k=0}^\infty b_m \right)
    \end{equation}
  \end{satz}
  
  \subsection{Potenzreihen}
  \begin{definition}
    Eine Reihe der Form $\sum\limits_{k = 0}^\infty a_k(z-z_0)^k$ heißt Potenzreihe.
  \end{definition}
	  \subsubsection{Taylorreihe}
	  Ist eine Funktion unendlich oft diffbar kann aus dem Taylorpolynom $T_n(x,x_0)$ für $n\rightarrow \infty$ die sogenannte Taylorreihe
	  \begin{equation}
	   T_\infty (x,x_0) = \sum\limits_{k = 0}^\infty \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k
	  \end{equation}
	  erhalten werden.
	  \begin{satz}
	    Die Taylorreihe $T_\infty(f,x,x_0)$ konvergiert in $x \in \R$ genau dann gegen $f(x)$, falls
	    \begin{equation*}
	      \lim\limits_{n \rightarrow \infty} R_n(f,x,x_0) = 0
	    \end{equation*}
	  \end{satz}
	  \begin{satz}
	    Identitätssatz:
	    \begin{align}
	      \text{Taylorreihe: } f(x) &= \sum\limits_{k = 0}^\infty \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k \nonumber \\
	      \text{Potenzreihe: } f(x) &= \sum\limits_{k = 0}^\infty a_k (x-x_0)^k \nonumber \\
	      &\Rightarrow a_k = \frac{f^{(k)}x_0}{k!}
	    \end{align}
	  \end{satz}
	  \subsubsection{Konvergenz der Potenzreihen}
	  \begin{satz}$\;$ \newline \vspace{-0.5cm}
	    \begin{itemize}
	      \item[a) ] Zu jeder Potenzreihe $\sum\limits_{k = 0}^\infty a_k(z-z_0)^k$ gibt es eine Zahl $r \in [0,\infty]$, den sogenannten Konvergenzradius der Potenzreihe, mit der Eigenschaft, dass $\sum\limits_{k = 0}^\infty a_k(z-z_0)^k$ absolut konvergent für $|z-z_0| < r$ ist und divergent für $|z-z_0| > r$.
	      \item[b) ] Für den Konvergenzradius gilt die Formel von Cauchy-Hadamard:
	      \begin{equation}
	        r = \frac{1}{\limsup\limits_{k \rightarrow \infty} |a_k|^\frac{1}{k}}
	      \end{equation}
	      wobei $\frac{1}{\infty}=0$ und $\frac{1}{0} = \infty$ gesetzt wird.
	    \end{itemize}
	    Dazu:
	    \begin{figure}[H] 
				\centering
				\begin{minipage}{.5\textwidth}
				  \centering
				  \includegraphics[width=0.8\linewidth]{reihen_potenzreihen_r.png}
				  \caption{Potenzradius in $\R$ \protect\cite{HM12}}
				  \label{fig:reihe_potenzradius_r}
				\end{minipage}%
				\begin{minipage}{.5\textwidth}
				  \centering
				  \includegraphics[width=0.55\linewidth]{reihen_potenzreihen_c.png}
				  \caption{Potenzradius in $\C$ \protect\cite{HM12}}
				  \label{fig:reihe_potenzradius_c}
				\end{minipage}
      \end{figure}
	  \end{satz}
    \begin{bem}
      Falls einer der folgenden Grenzwerte existiert bzw. $\infty$ ist, ist er gleich dem Konvergenzradius.
      \begin{equation*}
        r = \frac{1}{\lim |a_k|^\frac{1}{k}} \quad bzw. \quad r = \lim\limits_{k \rightarrow \infty} \left| \frac{a_k}{a_{k+1}}\right|
      \end{equation*}
    \end{bem}	  
    \begin{bem}
      Da absolute Konvergenz vorliegt, können Potenzreiuhen miteinander multipliziert werden. Der Konvergenzradius ist mindestens so groß wie das Minimum der Konvergenzradien.
    \end{bem}
	  \begin{bem}
	    Potenzreihen sind innerhalb ihres Konvergenzradius beliebig oft diffbar und können gliedweise abgeleitet werden.
	  \end{bem}
	  \begin{satz}
	    Die formal abgeleitete Reihe $\sum\limits_{k = 1}^\infty a_k kx^{k-1}$ hat den gleichen Konvergenzradius wie die Ausgangsreihe.
	  \end{satz}
	  \begin{satz}
	    Es sei $\sum\limits_{k=0}\infty a_k x^k$ eine Potenzreihe mit Konvergenzradius $1$. Ist die Reihe auch in $x = 1$ konvergent, d.h. konvergiert $\sum\limits_{k = 0}^\infty$, so gilt
	    \begin{equation}
	      \lim\limits_{x\rightarrow 1} \sum\limits_{k = 0}^\infty a_k x^k = \sum\limits_ {k = 0}^\infty a_k
	    \end{equation}
	  \end{satz}
\newpage	  
	  
\section{Stetigkeit}
  \begin{definition}
    \textbf{A:} \newline Eine Funktion $f:D\rightarrow \R$ mit $D \subset \R$ heißt stetig in $x^* \in D$, falls $\lim\limits_{x \rightarrow x^*} f(x) = f(x^*)$. \label{def:stet_a}
  \end{definition}
  \begin{bem}
    Jede in $x^*$ differenzierbare Funktion ist auch stetig in $x^*$.
  \end{bem}
  \begin{definition}
    Eine Funktion $f:D\rightarrow \R$ mit $D \subset \R$ heißt stetig, falls $f$ stetig für alle $x^* \in D$ ist.
  \end{definition}
  \begin{satz}
    Sind $f$, $g$ stetige Funktionen und $\lambda \in \R$, so sind innerhalb ihres Definitionsbereich auch $\lambda f$, $f+g$, $f\cdot g$, $f\circ g$, $\frac{f}{g}$ stetige Funktionen.
  \end{satz}
  \begin{satz}
  Stetige Fortsetzbarkeit: Eine Definitionslücke einer gebrochen rationalen Funktion ist hebbar, wenn die Vielfachheit der Nullstelle $x_0$ im Zäherpolynom $\geq$ der Vielfachheit der Nullstelle $x_0$ im Nennerpolynom ist.
  \end{satz}
  \subsection{Arten von Unstetigkeiten}
  \begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \captionsetup{justification=centering}
		  \includegraphics[width=0.78\linewidth]{stetigkeit_sprung.png}
		  \caption{$f(x) = \frac{|x|}{x}$ \\ (Sprungstelle)}
		  \label{fig:stetigkeit_sprung}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \captionsetup{justification=centering}
		  \includegraphics[width=0.7\linewidth]{stetigkeit_pol.png}
		  \caption{$f(x) = \frac{1}{x}$ \\ (Polstelle)}
		  \label{fig:stetigkeit_pol}
		\end{minipage}
  \end{figure}
  \begin{figure}[H] 
		\centering
	  \centering
	  \captionsetup{justification=centering}
	  \includegraphics[width=0.35\linewidth]{stetigkeit_luecke.png}
	  \caption{$f(x) = \frac{x^3}{x}$ \\ (Lücke)}
	  \label{fig:stetigkeit_luecke}
  \end{figure}
  \begin{definition}
    $f:D\rightarrow \R$, $f(x)$ ist stetig in $x_0 \in D$ falls \newline
    \begin{itemize}
      \item[1) ] Falls z.z. ist dass $f$ stetig ist:
      \begin{equation}
        \forall \varepsilon > 0 \quad \exists \delta_\varepsilon \quad \forall x \in D: \left(|x-x_0| < \delta_\varepsilon \Rightarrow |f(x) - f(x_0)| < \varepsilon \right)
      \end{equation}
      \item[2) ] Falls z.z. ist dass $f$ nicht stetig ist:
      \begin{equation}
        \forall \text{ Folgen } (x_n) \text{ mit }x_n \rightarrow x_0 \text{ für }n \rightarrow \infty \Rightarrow f(x_n) \rightarrow f(x_0) \text{ für } n \rightarrow \infty.
      \end{equation}
      \item[2) ] 
      \begin{equation}
        \lim\limits_{x \rightarrow x_o^-} f(x) = \lim\limits_{x \rightarrow x_o^+} f(x)
        \end{equation}
    \end{itemize}
  \end{definition}
  \subsection{Nullstellensatz und Zwischenwertsatz}
  \begin{satz}
    Es sei $f: [a,b]\rightarrow \R§$ eine stetige Funktion. Dann gilt:
    \begin{itemize}
      \item[a) ] Es sei $f(a)f(b) < 0$. Dann existiert ein $x^*\in(a,b)$ mit $f(x^*) = 0$, d.h. eine Nullstelle von $f$.
      \item[b) ] Sei $f(a) < c < f(b)$. Dann existiert ein $x^* \in (a,b)$ mit $f(x^*) = c$.
    \end{itemize}
    \begin{figure}[H] 
			\centering
			\begin{minipage}{.5\textwidth}
			  \centering
			  \captionsetup{justification=centering}
			  \includegraphics[width=0.75\linewidth]{potenzreihen_nullstellensatz.png}
			  \caption{Zu a) \\ (Nullstellensatz) \protect\cite{HM12}}
			  \label{fig:reihe_potenzradius_r}
			\end{minipage}%
			\begin{minipage}{.5\textwidth}
			  \centering
			  \captionsetup{justification=centering}
			  \includegraphics[width=0.8\linewidth]{potenzreihen_zwischenwertsatz.png}
			  \caption{Zu b) \\ (Zwischenwertsatz) \protect\cite{HM12}}
			  \label{fig:reihe_potenzradius_c}
			\end{minipage}
    \end{figure}
  \end{satz}
  \subsubsection{Sätze zu Stetigkeit und Monotonie}
  \begin{satz}
    $f:[a,b] \rightarrow \R$ sei streng monoton wachsend und  stetig. Dann existiert die Umkehrfunktion $f^{-1}:[f(a),f(b)]\rightarrow \R$. Diese ist streng monoton wachsend und stetig.
  \end{satz}
  \begin{definition}
    \textbf{B: ($\varepsilon -\delta$ Definition)} \newline
    Se $f:D\rightarrow \R$ mit $D \subset \R$. Dann heißt $f$ stetig in $x^*$, wenn für alle $\varepsilon > 0$ ein $\delta > 0$ existiert, sodass gilt: 
    \begin{equation}
      \forall x \in D:\quad |x - x^*| < \delta \Rightarrow |f(x) - f(x^*)| < \varepsilon
    \end{equation}
    Es muss zu jeder beliebig kleinen Seitenlänge $\varepsilon$ immer ein Rechteck mit Mitte $x^*$ existieren, sodass $f$ das Rechteck an den Seiten verlässt.
    \begin{figure}[H] 
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.2\linewidth]{stetigkeit_eps_del.png}
			\caption{$\varepsilon - \delta$ Kriterium \protect\cite{HM12}}
			\label{fig:stetigkeit_eps_del}
    \end{figure} \label{def:stet_b}
  \end{definition}
  
  \begin{definition}
    \begin{itemize}
      \item[a) ] $f$ heißt Lipschitz-stetig in $x^*$, wenn es ein $L > 0$ und $\delta > 0$ gibt, sodass für $x \in D$ mit $|x-x^*| < \delta$
	      \begin{equation}
	        |f(x) - f(x^*) \leq L|x-x^*|
	      \end{equation}
	      gilt.
      \item[b) ] $f$ heißt Hölder-stetig mit Exponent $\alpha \in (0,1]$, falls
	      \begin{equation}
	        |f(x) - f(x^*)| \leq L(|x-x^*|^\alpha
	      \end{equation}
	      gilt.
    \end{itemize}
    \begin{figure}[H] 
			\centering
			\begin{minipage}{.5\textwidth}
			  \centering
			  \captionsetup{justification=centering}
			  \includegraphics[width=0.9\linewidth]{stetigkeit_lipschitz.png}
			  \caption{Zu a) \\ (Lipschitzstetigkeit) \protect\cite{HM12}}
			  \label{fig:stetigkeit_lipschitz}
			\end{minipage}%
			\begin{minipage}{.5\textwidth}
			  \centering
			  \captionsetup{justification=centering}
			  \includegraphics[width=0.65\linewidth]{stetigkeit_hoelder.png}
			  \caption{Zu b) \\ (Hölder-Stetigkeit) \protect\cite{HM12}}
			  \label{fig:stetigkeit_hoelder}
			\end{minipage}
    \end{figure}
  \end{definition}
  \begin{bem}
  \textbf{Zu a)}
    Es ist die Idee, eine Sekante so in den Graph zu legen, dass sie zwei Punkte des Graphen schneidet. Existiert nun eine Sprungstelle, so kann man diese Bedingung einhalten und die Steigung der Sektanten gegen unendlich treiben (praktisch eine senkrechte gerade). Liegt keine Sprungstelle vor, so wird die Steigung unter Einhaltung der Bedingung immer kleiner unendlich sein. Die Schranke für die Steigung ist hier $L$.
  \end{bem}
  \begin{satz}
    Definition A \eqref{def:stet_a} und Definition B \eqref{def:stet_b} der Stetigkeit sind äquivalent.
  \end{satz}
  \begin{definition}
    Eine Funktion $f:D\rightarrow \R$ mit $D \subset \R$ heißt Hölder- bzw. Lipschitzstetig, falls $f$ Hölder- bzw. Lipschitzstetig in jedem $x^* \in D$ ist. \newline
    Bezeichnungen: $C^{0,\alpha}$ steht für Hölder-stetig, $C^{0,1} = Lip$ steht für Lipschitzstetig.
  \end{definition}
  \begin{bem}
    Es gilt:
    \begin{equation}
      \text{differenzierbar } \Rightarrow \text{Lipschitzstetig } \Rightarrow \text{Hölder-stetig } \Rightarrow \text{stetig }
    \end{equation}
  \end{bem}    
  \newpage 
  
\section{Extremalprobleme}
Es sei $f:\R \rightarrow \R$, dann ist $f'(x^*) = 0$ notenwide Bedingung dafür, dass ein Minimum oder Maximum vorliegt, sofern $f$ diffbar ist.
\begin{satz}
  \textbf{Globale Theorie: } $[a,b]$ sei ein abgeschlossenes Intervall. $f:[a,b] \rightarrow \R$ sei stetig. Dann gibt es je ein $\underline{x}, \overline{x} \in [a,b]$ mit $f(\underline{x}) = \min\limits_{x \in [a,b]}f(x)$ und $f(\overline{x}) = \max\limits_{x \in [a,b]}f(x)$.\label{ax:extrema_global}
\end{satz}
\begin{bem} (Zu \eqref{ax:extrema_global})
  Stetige Funktionen auf kompakten Mengen nehmen das Maximum und Minimum an.
\end{bem}
\begin{satz}
  \textbf{Lokale Theorie: } Sei $f:[a,b] \rightarrow \R$ mit $f\in \C^3$ (also 3 mal stetig diffbar). Für $x* \in (a,b)$ gilt dann:
  \begin{itemize}
    \item[a) ] Aus $f'(x^*) = 0$ und $f''(x^*) > 0$ folgt, dass $f$ in $x^*$ ein lokales Minimum hat.
    \item[b) ] Aus $f'(x^*) = 0$ und $f''(x^*) < 0$ folgt, dass $f$ in $x^*$ ein lokales Maximum hat.
  \end{itemize}
\end{satz}
\newpage

\section{Funktionenfolgen}
\begin{definition}
  Eine Funtionenfolge ist allgemein eine Folge von Funktionen $f_1, f_2, ...$ bei denen alle Funktion dieselbe Definitions- und Zielmenge haben.
  \begin{equation}
  f:D\times \N \rightarrow Z, \quad (x,n) \rightarrow f_n(x)
  \end{equation}
  Wobei $D$ die Definitionsmenge und $Z$ die Zielmenge ist.
\end{definition}
\begin{definition}
  \begin{itemize}
    \item[a )] Eine Folge von Funktionen $f_n$ mit $f_n: D \rightarrow \R$ mit $D \subset \R$ heißt für $n \rightarrow \infty$ punktweise gegen $f$ konvergent, falls für alle $x \in D: \lim\limits_{n \rightarrow \infty} f_n(x) = f(x)$ gilt.
    \item[b )] Eine Funktionenfolge heißt gleichmäßig konvergent, falls \newline $\lim\limits_{n \rightarrow \infty} \sup\limits_{x \in D} |f_n(x) -f(x)| = 0$ gilt.
  \end{itemize}
  \begin{figure}[H] 
		\centering
	  \centering
	  \captionsetup{justification=centering}
	  \includegraphics[width=0.5\linewidth]{funktionenfolgen_konvergenz.png}
	  \caption{Epsilonkanal Funktionenfolge \protect\cite{HM12}}
	  \label{fig:funktionenfolge_konvergenz}
  \end{figure}
\end{definition}
\begin{satz}
  Gegeben sei eine Folge stetiger Funktionen mit $f_n: D\rightarrow \R$. Gilt $f_n \rightarrow f$ gleichmäßig auf $D$, so ist auch die Grenzfunktion stetig.
\end{satz}
\begin{satz}
  Zu jeder Potenzreihe gibt es den Konvergenzradius $r$ mit $0 \leq r \leq \infty$ mit der Eigenschaft, dass $\sum\limits_{k=0}^\infty a_k(z-z_0)^k$ absolut konvergent für $|z-z_0| < r$ und divergent für $|z-z_0|>r$ ist.\newline
  Ferner konvergiert die Potenzreihe auf jeder Kreisscheibe $|z-z_0| < \delta$ mit $\delta < r$ auch gleichmäßig. 
\end{satz}
\begin{bem}
  Die Konsequenz aus obigem Satz ist, dass Potenzreihen für alle $z$ mit $|z-z_0| < r$ stetig sind.
\end{bem}
\begin{satz}
  $(f_n)_{n \in \N}$ seien auf $[a,b]$ differenzierbar. Für ein $x_0 \in [a,b]$ sei $f_n(x_0)$ konvergent. Ferner konvergiere $(f'_n)_{n \in \N}$ gleichmäßig (gegen $g$) auf $[a,b]$. Dann gilt:
  \begin{itemize}
    \item[1) ] $(f_n)_{n \in \N}$ konvergiert auf $[a,b]$ gleichmäßig.
    \item[2) ] $f(x) = \lim\limits_{n \rightarrow \infty} f_n(x)$ ist differenzierbar auf $[a,b]$ und es gilt $f'(x) = \lim\limits_{n \rightarrow \infty} f_n'(x)$ (und es gilt $f' = g$).
  \end{itemize}
\end{satz}
\begin{bem}
  Die Konsequenz aus obigem Satz ist, dass Potenzreihen innerhalb ihres Konvergenzradius beliebig oft differenzierbar sind und gliedweise abgeleitet werden können.
\end{bem}
\newpage
\chapter{HM2}
\section{Integralberechnung}
\subsection{Unbestimmtes Integral}
\begin{equation}
\int f(x) dx = F(x) + C = [F(x)]\qquad, C\in\R \label{eq:def_noBorder}
\end{equation}

\subsection{Bestimmtes Integral}
\begin{equation}
\int_a^b f(x) dx = F(b) - F(a) \label{eq:def_border}
\end{equation}

\subsection{Partielle Integration}
Entspricht der "Produktregel" der Differentialrechnung.
\begin{equation}
\int_{\colBord{a}}^{\colBord{b}} f'(x) g(x) dx = f(x) g(x) \colBord{\Big|_a^b} - \int_{\colBord{a}}^{\colBord{b}} f(x) g'(x) dx \label{eq:rule_partInt}
\end{equation}
Bietet sich zum Beispiel bei Produkten aus x-Potenz mit e-Funktionen, log, sin oder cos an.

\subsection{Integration durch Substitution}
Entspricht der "Kettenregel" der Differentialrechnung.
\begin{equation}
\int_{\colBord{a}}^{\colBord{b}} f(g(x))g'(x) dx = \int_{\colBord{g(a)}}^{\colBord{g(b)}} f(y) dy \qquad (setze \quad y = g(x) \label{eq:rule_subs}
\end{equation}

\subsubsection{Spezialfall}
\begin{equation}
\int \frac{f'(x)}{f(x)} dx = ln(|f(x)|) + C \qquad ,C\in\R \label{eq:rule_spec}
\end{equation}

\subsection{Gerade/Ungerade Funktionen}
\begin{align}
\int_{-a}^a f(x) = 
\begin{cases}
2 \int_0^a f(x) dx &,\; f\; gerade\\
0 &,\; f\; ungerade\\
\end{cases} \label{eq:evenodd}
\end{align}
\begin{align*}
\text{f gerade, falls }f(-x) &= f(x) \qquad &(z.B.: cos(x), x^2)\\
\text{f ungerade, falls }f(-x) &= -f(x) \qquad &(z.B.: sin(x), x^3)\\
\end{align*}

\subsection{Allgemeines zur Integration}
\subsubsection{Riemann Integrierbarkeit}
$f:[a,b] \rightarrow \R$ stetig bzw. monoton \newline
$\Rightarrow$ f ist R-integrierbar.

\subsubsubsection{Riemannsches Unterintegral}
\begin{equation}
\int_{a}^{\bar{b}} f(x) dx = \sup\{U_f(Z): \; \text{Z Zerlegung von }[a,b]\}
\end{equation}


\subsubsubsection{Riemannsches Oberintegral}
\begin{equation}
\int_{\bar{a}}^{b} f(x) dx = \inf\{O_f(Z): \; \text{Z Zerlegung von }[a,b]\}
\end{equation}

$\rightarrow \text{f heißt Riemann-integrierbar über }[a,b]$, falls
\begin{equation}
\int_{\bar{a}}^{b} f(x) dx = \int_a^{\bar{b}} f(x) dx
\end{equation}
\newline
In diesem Fall heißt der Wert das Riemannn-Integral und wird mit $\int_a^b f(x)dx$ bezeichnet.

\subsubsubsection{Riemannsche Untersumme}
\begin{equation}
  U_f(Z) = \sum_{j = 0}^{n-1} \inf\limits_{\xi \in [x_j, X_{j+1}]} f(\xi) \cdot (x_{j+1} -x_j)
\end{equation}
\subsubsubsection{Riemannsche Obersumme}
\begin{equation}
  O_f(Z) = \sum_{j = 0}^{n-1} \sup\limits_{\xi \in [x_j, X_{j+1}]} f(\xi) \cdot (x_{j+1} -x_j)
\end{equation}

\subsubsubsection{Eigenschaften}
\begin{description}
\item[a)]
Falls $a<b$ setzen wir:
\begin{align}
\int_b^a f(x) dx &= -\int_a^bf(x)dx \nonumber \\
\int_a^a f(x) dx &= 0
\end{align}
\item[b)]
f, g seien R-integrierbar, $\lambda , \mu \in \R \rightarrow \lambda f + \mu g$ ist R-integrierbar (Vektorraumeigenschaft).
\begin{equation}
\int_a^b \lambda f + \mu g)(x)dx = \lambda \int_a^b f(x) dx + \mu \int_a^b g(x) dx
\end{equation}
\item[c)]
$a<C<b$, f ist R-integrierbar.
\begin{equation}
\int_a^b f(x) dx = \int_a^C f(x) dx + \int_C^b f(x) dx
\end{equation}
\item[d)]
\begin{align}
f(x) \ge 0 &\Rightarrow \int_a^b f(x) dx \ge 0 \nonumber \\
f(x) \ge g(x) &\Rightarrow \int_a^b f(x) dx \ge \int_a^b g(x)dx
\end{align}
\item[e)]
\begin{equation}
\text{Sind $f$ und $g$ R-integrierbar ist auch $f*g$ R-integrierbar.}
\end{equation}
\item[f)]
\begin{align}
g(x) \ge C > 0 \Rightarrow \frac{f}{g} \text{ ist R-integrierbar.}
\end{align}
\item[g)]
\begin{equation}
\text{Ist $f$ R-integrierbar dann ist auch } |f| \text{ R-integrierbar.}
\end{equation} 
\item[h)]
\begin{equation}
(b-a) \inf_{x\in[a,b]}{f(x)} \le \int_a^b f(x) dx \le (b-a) \sup_{x\in [a,b]}{f(x)}
\end{equation}
\end{description}

\subsubsubsection{Kriterien zur Riemann-Integrierbarkeit}

\begin{description}
\item[a)]
$f$ monoton $\Rightarrow f$ R-integrierbar.
\item[b)]
$f$ stetig $\Rightarrow f$ R-integrierbar
\begin{satz}
\glqq Jede stetige Funktion $f:k \rightarrow \R$ auf einer kompakten Menge k, d.h. für $k<\R^d$ abgeschlossen und beschränkt, ist dort gleichmäßig stetig und damit R-integrierbar.\grqq \cite{HM12}
Beispiel für k: $k:[a,b]$
\end{satz}
\item[c)]
\begin{satz}
Kriterium: Jede Funktion deren Unstetigkeitsstellen eine Nullmenge bilden (z.B. abzählbare Mengen) sind R-integrierbar.
\glqq Satz: Eine Funktion $f:[a,b]\rightarrow \R$ ist genau dann R-integrierbar, wenn $f$ beschränkt ist und die Menge der Unstetigkeitsstellen eine Nullmenge ist. 
\grqq \cite{HM12}
\end{satz}
Die Konsequenz daraus lautet, dass jede stetige Funktion mit endlich vielen Sprungstellen R-integrierbar ist. \citeVgl{HM12}
\item[d)]
\begin{satz}
\glqq Sei $f:[a,b] \rightarrow \R$ beschränkt. Dann ist $f$ R-integrierbar genau dann, wenn es zu jedem $\varepsilon > 0$ eine Partition $Z$ gibt, 
so dass
$O_f(Z)  U_f(Z) < \varepsilon$. \grqq \cite{HM12}
\end{satz}
Anmerkung: \glqq In der Mengenlehre ist eine Partition (auch Zerlegung oder Klasseneinteilung) einer Menge M eine Menge P, deren Elemente nichtleere Teilmengen von M sind, sodass jedes Element von M in genau einem Element von P enthalten ist. Anders gesagt: Eine Partition einer Menge ist eine Zerlegung dieser Menge in nichtleere paarweise disjunkte Teilmengen.\grqq  \cite{wiki}

\end{description}

\subsubsection{MWS der Integralrechnung}
$f:[a,b]\rightarrow\R$ stetig, dann $\exists \; \xi \in[a,b]$ mit $\int_a^b f(x)dx = f(\xi)(b-a)$.

\subsubsection{Hauptsatz der Differential- und Integralrechnung}
$f:[a,b]\rightarrow\R$ stetig, dann ist $F(x) = \int_a^x f(t)dt$ diffbar und $F'(x) = f(x)$.

\subsubsubsection{Folgerungen}
\begin{satz}
Ist $f$ ungerade, so ist $f''$ gerade, und alle Stammfunktionen vonm $f$ sind gerade.\citevgl{HM2Vortragsubung}
\end{satz}
\begin{satz}
Ist $f$ gerade, so ist $f'$ ungerade, und $f$ besitzt eine ungerade Stammfunktion.\citevgl{HM2Vortragsubung}
\end{satz}

\subsection{Partialbruchzerlegung}
	\begin{equation}
	  R(x) = \frac{p(x)}{q(x)}, \qquad p,q\text{ Polynome}
	\end{equation}
	Vorgehensweise:
	\begin{description}
	  \item{\textbf{1)}} Zähler und Nennergrad untersuchen\newline
	  ist $grad(p) > grad(q)$, also Zählergrad > Nennergrad umformen in  \newline$R(x) = p_1(x) + \frac{p_2(x)}{q(x)}$ $\Rightarrow$ Polynomdivision.
	  \item{\textbf{2)}} Nullstellen und faktorisieren
	    \begin{itemize}
	      \item Nullstellen des Nenners bestimmen
	      \item Nenner Faktorisieren in $p1, p2, ...$
	    \end{itemize}
	  \item{\textbf{3)}} Ansatz
	    \begin{itemize}
	      \item Ansatz für Partialbruchzerlegung $R(x) = \frac{A}{p1} + \frac{B}{p2} + ...$
	      \item Bestimmung von A, B, C, ...
	    \end{itemize}
	\end{description}
  Bei quadratischen oder höhergradigen Nullstellen lautet der Ansatz:
  \begin{equation}
    NST = x^n \Rightarrow R(x) = \frac{A}{x} + \frac{B}{x^2} + ... + \frac{N}{x^n}
  \end{equation}
  Bei komplexen Nullstellen muss der Ansatz angepasst werden.
  \begin{align}
    &NST: 2, -2, 2i, -2i \nonumber \\
    &Ansatz: R(x) = \frac{A}{x-2} + \frac{B}{x+2} + \frac{Cx + D}{x^2+4}
  \end{align}
  Es werden also die komplexen Nullstellen ausmultipliziert und so reell dargestellt.
  
\subsection{Uneigentliche Integrale}
  \begin{satz}
    Sei $f:[a, \infty] := I \rightarrow \R$ lokal integrierbar. Konvergiert $\int_a^\infty f(x) dx$ absolut. d.h. ist $\int_a^\infty |f(x)| dx$ konvergent, so konvergiert auch $\int_a^\infty f(x) dx$.
  \end{satz}
  \begin{satz}
    Majorantenkriterium: Gilt für alle $x\in I$, dass $|f(x)| \leq g(x)$, und ist $\int_a^\infty g(x) dx$ konvergent, so ist $\int_a^\infty dx$ (im Script vom Prof ist hier die untere Grenze 0, ich denke es sollte aber a sein) absolut konvergent.    
  \end{satz}
  \begin{satz}
    Minorantenkriterium: Gilt für alle $x\in I$, dass $0 \leq g(x) \leq f(x)$ und divergiert $\int_a^\infty g(x) dx$, so divergiert auch $\int_a^\infty dx$.
  \end{satz}
  \begin{bem}
    Abschätzungen mit negativen Minoranten sind falsch da mit einer negativen Minorante alles nach unten abgeschätzt werden kann.
  \end{bem}
  \subsubsection{Typen uneigentlicher Integrale}
  \begin{align}
    \textbf{\colGreen{Singularität:} }\int_{\colGreen{0}}^1 \frac{1}{\sqrt{\colGreen{x}}} dx \nonumber \\
    \textbf{\colGreen{Unbeschränktes Gebiet:} }\int_1^{\colGreen{\infty}} e^{-x} dx \nonumber \\
  \end{align}
  \begin{definition}
    Eine Singularität ist die Stelle, an der die Funktion divergieren würde oder undefiniert wäre.
  \end{definition}
  \textbf{Methode:} Ersetzen der kritischen Stelle durch $z$ und setzen eines Grenzüberganges, z.B.:
  \begin{align*}
    \lim\limits_{z\rightarrow0} \int_z^1 \frac{1}{\sqrt{x}} dx, \quad \lim\limits_{z\rightarrow \infty} \int_1^z e^{-x} dx
  \end{align*}
  \textbf{Vergleichsintegrale}
  \begin{align}
    \int_1^\infty \frac{1}{x^\alpha} dx = 
    \begin{cases}
      \text{konvergiert}\quad , \alpha > 1 \\
      \text{divergiert}\quad , \alpha \leq 1
    \end{cases} \nonumber\\
    \int_0^1 \frac{1}{x^\alpha} dx = 
    \begin{cases}
      \text{divergiert}\quad , \alpha \geq 1 \\
      \text{konvergiert}\quad , \alpha < 1
    \end{cases} \nonumber\\    
  \end{align}

\subsection{Wichtige Integrale}
\begin{equation}
  \int \frac{1}{1+y^2} dx = arctan(y)
\end{equation}
\begin{equation}
  \int x^n dx = \frac{x^{n+1}}{n+1}, \qquad n \neq -1
\end{equation}
\begin{equation}
  \int \frac{1}{cos^2(x)} dx = tan(x)
\end{equation}
\begin{equation}
  \int \frac{1}{sin^2(x)} dx = cot(x)
\end{equation}
\begin{equation}
  \int a^x dx = \frac{a^x}{ln(a)}
\end{equation}
\begin{equation}
  \int \frac{1}{x} dx = ln|x|
\end{equation}
\begin{equation}
  \int \frac{1}{cosh^2(x)} dx = tanh(x)
\end{equation}
\begin{equation}
  \int \frac{1}{sinh^2(x)} dx = -coth(x)
\end{equation}
\begin{equation}
  \int ln(x) dx = x ln(x) -x
\end{equation}
\begin{equation}
  \int \frac{1}{x-x_1} dx = ln|x-x_1|
\end{equation}
\begin{equation}
  \int \frac{1}{/x-x_1)^k} dx = \frac{1}{-k+1}(x-x_1)^{-k+1}, \quad k>1
\end{equation}
\begin{equation}
  \int \frac{1}{(x-a)^2+b^2}dx = \frac{1}{b^2} \int \frac{1}{\left(\frac{x-a}{b}\right)^2 +1} dx = \frac{1}{b} arctan\left(\frac{x-a}{b}\right)
\end{equation}

\subsection{Separierbare DGL}
  \subsubsection{Wiederholung klassische DGL}
  Bisher: lineare DGl mit konstanten Koeffizienten. \newline
  z.B.: $y''(t) - 5y'(t) + 4y(t) = e^{\colGreen{2}t} \qquad ,\; y(0) = 1,\; y'(0) = 1$ \newline
  \begin{tabularx}{14.7cm}{l l}
	  Homogene DGL: & $y(t) = e^{\lambda t} \Rightarrow p(\lambda) = \lambda^2 - 5 \lambda +4 = 0$ \\
	  $\;$ & $\Rightarrow \lambda_1 = 1, \; \lambda_2 = 4$ \\
	  $\;$ & $\Rightarrow yh(t) = C_1 e^t + C_2 e^{4t} \qquad ,\;C_1,\;C_2 \in \R$\\
	  $\;$ & $\;$ \\
	  Inhomogenes DGL: & $\ubGreen{\text{da 2 keine NST}}{yp(t) = re^{2t}}$\\
  \end{tabularx}  
  \begin{align*}
    &\Rightarrow yp'(t) = 2re^{2t},\; yp''(t) = 4re^{2t} \\
    &\overset{DGL}{=} 4re^{2t} - 10re^{2t} + 4re^{2t} \overset{!}{=} e^{2t} \Rightarrow -2re^{2t} = e^{2t}\\
    &\Rightarrow r = -\frac{1}{2}
  \end{align*} 
  \begin{tabularx}{14.7cm}{l l}
	  Allgemeine Lösung: & $y(t) = yh(t) + yp(t) = C_1 e^t + C_2 e^{4t} - \frac{1}{2} e^{2t}$
  \end{tabularx}
  
  \subsubsection{Lösen von DGL mit Koeffizienten die von t abhängig sind}
  z.B. $ y'(t) - ty(t) = t \qquad ,\; y(0) = 1$\newline
  \newline
  Spezielle Form: 
  \begin{equation}
    y'(t) = f(t) g(y(t)) \qquad,\; y(t_0) = y_0
  \end{equation}     
  \begin{align*}
    \Rightarrow y'(t) = t+ty(t) = \ubGreen{f(t)}{t} \ubGreen{(1+y(t))}{g(y(t))}
  \end{align*}
  Lösung: Trennung der Veränderlichen:
  \begin{align}
  \frac{y'}{g(y)} = f(t) \overset{\colGreen{y' = \frac{dy}{dt}}}{\Rightarrow} \colGreen{\int} \frac{1}{g(y)} dy = \colGreen{\int} f(t)dt +C \quad ,\;C\in\R
  \end{align}
  $C$ erhält man aus der Anfangsbedingung $y(t_0) = y_0$.
  \newpage
	
\section{Lineare Algebra}
  \subsection{Definitionen}
  \subsubsection{Linearkombinationen}
  $V$ sei ein Vektorraum (im Folgenden VR), $v_1,\;...,\;v_m \in V\quad ,\;\lambda  _i \in \R$.
  \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{1}) & $\sum\limits_{i = 1}^m \lambda_i v_i$ & \colBlue{Linearkombinationen der $v_i$.}\\
		  \end{tabularx}
		  \label{def:linA_lineark}
    \end{equation}
    \subsubsection{Spann und Erzeugendensystem}
    \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{2}) & $span(v_1,\;...,\;v_m) = \left\lbrace \sum\limits_{i = 1}^m \lambda_i v_i \quad ,\; \lambda_i,\; v_i \in \R \right\rbrace$ & \colBlue{Spann der
				 $v_i$.}\\
				 $\;$ & Gilt $span(v_1,\;...,\;v_m) = V \Rightarrow \lbrace v_1,\;...,\;v_m \rbrace$ &ist ein Erzeugendensystem.\\
		  \end{tabularx}
		  \label{def:linA_span}
    \end{equation}
    \begin{bem}
      Es gilt:
      \begin{equation}
        dim = span \Rightarrow EZS
      \end{equation}
    \end{bem}
    \subsubsection{Lineare Unabhängigkeit}
    \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{3}) & \multiTwo{l}{$v_1,\;...,\;v_m$ linear unabhänig, falls}\\ 
				$\;$ & \multiTwo{l}{$\sum\limits_{i = 1}^m \lambda_i v_i = \vec{0} \Rightarrow \lambda_1 = \lambda_2 = ... = \lambda_m = 0$}\\
				$\;$ & \multiTwo{l}{$0$ darf die einzige Lösung sein, sonst linear abhängig.}\\
		  \end{tabularx}
		  \label{def:linA_linearabh}
    \end{equation}
    \begin{bem}
      Es gilt:
      \begin{equation}
        detA \neq 0 \Rightarrow \text{linear unabhängig}
      \end{equation}
    \end{bem}
    \subsubsection{Basis}
    \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{4}) & \multiTwo{l}{$B = \lbrace b_1,\;...,\;b_n \rbrace \subset V$ ist Basis von $V$, falls}\\
				$\;$ & \colGreen{(B1)} & $b_i$ linear unabhängig, $i = 1,\; ...,\; n$\\
				$\;$ & \colGreen{(B2)} & $B$ ist ein Erzeugendensystem.\\
		  \end{tabularx}
		  \label{def:linA_basis}
    \end{equation}	
    
    Es gilt:
    \begin{equation}
      \begin{tabularx}{14.7cm}{l l l}
      (\textbf{1}) & $dimV = |b|$ & \colBlue{(Mächtigkeit von $B$)}\\
      (\textbf{2}) & $dimV = n$ & \colBlue{($\Rightarrow n+1$ Vektoren sind linear abhängig)}\\
      (\textbf{3}) & \multiTwo{l}{$\forall v \in V: v = \sum\limits_{i = 1}^n \lambda_i b_i$ eindeutig darstellbar.}\\
      $\;$ & $\Rightarrow B^v = (\lambda_1,\;...,\; \lambda_n)^T$ & \colBlue{(Koordinaten von $v$ bezüglich $B$)}\\
      \end{tabularx}
      \label{def:linA_allg}
    \end{equation}

  \subsubsection{Kanonische Basis}
  \begin{equation}
    e_1 = \vecT{1\\0\\ \dddot \\ 0,} \quad e_2 = \vecT{0\\1\\ \dddot \\ 0}, \quad ..., \quad e_d = \vecT{0\\ \dddot \\0 \\1}
  \end{equation}
	\subsection{Vektorräume}
		\subsubsection{Definitionen}
		\subsubsubsection{$\R^d$}
		\begin{equation}
			\R^d: v = \vecT{v_1\\v_2\\ \dddot \\v_d},\; w = \vecT{w_1\\w_2\\ \dddot \\ w_d}
		\end{equation}	
		\subsubsubsection{Vektoraddition}
		\begin{equation}
			v + w = \vecD{v} + \vecD{w} = \vecT{v_1 + w_1 \\ \dddot \\ v_d + w_d}
		\end{equation}
		\subsubsubsection{Skalare Multiplikation}
		\begin{align}
			\alpha \in \R, \; v \in \R^d \nonumber \\
			\alpha \cdot v = \vecD{\alpha\cdot v}
		\end{align}
		\subsubsubsection{Nullvektor}
		\begin{equation}
		  \mathcal{O} = \vecT{0 \\ \dddot \\ 0}
    \end{equation}		
    
    \subsubsection{Struktur}
    Für $v,\;w,\;z \in \R^d$ gelten die folgenden Eigenschaften:
    \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{V1}) & $v + w = w + v$ &\\
				(\textbf{V2}) & $v+(w+z) = (v+w)+z$ &\\
				(\textbf{V3}) & $v + 0 = 0 + v = v$ & \colBlue{(0 ist das neutrale Element)}\\
				(\textbf{V4}) & $v+(-v) = (-v)+v = 0$ & \colBlue{(-v ist das inverse Element)}\\
		  \end{tabularx}
		  \label{ax:vektorraumeig_v}
    \end{equation}	
    \newline
    Für $\alpha\; \beta \in \R,\; v,\;w \in \R^d$ gilt:
    \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{S1}) & $1 \cdot v = v$ & $\qquad\colBlue{(1\in \R)}$\\
				(\textbf{S2}) & $\alpha (\beta v) = (\alpha \beta)v$ &\\
				(\textbf{S3}) & $(\alpha + \beta) v = \alpha \cdot v + \beta \cdot v$ &\\
				(\textbf{S4}) & $\alpha(v + w) = \alpha \cdot v + \alpha \cdot w$ &\\
		  \end{tabularx}
		  \label{ax:vektorraumeig_s}
    \end{equation}		 
    $(V1) - (V4)$ und $(S1) - (S4)$ gelten auch für Funktionen.
    \begin{definition}
    Ist $V$ eine Menge für deren Elemente eine Addition und eine skalare Multiplikation erklärt ist, so heißt sie Vektorraum, falls die Eigenschaften 
    $(V1)$ - $  (V4)$ und $(S1)$ - $(S4)$ erfüllt sind, wobei jetzt $v,\;w,\;z\in V$.
    Je nach Skalarkörper, also $\R$ oder $\C$ sprechen wir von einem reellen oder komplexen Vektorraum. Teilmengen von Vektorräumen, die ebenfalls Vektorräume
    sind, heißen Untervektorräume. \cite{HM12}
    \end{definition}

	  \subsubsection{Untervektorraum}
	  $V$ sei ein Vektorraum und es gelte $U \subset V$.
	  \begin{definition}
	  Eigenschaften $(V1)$ - $(V4)$, $(S1)$ - $(S4)$ sind als Teilmenge von V erfüllt, aber mit $u,\;v \in U$ und $\alpha \in \R$ muss auch $u + v \in U$
	  , $\alpha  u \in U$ gelten (Abgeschlossenheit bezüglich Vektoraddition und skalarer Multiplikation). \cite{HM12}
	  \end{definition}
	  \subsubsubsection{Untervektorraumkriterien}
	  \begin{equation}
			  \begin{tabularx}{14.7cm}{l l l}
					(\textbf{UV0}) & $0 \in U$ &\\
					(\textbf{UV1}) & $u,\;v \in U \Rightarrow u + v \in U$ &\\
					(\textbf{UV2}) & $u \in U,\; \lambda \in \R \Rightarrow \lambda u \in U$ &\\
			  \end{tabularx}
			  \label{ax:vektorraumeig_uv}
	   \end{equation}
	   \newline
	   Es gilt: $U_1,\; U_2 \;$ UVR von V
	   \begin{equation}
	     \begin{tabularx}{14.7cm}{l l l}
					(\textbf{1}) & $U_1 \cap U_2 = \{v\in V: v \in U_1 \land v \in U_2 \}$ UVR von  V &\\
					(\textbf{2}) & $U_1 \cup U_2 = \{v\in V: v \in U_1 \lor v \in U_2\}$ kein UVR von V &\\
			  \end{tabularx}
			  \label{ax:vektorraumeig_uv}
	   \end{equation}
	   \subsubsubsection{Triviale UVR von V}
	   \begin{align*}
		   U = \{0\}\\
		   U = V
	  \end{align*}
	 
	  \subsubsubsection{Interessante Untervektorräume}
	  \begin{itemize}
	    \item Die Menge der stetigen Funktionen: $\brac{\brac{\bracks{a,b},\R}}$
	    \item Die Menge der n-mal stetig diffbaren Funktionen $\C^n \brac{\bracks{a,b} , \R}$
	    \item Die Menge der Riemann-integrierbaren Funktionen
	  \end{itemize}
	  
	  \subsection{Erzeugendensystem, Basis, Dimension, Kern lineare Unabhängigkeit}
	  \subsubsection{Linearkombination, Spann}
	  \begin{definition}
	    Für $v_1,...,v_m \in V$(Vektorraum) und $\lambda_j \inRs $ heißt ein Vektor der Form 
	    \begin{equation}
	      v = \sum_{j = 1}^m \lambda_j v_i
	    \end{equation}
	    eine Linearkombination der Vektoren $v_1,...,v_m$. Die Menge aller Linearkombinationen heißt der Spann von $v_1,...,v_m$, d.h.
	    \begin{equation}
	      Spann(v_1,...,v_m) = \lbrace \sum_{j=1}^m \lambda_j v_j: \lambda_j \inRs \rbrace
      \end{equation}	     
      bzw. der von $v_1,...,v_m$ aufgespannte Raum. \cite{HM12}
	  \end{definition}
	  
	  Beispiel:
	  \begin{align*}
	    v_1 = \vecT{1\\0\\1}, \quad v_2 = \vecT{2\\1\\1}, \quad v_3 &= \vecT{3\\1\\2}, \quad v_4 = \vecT{1\\1\\0}\\
	    v_3 = v_1 + v_2 \quad \quad v_4 &= v_2-v_1\\
	    \Rightarrow Spann\brac{v_1,...,v_4} &= \lbrace \lambda_1 v_1 + \lambda_2 v_2: \lambda_1, \lambda_2 \inRs \rbrace
	  \end{align*}
	  
	  \subsubsection{Lineare Unabhängigkeit}
	  \begin{definition}
	    Die Vektoren $v_1,...,vm$ heißen linear unabhängig, falls aus 
	    \begin{equation}
	      \sum_{j=1}^m \lambda_j v_j = \vecN \in V
	    \end{equation}
	    bereits 
	    \begin{equation}
	      \lambda_1 = ... = \lambda_m = 0 \inRs 
	    \end{equation}
	    folgt.
	  \end{definition}
	  
	  \subsubsection{Dimension}
    \begin{satz}
      Für jede $mxn$ Matrix $A$ gilt die Dimensionsformel: \newline 
      \begin{equation}
      dim(KernA) + RangA = n
      \end{equation}
    \end{satz}	 
    
    \subsubsection{Kern}
    \begin{satz}
      Die Lösungen von $Ax = 0$ bilden einen Untervektorraum des $\R^n$, der sogenannte Kern von $A$, Schreibweise $KernA$.
    \end{satz} 
    Der Kern beinhaltet alle Elemente die auf $0$ abgebildet werden.
    \begin{equation}
      Kern(A) = \lbrace v \in V |A(v) = \mathcal{O}\rbrace \leq V
    \end{equation}
    
	  \subsubsection{Rang}
	  \begin{definition}
	    Die maximale Anzahl linearer Zeilenvektoren der Matrix $A$ heißt der Rang von $A$. 
   \end{definition} 
	  
	  \subsubsection{Basis}
	  \begin{definition}
	    $B = \lbrace v_1,...,v_M\rbrace \subset V$ heißt eine Basis von $V$, falls $B$ linear unabhängig und $V = spann \lbrace v_1,...,v_m \rbrace$ ist.
	  \end{definition}
	 
    \begin{satz}
      \glqq (Basen von endlich-dimensionalen Vektorräumen sind gleich groß).\newline
      Sei $V$ ein Vektorraum mit Basis $\lbrace v_1,...,v_m\rbrace$. Dann sind je $n$ Vektoren $w_1,...,w_n$ aus $V$ mit $n>m$ linear abhängig. \grqq \cite{HM12}
    \end{satz}
     
     \subsubsubsection{Folgerungen}
     \begin{definition}
		  \glqq Ist $\lbrace v_1,...,v_m \rbrace$ eine Basis des Vektorraums $V$, so lässt sich jeder Vektor $v \in V$ eindeutig als Linearkombination der $\lbrace               
		    v_1,...,v_m$ schreiben, d.h.:
		    \begin{equation}
		      \exists x_k \text{ mit } v = \sum_{k=1}^m x_k v_k
		    \end{equation}
		    \grqq \cite{HM12}\newline
		    Mit $x_k$ und $v_k$ als Koordinaten bezüglich dieser Basis.
	    \end{definition}
	    
	    \begin{definition}
		    \glqq Die Anzahl der Elemente einer Basis von $V$ ist unabhängig von der speziellen Wahl der Basis. Die Anzahll der Elemente der Basis heißt die Dimension des Vektorraumes $V$.\grqq \cite{HM12} Schreibweise:
		    \begin{equation}
		      dim V = m
		    \end{equation}
		   \end{definition}
		   
		   \mDef{
		     \glqq Besitzt $V$ eine Basis mit endlich vielen Elementen, so heißt $V$ edlich dimensional, sonst heißt $V$ unendlich dimensional. \grqq \cite{HM12}
		   }
		   
		   \subsubsubsection{Matrixdarstellung einer linearen Abbildung bezüglich zweier Basen}
		   Gegeben sei der Vektorraum $\R^2$ mit der Basis $C = {c_1, c_2}$ mit $c_1 = (1,1)^T$ und $c_2 = (0,-2)^T$, sowie der Vektorraum $\R^3$ mit der Basis $B$. Die lineare Abbildung $L: \colBlue{\R^3} \rightarrow \colGreen{\R^2}$ sei gegeben durch
		   \begin{align*}
		     L\left(\begin{array}{c} x_1 \\ x_2 \\ x_3  \end{array}\right) 
		     = \left(\begin{array}{c} 2x_1+x_2 \\ x_2 - x_3  \end{array} \right)
		   \end{align*}
		   Bestimmen Sie die Matrixdarstellung von $L$ bezüglich der Basen $\colBlue{B}$ und $\colGreen{C}$.
		   \begin{align*}
		     &\textbf{Matrixdarstellung } m_L^{\colGreen{C},\colBlue{B}} &\nonumber \\
		     &\textbf{1. Schritt:} \text{ Bilde die Basisvektoren von $B$ mit der linearen Abb. $L$ ab:}&
		   \end{align*}
		   \begin{align}
		     L\vecT{1\\0\\-1} = \vecT{2\\1} \nonumber \\
		     L\vecT{0\\2\\0} = \vecT{2\\2} \nonumber \\
		     L\vecT{0\\1\\1} = \vecT{1\\0} \nonumber 
		  \end{align}
		  \begin{align*}
		     &\textbf{2. Schritt: } \text{Stelle die Bildvektoren als LInearkombination der Basiselemente von $C$ dar:}& \nonumber 
		  \end{align*}
		  \begin{align}
		     \vecT{2\\1} = \colGreen{2}\vecT{1\\1} + \colGreen{\frac{1}{2}} \vecT{0\\-2} \nonumber\\
		     \vecT{2\\2} = \colGreen{2}\vecT{1\\1} + \colGreen{0} \vecT{0\\-2} \nonumber\\
		     \vecT{1\\0} = \colGreen{1}\vecT{1\\1} + \colGreen{\frac{1}{2}} \vecT{0\\-2} \nonumber
		  \end{align}
	    \begin{align*}
		     &\textbf{3. Schritt: } \text{Eintragen der Koeffizienten in eine Matrix:}& \nonumber 
		  \end{align*}
      \begin{align}
        m_L^{C,B} = \left(\begin{array}{c c c} 2 & 2 & 1 \\ \frac{1}{2} & 0 & \frac{1}{2} \end{array}\right)
      \end{align}        		   
		   
	    \subsection{Lösungsmengen linearer Gleichungssysteme}
	    \begin{align}
		    \left(
	      \begin{array}{c c c c c c c}
	      a_{11}x_1 & + &  ...   & + & a_{1n}x_n & =  & b_1\\
	      .         & \;& \;     & \;& .         & \; & . \\
	      .         & \;& \;     & \;& .         & \; & . \\
	      .         & \;& \;     & \;& .         & \; & . \\
	      a_{m1}x_1 & + & ...    & + & a_{mn}x_n & =  & b_m\\
	      \end{array}
	    \right) \rightarrow Ax = b   
	   \end{align} 
	   
	   \subsubsection{Zeilenvektoren}
	   Die Koeffizienten der Zeilen nennt man Zeilenvektoren.
	   \begin{equation}
	     v_1 = \vecDt{a_{11}}{a_{1n}}, \quad ..., \quad v_m = \vecDt{a_{m1}}{a_{mn}}
	   \end{equation}
	   
	   \subsubsection{Gauss Algorithmus}
	   Beim Gauss-Algorithmus werden Linearkombinationen der Zeilenvektoren gebildet. Das LGS hat nach der Anwendung folgende Form:
	   
	   \begin{align}
	     \begin{array}{c c c c c c c c c c l}
	       \tilde{a}_{11} x_1 & +   & ...                & + & \tilde{a}_{1r} x_r & + & ...                & + & \tilde{a}_{1n} x_n   & = & \tilde{b}_1\\
	       0                  & +   & \tilde{a}_{22} x_1 & + & ...                & + & \tilde{a}_{2r} x_r & + & ...                  & = & \tilde{b}_1\\
	       \;                 & .   & \;                 &\; & \;                 &\; & \;                 &\; & \;                   &\; & \;         \\
	       \;                 &\; . & \;                 &\; & \;                 &\; & \;                 &\; & \;                   &\; & \;         \\
	       \;                 &\;   & .                  &\; & \;                 &\; & \;                 &\; & \;                   &\; & \;         \\
	       \;                 &\;   & \;.                &\; & \;                 &\; & \;                 &\; & \;                   &\; & \;         \\
	       \;                 &\;   & \;                 &\, & \tilde{a}_{rr} x_r & + & ...                & + & \tilde{a}_{rn} xn    & = & \tilde{b}_r\\
	       \;                 &\;   & \;                 &\; & \;                 &\; & \;                 &\; & 0                    & = & \tilde{b}_r + 1\\ 
	       \;                 &\;   & \;                 &\; & \;                 &\; & \;                 &\; & 0                    & = & \tilde{b}_n\\
	     \end{array}
	   \end{align}
	   Die neuen Zeilenvektoren nach dem Gauss-Algorithmus sind:
	   \begin{equation}
	     \tilde{v_1} = \vecDt{\tilde{a}_{11}}{\tilde{a}_{1n}}, \quad ..., \quad \tilde{v_m} = \vecDt{\tilde{a}_{m1}}{\tilde{a}_{mn}}
	   \end{equation}
	   
	   Da beim Gauss-Algorithmus nur Linearkombinationen verwendet werden bleibt der Spannn erhalten.
	   
	   \subsubsection{Gauss-Algorithmus: Spann}
	   \begin{equation}
	     \vspan{v_1, ..., v_m} = \vspan{\tilde{v}_1,...,\tilde{v}_m}
	   \end{equation}
	   
	   \subsubsection{Gauss-Algorithmus: Dimension, Rang}
	   Weiterhin gilt:
	   
	   \begin{equation}
	   dim \lbrace \vspan{v_1,...,v_m} \rbrace = dim \lbrace \tilde{v}_1,...,\tilde{v}_m \rbrace = r
	   \end{equation}
	   $r$ heißt Zeilenrang von $A$ bzw. der Rang von $A$.
	   
	   \subsubsection{Folgerungen}
	   \begin{satz}$\\$
	   \textbf{a)}
	     \glqq Die Lösungen von $Ax = 0$ bilden einen Untervektorraum des $\R^n$, den sogenannten Kern von $A$. \grqq \cite{HM12} \newline
	     Schreibweise:
	     \begin{equation}
	       Kern(A) = ker(A) = kernel(A)
	     \end{equation}
	     \newline
	     \textbf{b)}
	     \begin{equation}
	       \ubGreen{n-r}{\vdim{\vker{A}}} + \ubGreen{r}{\vrang{A}} = n
	     \end{equation}
	     \newline
	     \textbf{c)}
	     \ccite{Ein LGS $Ax = 0$ mit $A \in \R^{nxn}$ besitzt nur die Lösung $x = 0$, wenn $\\\vrang{A} = n$.}{HM12}
	     \newline
	     \textbf{d)}
	     \ccite{Ist $\left(w_1,...,w_k\right)$ eine Basis des Kerns, so lautet die allgemeine Lösung von $Ax = 0$:
	     \begin{equation}
	       x = sum_{j = 1}^k \alpha_j w_j \qquad ,\alpha_j \in \R \qquad \colGreen{(Superpositionsprinzip)}
	     \end{equation}}{HM12}
	     \newline
	     \textbf{e)}
	     \ccite{Ist $x_s$ eine spezielle Lösung von $Ax = b$, so lautet die allgemeine Lösung von $Ax = b$:
	     \begin{equation}
	       x = x_s + \sum_{j = 1}^k \alpha_j w_j \qquad, \inR{\alpha_j}
	     \end{equation}
       Die Lösungsmenge von $Ax = b$ ist ein \underline{affiner Raum}, d.h. die Differenz von jeweils zwei Elementen bildet einen Vektorraum.}{HM12}
	   \end{satz}
	   
  \subsection{Lineare Abbildungen und Matrizen}
  \begin{definition}
    \ccite{Es seien $V$ und $V$ Vektorräume. Eine Abbildung $T: V\rightarrow W$ heißt linear, falls für alle $u,v \in W$ und $\lambda \in \R$ bzw. $\C$
    \begin{align}
      T(v+w) &= T(v) + T(w)\\
      T(\lambda v) &= \lambda T(v)
    \end{align}
    gilt.}{HM12}
  \end{definition}
  
  Wie erhält man die Matrix zu einer linearen Abbildung?\newline
  \ccite{
    Sei $T:V \rightarrow W$ eine lineare Abbildung. $\lbrace v_1,...,v_n\rbrace$ sei eine Basis von $V$ und $\lbrace w_1,...,w_n\rbrace$ sei eine Basis von W. Dann lässt sich jeder Vektor $T(v_j)$ in eindeutiger Weise als Linearkombintion der $w_1,...,w_m$ darstellen, d.h. es gibt $a_{ij} \in \R$ mit 
    \begin{equation}
      T(v_j) = \sum_{i = 1}^m a_{ij} w_i\text{.}
    \end{equation}
    $
      A = \left( 
      \begin{array}{c c c}
        a_{11} & ... & a_{1n}\\
        .      & .   &.      \\
        a_{m1} & ... & a_{mn}\\
      \end{array}
      \right)
    $
    heißt die Matrix der linearen Abbildung $T$ bezüglich der Basen $\lbrace v_1,...,v_m\rbrace$ und $\lbrace w_1,...,w_m\rbrace$.
  }{HM12}
  
  \begin{satz}
  \ccite{
    Die Koordinaten von $w = T(v)$ entstehhen aus den Koordinaten von $v$ durch Multiplikation mit der Matrix $A$.  
    \begin{align}
      w &= T(v) \nonumber\\ 
      mit \; v &= \sum_{j = 1}^n x_j v_j \; und \; w = \sum_{i = 1}^m y_i w_i \nonumber\\
      \text{ergibt sich} \nonumber\\
      \vecDt{y_1}{y_m} &= 
      \left(\begin{array}{c c c}
        a_{11} & ... & a_{1n} \\
        .      & \;  & .      \\
        .      & \;  & .      \\
        .      & \;  & .      \\
        a_{m1} & ... & a_{mn} \\
      \end{array}\right) 
      \vecDt{x_1}{x_n}
    \end{align}
    bzw. $y = Ax$.
  }{HM12}
  \end{satz}
  
  \subsubsection{Das Matrizenkalkül}
  Matrizenmultiplikation tritt bei der Hintereinanderausführung linearer Abbildungen auf.
  \begin{equation}
    T(v) = Av,\quad S(w) = Bw \rightarrow (SoT)(v) = \ubGreen{Matrizen}{BA} \ubGreen{Vektoren}{v}
  \end{equation}
  Der Eintrag in der i-ten Zeile und j-ten Spalte von $BA$ sieht wie folgt aus:
  \begin{equation}
    a_{ij} = \sum_k b_{ik} a_{kj}
  \end{equation}
  
  Damit Matrizen multipliziert werden könnnen, müssen sie die richtige Größe haben.
  \begin{equation}
    Mit  \; A \in \R^{\colBlue{m}x\colGreen{n}} \; und \; B \in \R^{\colGreen{n}x\colBlue{k}} = C \in \R^{\colBlue{m}x\colBlue{k}}
  \end{equation}
  Matrizenmultiplikation ist im allgemeinen
  \begin{itemize}
	  \item nicht kommutativ, d.h. $AB \neq BA$ 
	  \item nicht nullteilerfrei
  \end{itemize}
  
  \subsubsection{Rechenregeln}
  \begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
				(\textbf{M1}) & $(A+B)C = AC + BC$ &\\
				(\textbf{M2}) & $A(B+C) = AB + AC$ &\\
				(\textbf{M3}) & $A(BC) = (AB)C$ &\\
				(\textbf{M4}) & $A(\lambda B) = (\lambda A)B = \lambda(AB)$ &\\
		  \end{tabularx}
		  \label{ax:matrizen_rregel}
    \end{equation}	
    \begin{bem}
      Es gilt weiterhin:
      \begin{align}
        S^{-1}AS &= D\\
        A^n &= S D^n S^{-1}\\
      \end{align}
    \end{bem}
    
  \subsubsection{Einheitsmatrix}
  Die Einheitsmatrix (Identität) Besteht nur aus in der Diagonale Einsen, alle anderen Stellen sind mit Nullen aufgefüllt.
  \begin{equation}
    I = id = \left(\begin{array}{c c c c c}
    1 & \; &  0 & \; & 0\\
    \;& .  & \; & \; & 0\\
    0 & \; &  . & \; & 0\\
    \;& \; & \; & .  & 0\\
    0 & \;  & 0 & \; & 1\\
    \end{array}\right)
  \end{equation}
  
  Die Einträge an i-ter Zeile und j-ter Spalte sind über das Kronecker-Delta beschrieben.
  \begin{equation}
    \delta_{ij} = \begin{cases}
	    1,\quad i = j\\
	    0,\quad i\neq j\\
    \end{cases}
  \end{equation}
  
  \subsubsection{Inverse Matrix}
  Es gilt
  \begin{equation}
    A^{-1} = A
  \end{equation}
  \subsubsubsection{Bestimmung der Inversen bei $nxn$ Matrizen}
    $\Rightarrow$ Gauss Algorithmus angewandt auf $A|I$ bis $A$ zu $I$ umgeformt ist.
  \subsubsubsection{Inverse bei $2x2$ Matrizen}
    \begin{equation}
    \text{Mit } A = \left(\begin{array}{c c} a & b \\ c & d \\ \end{array}\right), A^{-1} = \frac{1}{detA} \mzxz{d}{-b}{-c}{a} = \frac{1}{ad-bc} \mzxz{d}{-b}{-c}{a}
    \end{equation}
    
   \subsubsection{Symmetrische Matrizen}
   \begin{definition}
     Eine reelle $nxn$ Matrix $A$ heißt symmetrisch, falls $A = A^T$.
   \end{definition}
   
   \subsubsection{Schiefsymmetrische Matrizen}
   \begin{definition}
     Eine Matrix $A$ heißt schiefsymmetrisch, falls $A^T = -A$.
   \end{definition}  
   \begin{bem}
     $detA = 0$, falls $n$ gerade
   \end{bem}
   \begin{bem}
     $x^TAx = 0$ für alle $x \in \R^n$
   \end{bem}
     
  \subsubsection{Hermitesche Matrizen}  
  \begin{definition}
    $A^* := \bar{A}^T$ \newline
    Eine komplexe $nxn$ Matrix $A$ heißt hermitesch, falls $A = \bar{A}^T$. \label{def:hermitesch}    
  \end{definition}
 
	\subsubsection{Transposition}
		Die Transposition besitzt folgende Eigenschaften
		\begin{equation}
		  \begin{tabularx}{14.7cm}{l l l}
			(\textbf{T1}) & $(A+B)^T = A^T + B^T$ &\\
			(\textbf{T2}) & $(\lambda A)^T = \lambda A^T$ &\\
			(\textbf{T3}) & $(A^T)^T = A$ &\\
			(\textbf{T4}) & $(AB)^T = B^T A^T$ & \\
			(\textbf{T5}) & $(A^T)^{-1} = (A^{-1})^T$ & \\
		  \end{tabularx}
		  \label{ax:det_transp}
	  \end{equation}
 
\subsection{Determinanten}
  \begin{satz}
    Determinantenentwicklungssatz: \newline
    $det(AB) = detA \cdot detB$
  \end{satz}
  \begin{bem} Aussagen zu Determinante einer Matrix 
	  $A \in \R^{nxn}$
	  \begin{flalign*}
	    &det(A) \neq 0 \leftrightarrow Rang(A) = n \leftrightarrow A^{-1} existiert&\\
	    &det(A) \neq 0 \leftrightarrow \text{ Zeilen bzw. Spalten linear unabhängig}&\\
	    &det(A^T) = det(A)&\\
	    &det(A^{-1}) = \displaystyle\frac{1}{det(A)}&\\
	    &det(\lambda A) = \lambda^n det(A)&\\
	    &det(A+B) \neq det(A) + det(B)&\\
	    &detA = \prod\limits_{j} \lambda_j &\\
	  \end{flalign*}
  \end{bem}  
  
  \subsubsection{Bestimmung der Determinanten}
	  \subsubsubsection{Determinante einer $2x2$ Matrix}
	  \begin{equation}
	    A = \mzxz{a}{b}{c}{d},\quad \det A = ad-bc
	  \end{equation}
	  
	  \subsubsubsection{Determinante einer $3x3$ Matrix}
	  Die Determinante wird über die Sarrussche Regel bestimmt:
  \begin{align}
    \det A &= \left|
      \begin{array}{c c c}
      A_{aa} & A_{ab} & A_{ac}\\
      A_{ba} & A_{bb} & a_{bc}\\
      A_{ca} & A_{cb} & A_{cc}\\
      \end{array}
    \right|    
    = \left|
      \begin{array}{c c c}
      \colGreen{A_{aa}} & \colGreen{A_{ab}} & \colGreen{A}_{\colBlue{ac}}\\
      A_{ba} & \colGreen{A}_{\colBlue{bb}} & \colGreen{A}_{\colBlue{bc}}\\
      \colBlue{A_{ca}} & \colBlue{A_{cb}} & \colGreen{A}_{\colBlue{cc}}\\
      \end{array}
    \right| 
    \begin{array}{c c}
      \colBlue{A_{aa}} & \colBlue{A_{ab}}\\
      \colGreen{A}_{\colBlue{ba}} & A_{bb}\\
      \colGreen{A_{ca}} & \colGreen{A_{cb}}\\
    \end{array} \nonumber \\
    \nonumber \\
    &\Rightarrow \det A =   \colGreen{A_{aa}} \colGreen{A_{bb}} \colGreen{A_{cc}} 
      + \colGreen{A_{ab}}   \colGreen{A_{bc}} \colGreen{A_{ca}}
      + \colGreen{A_{ac}}   \colGreen{A_{ba}} \colGreen{A_{cb}} \nonumber \\
      &\qquad \qquad \qquad- \colBlue{A_{ca}}    \colBlue{A_{bb}}  \colBlue{X_{ac}} 
      - \colBlue{A_{cb}}    \colBlue{A_{bc}}  \colBlue{A_{aa}}
      - \colBlue{A_{cc}}    \colBlue{A_{ba}}  \colBlue{A_{ab}}
  \end{align}
  
  \subsubsubsection{Determinante einer Matrix > $3x3$}
  \begin{satz}
    Determinantenentwicklungssatz: Für $A \in \R^{nxn}$ bezeichne $A_{ik}$ die aus $A$ durch Streichung der $i-ten$ Zeile und $k-ten$ Spalte entstehende Matrix. Dann gilt:
    \begin{align*}
      \det A = \sum_{k=1}^{n} (-1)^{i+k} a_{ik}\det A_{ik}
    \end{align*}
    wobei $i$ fest gewählt wird.
  \end{satz}
  
  Es bietet sich hierbei an $i$ so zu wählen, dass möglichst viele Teile der Entwicklung sich durch eine $0$ löschen.
  \begin{align*}
    det \mdxd{1 & 2 & 2}{3 & 4 & 5}{5 & 6 & 7} &= a_{11} det(A_{11}) (-1)^{1+1} + a_{12} det(A_{12})(-1)^2{1+2} + a_{13}det(A_{13})(-1)^{1+3} \\
    &= 1\cdot \left|\begin{array}{c c} 4 & 5 \\ 6 & 7 \\ \end{array}\right|
      -2 \cdot \left|\begin{array}{c c} 3 & 5 \\ 5 & 7 \\ \end{array}\right|
      + 2 \cdot \left|\begin{array}{c c} 3 & 4 \\ 5 & 6 \\ \end{array}\right|\\
    &= 1(-2)-2(-4)+2(-2) = 2
  \end{align*}
  
  \subsection{Eigenwerttheorie}
  \begin{definition}
    Eine Zahl $\lambda \in \C$ heißt Eigenwert (EW) der Matrix $A$, falls es einen Vektor $v \in \C^n$ gibt mit $Av = \lambda v$ und $v \neq 0$. $v$ heißt ein zum Eigenwert $\lambda$ gehöriger Eigenvektor (EV). Die Menge aller Eigenvektoren eines Eigenwertes $\lambda$ zusammen mit $0$ heißt der Eigenraum.
    \begin{equation}
      E_{\lambda} = \lbrace v \in \C^n | Av = \lambda v \rbrace
    \end{equation}      
  \end{definition}
  \begin{satz}
    $\lambda \in \C$ ist ein EW von $A$ genau dann, wenn
    \begin{equation}
      p_A(\lambda) = det(A-\lambda I) = 0 \label{eq:ew_charpol}
    \end{equation}
  \end{satz}
  
  \subsubsection{Bestimmung der Eigenwerte und Eigenvektoren}
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{\eqref{eq:ew_charpol} anwenden und NST bestimmen. Es gilt}&
  \end{flalign*}
  \begin{align}
    \text{Vielfachheit der NST} &= \text{algebraische Vielfachheit}; Schreibweise: \\ a(\lambda_j) &= x \in \R 
  \end{align}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Eigenvektoren zur Matrix $A$ bestimmen. Es gilt:}&
  \end{flalign*}
  \begin{align}
    Av = \lambda_j v \Rightarrow (A- \lambda_j I)v = 0
  \end{align}
  \begin{flalign*}
    &\textbf{Schritt 2.1: } \text{Gegebenenfalls geometrische Vielfachheiten ablesen(Anzahl an Nullzeilen), es gilt:}&
  \end{flalign*}
  \begin{align}
    \text{geometrische Vielfachheit} := g(\lambda_j) = dim(A_{\lambda_j}) = n-Rang(A - \lambda I)\label{meth:eigenwerte_eigenvektoren_best}
  \end{align}
  \begin{bem}
    Die Anzahl an Eigenvektoren pro Eigenwert müssen der geometrischen Vielfachheit entsprechen. Die Eigenvektoren pro Eigenwert müssen linear unabhängig sein. \label{meth:eigenwert_eigenvektor}
    \end{bem}
  \subsubsection{Basis des Eigenraumes bestimmen}
  Die Eigenvektoren bilden eine Basis des Eigenraumes. Es gilt:
  \begin{equation}
    S_1 = (v_1, v_2, ..., v_j)
  \end{equation}
  
  \begin{satz}
    Sei $A \in \C^{nxn}$ eine hermitesche Matrix (siehe \eqref{def:hermitesch}). Dann gilt:
    \begin{itemize}
      \item Die Eigenwerte von $A$ sind reell
      \item Die Eigenvektoren zu verschiedenen Eigenwerten sind orthogonal zueinander
    \end{itemize}     
  \end{satz}
  
  \subsubsection{Orthonormalbasis des Eigenraumes bestimmen (Gram Schmidt)}
  Im Folgenden sei davon ausgegangen, dass die Eigenvektoren nicht orthogonal zueinander stehen. Im Falle bereits orthogonaler Eigenvektoren kann auf die Orthogonalisierung verzichtet werden (nicht jedoch auf die Normierung).
  Es seien $v_1, v_2, v_3$ Eigenvektoren von $A$. 
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{Ersten Eigenvektor normieren}&
  \end{flalign*}
  \begin{align*}
    w_1 = \frac{v_1}{||v_1||}
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Zweiten Eigenvektor orthogonalisieren}&
  \end{flalign*}
  \begin{align*}
    \tilde{w_2} = v_2 - <w_1, v_2> w_1
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Zweiten Eigenvektor normieren}&
  \end{flalign*}
  \begin{align*}
    w_2 = \frac{\tilde{w_2}}{||\tilde{w_2}||}
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 4: } \text{Dritten Eigenvektor orthonormieren}&
  \end{flalign*}
  \begin{align}
    \tilde{w_3} = v_3 - <w_2, v_3&> w_2 - <w_1, v_3> w_1 \nonumber \\
    w_3 &= \frac{\tilde{w_3}}{||\tilde{w_3}||} \nonumber\\\label{meth:eigenraum_onb}
  \end{align}
  Das Verfahren kann auf beliebig viele Eigenvektoren erweitert werden. Die so erhaltenen Vektoren bilden eine Orthonormalbasis des Eigenraums:
  \begin{equation}
    S_2 = (w_1, w_2, w_3)
  \end{equation}
  Weiter gilt:
  \begin{equation}
    S_2 S_2^T = I, \qquad \text{\colBlue{da $S_2$ orthogonal ist}}
  \end{equation}
  und
  \begin{equation}
    D := \left(\begin{array} {c c c}
      \lambda_1 & 0         & 0 \\
      0         & \lambda_2 & 0 \\
      0         & 0         & \lambda 3 \end{array} \right) 
      = S_2^T A S_2 \label{eq:diag_matrix}
  \end{equation}
  und somit
  \begin{align}
    A^{-1} = (S_2 D S_2^T)^{-1} = (S_2^{-1})^{-1} D^{-1} S_2^{-1} = S_2 D^{-1} S_2^T
  \end{align}
  
  \subsubsection{Projektion}
  \begin{bem}
  Projektion von x auf w:
  \begin{equation}
    P(x) = \sum\limits_{i=1}^m <v_i, x> v_i    
  \end{equation}
  \end{bem}
  
  \subsection{Differentialgleichungssysteme}
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{Überführung in Matrixdarstellung}&
  \end{flalign*}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Eigenwerte und Eigenvektoren bestimme (siehe \eqref{meth:eigenwert_eigenvektor}}&
  \end{flalign*}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Allgemeine Lösung bilden:}&
  \end{flalign*}
  \begin{align*}
    x(t) = Sy(t) = (v_1, ..., v_n)\vecT{C_1 e^{\lambda_1 t} \\ \vdots \\ C_n e^{\lambda_n t}} = C_1 e^{\lambda_1 t} + ... + C_n e^{\lambda_n t}
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 4: } \text{Anfangsbedingungen (z.B. $x^* = 0$) einsetzen und bestimmen}&
  \end{flalign*}
  \begin{align*}
     &S:= (v_1, ..., v_n) \\
     x(0) = &Sy(x^*) = \left(\begin{array}{c c c} \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot \\ \end{array}\right) \cdot \vecT{C_1 \\ C_2 \\C_3} = x^* 
  \end{align*}
  
  \subsection{Quadriken bestimmen}
    \subsubsection{Methode}
    \begin{flalign*}
    &\textbf{Schritt 1: } \text{Diagonalmatrix $A$, Vektor $b$ und Konstante $c$ ablesen}&
  \end{flalign*}
    \begin{align*}
    \textbf{Beispiel:}\\
      \colGreen{-1}x_1^2 \colGreen{-1}x_2^2 + \colGreen{1}x_3^2 + \colRed{6}x_1x_2 + 2x_1x_2 + \colBord{2}x_1x_3 + &\colBlue{2}x_2x_3 -12x_1 + 4x_2 -10x_3 - 11 = 0 \\
      A = \left(\begin{array}{c c c}
      \colGreen{-1} & \colRed{3}  & \colBord{1}\\
      \colRed{3}  & \colGreen{-1} & \colBlue{1}\\
      \colBord{1}  & \colBlue{1}  & \colGreen{1} \\
      \end{array} \right), b &= \vecT{-12 \\ 4 \\ -10}, c = 11\\
    \end{align*}
    Im Zuge der Vorlesung gilt immer $A = A^T$.
    \begin{flalign*}
    &\textbf{Schritt 1.1: } \text{Als Quadrik aufschreiben}&
    \end{flalign*}
    \begin{align*}
      q(x) &= x^TAx+b^Tx + c\\
      &= \vecT{x_1 \\ x_2 \\ x_3}^T \left(\begin{array}{c c c}
      -1 & 3  & 1\\
      3  & -1 & 1\\
      1  & 1  & 1 \\
      \end{array} \right) + \vecT{-12 \\ 4 \\ -10}^T \vecT{x_1 \\ x_2 \\ x_3} + 11
     \end{align*}
     \begin{flalign*}
      &\textbf{Schritt 2: } \text{Diagonalmatrix $\Lambda$ bestimmen}&
     \end{flalign*}
     Dazu nach \eqref{meth:eigenwerte_eigenvektoren_best} Eigenwerte und Eigenvektoren bestimmen, nach \eqref{meth:eigenraum_onb} eine Orthonormalmatrix mit den Eigenvektoren erzeugen und schließlich nach \eqref{eq:diag_matrix} die Diagonalmatrix $\Lambda$ bilden.
     \begin{flalign*}
      &\textbf{Schritt 3: } \text{Vektor $\tilde{b}$ bestimmen sodass gilt $q(Sy) = y^T\Lambda y + \tilde{b}^T y + c$:}&
     \end{flalign*}
     \begin{align*}
       q(Sy) &= (Sy)^T ASy + \tilde{b}^T Sy + c = y^T S^T ASy + (S^T \tilde{b})^T y + c \\
       &= y^T\Lambda y + \tilde{b}^T y + c \Rightarrow \tilde{b} := S^T b
     \end{align*}
     \begin{flalign*}
      &\textbf{Schritt 4: } \text{In $q(Sy) = y^T\Lambda y + \tilde{b}^T y + c$ einsetzen:}&
     \end{flalign*}
     \begin{align*}
       q(Sy) &= y^T\Lambda y + \tilde{b}^T y + c \\
       &= \vecT{y_1 \\ y_2 \\ y_3}^T 
       \left(\begin{array}{c c c}
       \lambda_1 & 0         & 0 \\
       0         & \lambda_2 & 0 \\
       0         &  0        & \lambda_3
       \end{array}\right) \vecT{y_1 \\ y_2 \\ y_3} + \vecT{\tilde{b_1} \\ \tilde{b_2} \\ \tilde{b_3}} + c
     \end{align*}
     \begin{flalign*}
      &\textbf{Schritt 5: } \text{Ausmultiplizieren und quadratisch ergänzen}&
     \end{flalign*}
     Hierbei ist wichtig, dass die quadratischen Anteile ohne Faktor in das Binom übergehen (also vorher ausklammern).
     \begin{flalign*}
      &\textbf{Schritt 6: } \text{Quadrik anhand nachfolgender Tabellen ablesen}&
     \end{flalign*}
     
     \subsubsection{Normalformen von Quadriken im $\R^2$}
     \formTab{Ellipse}{\displaystyle\left(\frac{x_1}{a}\right)^2 + \left(\frac{x_2}{b}\right)^2 - 1 = 0}
     \formTab{Kreis}{x_1^2 + x_2^2 = r}
     \formTab{Schneidendes Geradenpaar}{\displaystyle\frac{x_1^2}{a^2} - \frac{x_2^2}{b^2} = 0}
     \subsubsection{Normalformen von Quadriken im $\R^3$}
     \formTab{Ellipsoid}{x^2 +y^2 +z^2 -1 = 0}
     \formTab{Einschaliges Hyperboloid}{x^2 + y^2 -z^2 -1 = 0}
     \formTab{Zweischaliges Hyperboloid}{x^2+y^2-z^2 + 1 = 0}
     \formTab{Elliptisches Paraboloid}{x^2 + y^2 -2z = 0}
     \formTab{Hyperbolisches Paraboloid}{x^2 - y^2 -2z = 0}
     
     
     
  \subsection{Äquivalenzaussagen}
  \begin{satz}
  Für $A \in \R^{dxd}$ sind folgende Aussagen äquivalent
  \begin{description}
   \item[a)] $A$ ist regulär
   \item[b)] Rang von $A$ ist gleich $d$
   \item[c)] Die Zeilenvektoren von $A$ sind linear unabhängig
   \item[d)] $Ax = 0$ hat nur $x=0$ als Lösung
   \item[e)] $Ax = b$ ist eindeutig lösbar
   \item[f)] Die Spaltenvektoren sind linear unabhängig
   \item[g)] Die Matrix besitzt eine Inverse $A^{-1}$
   \item[h)] Die Determinante von $A$ ist ungleich $0$
   \item[i)] $A$ besitzt keinen Eigenwert $0$
  \end{description}
  \end{satz}
  \newpage

\section{Mehrdimensionale Analysis}
  \subsection{Partielle Ableitung}
  \begin{definition}
    $f(x)$ heißt in $x^* \in \R^d$ nach der $j-ten$ Koordinate $x_j$ partiell diffbar, falls der Grenzwert
    \begin{equation}
      \frac{\partial f}{\partial x_j}(x^*) = \lim\limits_{t\rightarrow 0} \frac{f(x^* + t e_j) - f(x^*)}{t}
    \end{equation}
    existiert bzw. falls $\tilde{f}_j(x_j)$ in $x_j$ diffbar ist. $\frac{\partial f}{\partial x_j}(x^*)$ heißt die partielle Ableitung von $f$. 
  \end{definition}
  \begin{bem}
    Alternative Schreibweisen:
    \begin{equation}
      \frac{\partial f}{\partial x_j} = \frac{partial}{\partial x_j} f = \partial_{x_j} f = D_j f  = f_{x_j} = ...
    \end{equation}
  \end{bem}
  \begin{definition}
    Allgemein: n-te partielle Anleitung:
    \begin{equation}
      \frac{\diffp^2f}{\diffp x_k \diffp x_j} = \frac{\diffp f}{\diffp x_k}\left(\frac{\diffp f}{\diffp x_k} \right)
    \end{equation}
  \end{definition}
  
  \subsubsection{Gradient und Nabla Operator}
  \begin{definition}
    Der Zeilenvektor
    \begin{equation}
      \grad f(x^*) = \left( \dfp{f}{x_1}(x^*), ..., \dfp{f}{x_d}(x^*)\right)
    \end{equation}
    heißt der Gradient von $f$ in $x^*$. Es gilt weiter:
    \begin{equation}
      \nabla f(x^*) = (\grad f(x^*))^T
    \end{equation}
    Der eingeführte Operator heißt Nabla-Operator.
  \end{definition}
  \begin{satz}
    Sei $f:\R^d \rightarrow \R$ diffbar, dann gilt:
    \begin{itemize}
      \item[a) ] Der Gradientenvektor $\nabla f(x^*)$ steht senkrecht auf der Niveaumenge $N_{x^*} = \lbrace x \in \R^d: f(x) = f(x^*)\rbrace$
      \item[b) ] $\nabla f(x^*)$ gibt die Richtung des steilsten Anstiegs von $f(x)$ im Punkt $x^*$ an.
    \end{itemize}
  \end{satz}
  \begin{bem}
    Es gilt weiterhin:
    \begin{align}
      \grad(\alpha f + \beta g) &= \alpha \grad f + \beta \grad f, \qquad \alpha, \beta \in \R    \\  
      \grad (f\cdot g) &= f \cdot \grad g + g \cdot \grad f
    \end{align}
  \end{bem}
  
  \subsubsection{Jacobi Matrix}
  \begin{definition}
    Die Matrix der ersten Ableitung heißt die Jacobi Matrix. 
    \begin{equation}
      Jf = \left(\begin{array}{c c c}
      \dfp{f_1}{x_1} & \dots  & \dfp{f_1}{x_d} \\
      \vdots         & \ddots & \vdots         \\
      \dfp{f_m}{x_1} & \dots  & \dfp{f_m}{x_d} \end{array}\right)
    \end{equation}
  \end{definition}
  
  \subsection{Richtungsableitung}
  \begin{definition}
    Sei $f: \R^d \rightarrow \R$. Für $x^* \in \R^d$ und $v \in \R^d _{/ \lbrace 0 \rbrace}$ heißt
    \begin{equation}
      D_v f(x^*) = \lim\limits_{t\rightarrow 0} \frac{f(x^* +tv)-f(x^*)}{t}
    \end{equation}
    die Richtungsableitung von $f$ in $x^*$ in Richtung $v$. (Andere Schreibweise: $\dfp{f}{v})$
  \end{definition}
  Wenn alle Ableitungen existieren gilt:
  \begin{equation}
    D_v f = \grad f \cdot v
  \end{equation}
  \begin{bem}
    Der Gradient zeigt dabei selbst in Richtung des steilsten Anstiegs.
  \end{bem}
  
  \begin{satz}
    Ist $f: D\rightarrow \R$ mit $D \subset \R^d$ offen in einer Umgebung von $x^* \in D$ partiell diffbar und sind die partiellen Ableitungen dort beschränkt, dann ist $f$ stetig in $x^*$.
  \end{satz}
  
  \begin{satz}
    Existieren in einer Umgebung von $x^*$ alle partiellen Ableitungen und sind dann stetig in $x^*$, so ist $f$ diffbar in $x^*$. Allgemein gilt, existieren alle partiellen Ableitungen bis zur Ordnung $k$ und sind diese stetig, so ist $f \in C^k$, d.h. $k-fach$ stetig diffbar.
  \end{satz}
  
  \begin{bem}
    Folgerung: Alle Polynome, alle sin, cos, exp, ... Funktionen sind mehrdimensional diffbar.
  \end{bem}
  
  \begin{bem}
    Ist $f:\R^2 \rightarrow \R$ in einer Umgebung $U$ von $x^*$ partiell diffbar und sind diese in $x^*$ stetig, so ist $f$ diffbar in $x^*$.
  \end{bem}
  
  \subsection{Mehrdimensionale Kettenregel}
  \begin{satz}
  Sei $f: \R^n \rightarrow \R^m$ und $g: \R^m \rightarrow \R^d$. Sei $f$ in $x^*$ diffbar und $g$ in $y^* = f(x^*)$ diffbar. Dann ist $g \circ f$ in $x^*$ diffbar mit Jacobimatrix.
  \begin{equation}
    J(g \circ f) (x^*) = Jg(f(x^*)) \cdot Jf(x^*)\label{eq:mehrd_kettenr_jac}
  \end{equation}
  Somit gilt für $h(x) = g(f(x))$, dass
  \begin{equation}
    \dfp{z_{\colRed{k}}}{x_{\colBord{j}}} = \sum\limits_{\colBlue{i}} \dfp{z_{\colRed{k}}}{y_{\colBlue{i}}} \cdot \dfp{y_{\colBlue{i}}}{x_{\colBord{j}}}
  \end{equation}
  \end{satz}
  Vorgehen mit Jacobimatrix:\newline
  Es sei gegeben:
  \begin{align*}
    g: \R^2 \rightarrow \R ,\quad g(x,y) = f(x^2y, x+2y)
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{Zur Übersichtlichkeit gegebenenfallsneue Funktionen definieren}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    q(x,y):= (x^2y, x+2y) \Rightarrow g(x,y) = f(q(x,y))
  \end{align*}
  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Jacobimatrix der inneren Funktion aufstellen}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    Jq(x,y) = \left( \begin{array}{c c}
    2yx & x^2 \\
    1 & 2
    \end{array} \right)
  \end{align*}
  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Jacobimatrix der äußeren Funktion in Abhängigkeit von der Inneren aufstellen}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    J_f(q(x,y)) = \left(\begin{array}{c c}
      \frac{\diffp f}{\diffp x}(q(x,y)) & \frac{\diffp f}{\diffp y} (q(x,y))
    \end{array}\right)
  \end{align*}
   \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 4: } \text{\eqref{eq:mehrd_kettenr_jac} anwenden}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    J_g(x,y) = J_f(q(x,y)) \cdot J_q(x,y) = \left(\begin{array}{c c}
      \frac{\diffp f}{\diffp x}(q(x,y)) & \frac{\diffp f}{\diffp y} (q(x,y))
    \end{array}\right) \cdot
    \left( \begin{array}{c c}
    2yx & x^2 \\
    1 & 2
    \end{array} \right)
  \end{align*}  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 5: } \text{Ausmultiplizieren}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    J_g(x,y) &= 
    \left(
    \begin{array}{c c}
      2yx \frac{\diffp f}{\diffp x}(q(x,y)) + \frac{\diffp f}{\diffp y} (q(x,y)) & x^2 \frac{\diffp f}{\diffp x}(q(x,y)) + 2 \frac{\diffp f}{\diffp y (q(x,y))}
    \end{array}
    \right)\\
    \Rightarrow \frac{\diffp g}{\diffp x} (x,y) &= 2yx \frac{\diffp f}{\diffp x}(q(x,y)) + \frac{\diffp f}{\diffp y} (q(x,y)) \qquad \frac{\diffp g}{\diffp y} = x^2 \frac{\diffp f}{\diffp x}(q(x,y)) + 2 \frac{\diffp f}{\diffp y (q(x,y))}
  \end{align*}  
  
  \subsection{Wichtige Operatoren}
  \subsubsection{Divergenz}
  \begin{align}
    f: \R^d \rightarrow \R^d, f = (f_1, ..., f_d), x = (x_1, ..., x_d) \nonumber \\
    \diverg f = \sum\limits_{j=1}{d} \dfp{f_j}{x_j} = \dfp{f_1}{x_1} + \dfp{f_2}{x_2} + ... + \dfp{f_d}{x_d}
  \end{align}
  \begin{bem}
   Die Divergenz ist die Spur der Jacobimatrix, also die Summe der Diagonalelemente.
  \end{bem}
  
  \subsubsection{Laplace-Operator}
  \begin{align}
    &\varphi: \R^d \rightarrow \R \nonumber \\
    &\laplace \varphi = \nabla \nabla \varphi= \sum\limits_{j=1}^d \frac{diffp^2 \varphi}{\diffp x_j^2} = \frac{\diffp^2 \varphi}{\diffp x_1^2} + ... + \frac{\diffp^2 \varphi}{\diffp x_d^2}
  \end{align}
  \begin{bem}
   Der Laplace Operator bildet die Spur der Hesse Matrix.
  \end{bem}
  
  \subsubsection{Rotation}
  \begin{align}
    f:\R^3 \rightarrow \R^3 \nonumber \\  
    \rot f = \left( \begin{array}{c}
    \displaystyle\dfp{f_3}{x_2} - \dfp{f_2}{x_3} \\
    \displaystyle\dfp{f_1}{x_3} - \dfp{f_3}{x_1} \\
    \displaystyle\dfp{f_2}{x_1} - \dfp{f_1}{x_2}
    \end{array} \right)
  \end{align}
  \begin{bem}
    Die Bestandteile der Rotation sind die Nebendiagonalelemente der Jacobimatrix. Die Rotation gibt an wie \enquote{schief} die Jacobimatrix ist.
  \end{bem}    

  
  \subsection{Lemma von Schwarz}
  \begin{satz}
    Sei $f: \R^d \rightarrow \R$ eine $C^2$-Funktion, so gilt für alle $i,j \in \lbrace 1, ..., d\rbrace$:
    \begin{equation}
      \frac{\diffp^2 f}{\diffp x_i \diffp x_j} = \frac{\diffp^2 f}{\diffp x_j \diffp x_i}
    \end{equation}
  \end{satz}
  \begin{bem}
    Allgemein: Ist $f \in C^k$, dann ist die Reihenfolge des Differezierens bis zur $k-ten$ Ordnung egal.
  \end{bem}
  
  \subsection{Taylorscher Satz}
  \subsubsection{Wiederholung eindimensionaler Taylorscher Satz}
  \begin{equation}
    T(f, x, x^*) = \sum\limits_{k= 0}^n \frac{f^{(k)}(x^*)}{k!} (x-x^*)^k
  \end{equation}
  \subsubsection{Mehrdimensionaler Mittelwertsatz}
  \begin{satz}
    Sei $f: \R^d \rightarrow \R$ (hier wichtig: $\R$, nicht $\R^m$, $m\geq 2$) diffbar. Dann gibt es ein $\Theta\in (0,1)$ mit 
    \begin{equation}
      f(b) - f(a) = \ubGreen{\in \R^d}{\grad f(a+\Theta (b-a))}\ubGreen{\in \R^d}{(b-a)}
    \end{equation}
  \end{satz}
  \begin{bem}
    Der MWS gilt nicht für vektorwertige Funktionen.
  \end{bem}
  
  \subsubsection{Mehrdimensionaler Taylorscher Satz}
  \begin{definition}
    Offen: Um jeden Punkt $x \in D$ lässt sich eine Kugel in $D$ legen mit $r < 0$.
  \end{definition}
  \begin{definition}
    Konvex: Zu $x, y \in D$ ist auch die Verbindungsstrecke in $D$.
  \end{definition}
      
  \begin{satz}
    Sei $f: D\rightarrow \R$ eine $C^{m+1}$ Funktion auf einer offenen und konvexen Menge $D \in \R^d$ und $x^* \in D$. Dann gilt:
    \begin{align}
    &f(x) = Tm(x, x^*) + Rm(x, x^*) \\
    \text{mit} \nonumber \\
    &Tm(x, x^*) = \sum\limits_{j=0}^{m} \sum\limits_{j_1+j_2+...+j_d = j} \frac{1}{j_1!j_2!...j_d!} (x_1 - x_1^*)^{j_1} ... (x_d - x_d^*)^{j_d} \frac{\diffp^m f(x^*)}{\diffp x_1^{j_1} ... \diffp x_d^{j_d}} &\\
    \text{und} \nonumber \\
    &Rm(x, x^*) = \sum\limits_{j_1+j_2+...+j_d = m+1} \frac{1}{j_1!j_2!...j_d!} (x_1 - x_1^*)^{j_1} ...\nonumber \\ 
    &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot(x_d - x_d^*)^{j_d} \frac{\diffp^m f(x^*)}{\diffp x_1^{j_1} ... \diffp x_d^{j_d}}(x^* + \Theta(x-x^*))
    \end{align}
  \end{satz}
  
  \begin{bem}
    Taylorpolynome sind lokal die beste Approximation.
    \begin{equation}
      T1(x, x^*) = f(x^*) + \dfp{f}{x_1}(x^*)(x_1-x_1^*) + ... + \dfp{f}{x_d}(x_d-x_d^*)
    \end{equation}
    ist die Tangentialebene an $x^*$.
  \end{bem}
  
  \begin{bem}
    Zum Berechnen der Taylorpolynome ist es oft einfacher bekannte 1-dim Taylorpolynome bzw. Taylorreihne zu verwenden. Es git dann:
    \begin{equation}
      Tm(f, x, x^*) = T_a(f_a, x, x^*) \cdot ... \cdot T_d(f_d, x, x^*)
    \end{equation}
    Wobei auch mehrere Dimensionen mit einer 1-dim Reihe substituiert werden können.
  \end{bem}
  
  \subsubsection{Wichtige 1-dim Reihen}
  \begin{itemize}
    \item Exponentialreihe:
    \begin{equation}
      e^x = \sum\limits_{k=0}^\infty \frac{x^k}{k!} \qquad \forall x \in \R
    \end{equation}
    \item Geometrische Reihe
    \begin{equation}
      \frac{1}{1-q} = \sum\limits_{k = 0}^\infty q^k \qquad |q| < 1
    \end{equation}
    \item Sinusreihe
    \begin{equation}
      sin(y) = \sum\limits_{k = 0}^\infty \frac{(-1)^k y^{2k+1}}{(2k+1)!} = y -\frac{1}{3!}y^3 + \frac{1}{5!}y^5 + ...
    \end{equation}
    \item Cosinusreihe
    \begin{equation}
      cos(y) = \sum\limits_{k = 0}^\infty = (-1)^k \frac{y^2k}{(2k)!} = 1 - \frac{1}{2}y^2 + \frac{1}{4!}y^4 + ...
    \end{equation}
  \end{itemize}
  
  \subsection{Mehrdimensionale Extremwertaufgaben}
  Im Folgenden sei $f: \R^d \rightarrow \R$. 
  \begin{satz}
    Besitzt $f = f(x)$ in einem Punkt $x^*$ ein lokales Extremum (d.h. Minimum oder Maximum), so gilt
    \begin{equation}
      \nabla f(x^*) = 0
    \end{equation}   
  \end{satz}
  \begin{bem}
    $\nabla f(x^*) = 0$ liefert häufig keine Extrema sondern Sattelpunkte (je höher die Raumdimension desto wahrscheinlicher, dass kein Extrema vorliegt).
  \end{bem}   
  $f$ um $x^*$ lässt sich besser verstehen in dem man das Taylorpolynom 2. Grades um $x^*$ betrachtet. Dieses lässt sich mit Hilfe der Hesse-Matrix folgendermaßen formulieren:
  \begin{equation}
    T2(x, x^*) = f(x^*) + \nabla f(x^*)^T(x-x^*) + \frac{1}{2}(x-x^*)^T Hf(x^*)(x-x^*)
  \end{equation}
  Vorgehen zur Bestimmung von $T_2$: \newline
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{Gradient von $f$ bestimmen}&
  \end{flalign*}
  \vspace{-0.5cm}
   \begin{flalign*}
    &\textbf{Schritt 2: } \text{Hesse Matrix von $f$ bestimmen}&
  \end{flalign*}
  \vspace{-0.5cm}
   \begin{flalign*}
    &\textbf{Schritt 3: } \text{$x^*$ einsetzen und ausrechnen}&
  \end{flalign*}
  \subsubsection{Hesse Matrix}
  Die Hesse Matrix ist die Matrix der zweiten Ableitung.
  \begin{equation}
    Hf(x^*) = J(Jf(x^*)) = f''(x^*) = \left(\begin{array}{c c c} 
    \displaystyle \frac{\diffp^2f}{\diffp x_1 \diffp x_1} & \dots & \displaystyle\frac{\diffp^2 f}{\diffp x_1 \diffp x_d} \\
    \vdots & \ddots & \vdots \\
    \displaystyle\frac{\diffp^2 f}{\diffp x_d \diffp x_1} & \dots & \displaystyle\frac{\diffp^2 f}{\diffp x_d \diffp x_d} \end{array} \right)
  \end{equation}
  \begin{bem}
    Die Hesse Matrix ist symmetrisch. Daher besitzt sie reelle Eigenwerte und kann durch eine orthogonale Transformation diagonalisiert werden.
  \end{bem}
  \begin{bem}
    \begin{align}
      &\forall \lambda_j > 0 \Rightarrow \text{Minimum} \nonumber \\
      &\forall \lambda_j < 0 \Rightarrow \text{Maximum} \nonumber \\
      &\exists \lambda_j \text{ und } \lambda_i \text{ mit unterschiedliche Vorzeichen } \Rightarrow \text{Sattelpunkt} \nonumber \\
      \text{Umkehrung:} \nonumber \\
      &\text{Minimum} \Rightarrow \forall \lambda_j| \lambda_j \geq 0 \nonumber \\
      &\text{Maximum} \Rightarrow \forall \lambda_j| \lambda_j \leq 0 \nonumber \\ \label{eq:definitheit_ew}
    \end{align}
  \end{bem}
  \begin{satz}
    Sei $f \in C^2$ mit $\nabla f(x^*) = 0$ ($x^*$ ist ein kritischer Punkte).
    \begin{description}
      \item[a)] Ist $x^*$ ein lokales Minimum, so ist $Hf(x^*)$ positiv semidefinit, d.h. es gilt $v^T Hf(x^*)v \geq 0\quad \forall v \in \R^d$.
      \item[b)] Ist $x^*$ ein lokales Maximum, so ist $Hf(x^*)$ negativ semidefinit, d.h. es gilt $v^T Hf(x^*)v \leq 0\quad \forall v \in \R^d$.
      \item[c)] Ist $Hf(x^*)$ positiv definit, d.h. es gilt $v^T Hf(x^*)v > 0\quad \forall v \in \R^d_{/\lbrace 0 \rbrace}$, so ist $x^*$ ein Minimum.
      \item[d)] Ist $Hf(x^*)$ negativ definit, d.h. es gilt $v^T Hf(x^*)v < 0\quad \forall v \in \R^d_{/\lbrace 0 \rbrace}$, so ist $x^*$ ein Maximum.
    \end{description} \label{satz:definitheit_1}
  \end{satz}
  
  \begin{bem}
    Für $f: \R^2 \rightarrow \R$ ist die Hessematrix eine $2x2$ Matrix.
    \begin{equation}
      \Rightarrow \spur Hf = \lambda_1 + \lambda_2, \qquad \det Hf = \lambda_1 \cdot \lambda_2
    \end{equation}
  \end{bem}
  
  \begin{satz}
    Hurwitz Kriterium für $2x2$ Matrizen: Sei $f:\R^2 \rightarrow \R \in \C^2$, $x^*$ sei ein kritischer Punkt von $f$ und sei $\det Hf(x^*) > 0$, dann gilt:
    \begin{align}
      \diffp^2_{x_1} f(x^*) > 0 \Rightarrow \text{lokales Minimum} \nonumber \\
      \diffp^2_{x_1} f(x^*) < 0 \Rightarrow \text{lokales Maximum} \nonumber \\
      \label{satz:hurwitz_2x2}
    \end{align}
  \end{satz}
  
  \subsubsection{Untersuchung nach Extremwerten}
    \begin{flalign*}
    &\textbf{Schritt 1: } \text{Gradient und Hesse Matrix von $f$ bestimmen:}&
  \end{flalign*}
  \begin{align*}
    \nabla f(x) = \vecT{\dfp{f}{x_1}(x) \\ \vdots \\ \dfp{f}{x_d}(x^*)}, \qquad Hf(x) = \left(\begin{array}{c c c} 
    \displaystyle \frac{\diffp^2f}{\diffp x_1 \diffp x_1} & \dots & \displaystyle\frac{\diffp^2 f}{\diffp x_1 \diffp x_d} \\
    \vdots & \ddots & \vdots \\
    \displaystyle\frac{\diffp^2 f}{\diffp x_d \diffp x_1} & \dots & \displaystyle\frac{\diffp^2 f}{\diffp x_d \diffp x_d} \end{array} \right)
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Kritische(n) Punkt(e) bestimmen}&
  \end{flalign*}
  \begin{align*}
    \nabla f(x) = 0
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Kritische Punkte in Hessematrix einsetzen und auswerten}&
  \end{flalign*}
  Abhängig der Dimension der Matrix \eqref{satz:hurwitz_2x2}, \eqref{satz:definitheit_1} oder \eqref{eq:definitheit_ew} anwenden.
  \newpage
  
  \subsection{Satz über implizite Funktionen}
  Begrifflichkeit: \qquad
  \begin{tabular}{l l l}
    $g(x,y) = 0$ &: & implizite Darstellung \\
    $y = y(x)$ &: & explizite Darstellung  
  \end{tabular}
  \begin{satz}
    Sei $F(x,y) \in C^1$ in einer Umgebung von $N_0(x_0,y_0)$ sodass
    \begin{align*}
      &\textbf{(1) } F(N_0) = 0 \\
      &\textbf{(2) } \frac{\diffp F}{\diffp y}(N_0) \neq 0
    \end{align*}     
    Dann existiert eine eindeutige Funktion $y = f(x) \in C^1$ in einer Umgebung $U$ von $x_0$ sodass:
    \begin{align*}
      &\textbf{(i  ) } y_0 = f(x_0) \\
      &\textbf{(ii ) } F\left(x,f(x)\right) = 0 \quad \forall x \in U \\
      &\textbf{(iii) } \frac{\diffp f}{\diffp x} = - \frac{\frac{\diffp F}{\diffp x_i} \left(x, f(x)\right)}{\frac{\diffp F}{\diffp y}\left(x,f(x)\right)} ,\qquad (i = 1,2,3,...,n)
    \end{align*}
  \end{satz}
  \subsubsection{Satz über implizite Funktionen für Gleichungssysteme}
  \begin{satz}
    Sei $\laplace \neq 0$, dann existieren in einer Umgebung von $(x^0, z^0)$ eindeutige Funktionen $z = f_i (x_1, ..., x_n)$ mit $i = 1, ..., m$ und $z_i \in C^1$ und die partiellen Ableitungen können mit impliziter Differentialrechnung gebildet werden.
  \end{satz}
  \begin{satz}
    Sei $g:\R^d \rightarrow \R^m$ eine $C^1$-Funktion. Sei $x \in \R^{d-m}$und $y \in \R^m$. 
    \begin{itemize}
      \item[V1) ] Sei $(x^*, y^*)$ eine Lösung, d.h. $g(x^*,y^*) = 0$
      \item[V2) ] Weiter sei 
      \begin{equation*}
        \frac{\diffp g}{\diffp y}\bigg|_{(x^*,y^*)} 
        \left(
          \begin{array}{c c c}
            \frac{\diffp g_1}{\diffp y_1} & \dots & \frac{\diffp g_1}{\diffp y_m} \\
            \vdots & \ddots & \vdots \\
            \frac{\diffp g_m}{\diffp y_1} & \dots & \frac{\diffp g_m}{\diffp y_m} 
          \end{array}
        \right) \Bigg|_{(x^*,y^*)}
      \end{equation*}
    \end{itemize}
    Dann gibt es Umgebungen $U$ um $x^*$ und $V$ um $y^*$ in denen eine eindeutige $C^1$-Funktion $f:\underbrace{\R^{d-m}}_x \rightarrow \underbrace{\R}_{y}$ existiert mit $f(x^*)= y^*$ und $g(x,f(x)) = 0 \forall x \in U$.
  \end{satz}
  \subsubsubsection{Vorgehen für die Konstruktion einer Tangente}
  Es sei gegeben:
  \begin{align*}
    K = \lbrace(x,y) \in \R^2: x^3-xy+y^2  = 3\rbrace \qquad P = (1,2)
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{Ableitung bilden:}&
  \end{flalign*}
  \begin{flalign*}
    &\frac{\diffp}{\diffp x} x^3 \frac{\diffp}{\diffp x} xy + \frac{\diffp}{\diffp x}y^2 = \frac{\diffp}{\diffp x}3 = 0 &\\
    &\Rightarrow 3x^2 - (x\frac{\diffp y}{\diffp x} + y) + 2y \frac{\diffp y}{\diffp x} = 0 &&\Rightarrow 3x^2 - x \frac{\diffp y}{\diffp x} - y + 2y \frac{\diffp y}{\diffp x} = 0& \\
    &\Rightarrow 3x^2 - y + \frac{\diffp y}{\diffp x} (2y - x) = 0 &&\Rightarrow \frac{\diffp y}{\diffp x} = \frac{y - 3x^2}{2y - x}&
  \end{flalign*}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Werte einsetzen und Steigung berechnen}&
  \end{flalign*}
  \begin{align*}
    \frac{2-3}{4-1} = -\frac{1}{3}
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Geradengleichung aufstellen}&
  \end{flalign*}
  \begin{align*}
     \Rightarrow t = 2 + \frac{1}{3} = \frac{7}{3} \\
     \Rightarrow y = - \frac{1}{3} x + \frac{7}{3}
  \end{align*}
  \subsubsubsection{Anwenden des Satzes}
  Es sei gegeben:
  z.Z. $U \subset \R$ existiert mit $1 \in U$ und es existiert eine diffbare Funktion $f: U \rightarrow \R$, sodass $\lbrace (x,f(x)): x \in U \rbrace \subset K \rbrace$.
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{\textbf{(1)} prüfen:}&
  \end{flalign*}
  \begin{align*}
    F(x,y) = x^3 - xy + y^2 -3 \\
    \Rightarrow F(1,2) = 1-2+4-3 = 0 \checkmark
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{\textbf{(2)} prüfen:}&
  \end{flalign*}
  \begin{align*}
    \frac{\diffp}{\diffp y} F(1,2) = x + 2y = -1 + 4 = 3 \neq 0 \checkmark
  \end{align*}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Folgerung aufschreiben}&
  \end{flalign*}
  Hier: Nach S.ü.i.F. ex. ein solches Intervall $U$ und es ex. $f: U\rightarrow \R$ bzw. $y = f(x)$.
  \subsubsubsection{Vorgehen für Systeme}
  Gegeben sei:
  \begin{align*}
  \begin{array}{c}
    x^2 y^2 + zu + yv^2 = 3 \\
    y + 2xv - u^2v^2 = 2
  \end{array}
  \quad (x,y,z,u,v) = (1,1,1,1,1) \quad (x,y,z) = (1,1,1)
\end{align*}    
  Z.Z.: In einer Umgebung von $(x,y,z,u,v) = (1,1,1,1,1)$ können $u$ und $v$ eindeutig als Funktionen von $x$, $y$ und $z$ bestimmt werden.
  \begin{flalign*}
    &\textbf{Schritt 1: } \text{System als Gleichungen aufschreiben}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    \begin{cases}
      x^2 y^2 + zu + yv^2 = 3 \\
      y + 2xv - u^2v^2 = 2
    \end{cases} = 
    \begin{cases}
    F_1 ( x,y,z,u,v) = x^2 y^2 + zu + yv^2 -3 \\
    F_2 ( x,y,z,u,v) = y + 2xv - u^2v^2 - 2
    \end{cases}
  \end{align*}
  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 2: } \text{Prüfen ob der Punkt die Gleichungen erfüllt}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    \Rightarrow \begin{cases}
    F_1 ( 1,1,1,1,1) = x^2 y^2 + zu + yv^2 -3 = 1+1+1-3 = 0 \checkmark \\
    F_2 ( 1,1,1,1,1) = y + 2xv - u^2v^2 - 2 = 1 + 2 - 1 - 2 = 0\checkmark
    \end{cases}
  \end{align*}
  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 3: } \text{Prüfen ob eine Inverse existiert}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    \nabla = \left| \begin{array}{c c}
    \frac{\diffp F_1}{\diffp u} & \frac{\diffp F_1}{\diffp v} \\
    \frac{\diffp F_2}{\diffp u} & \frac{\diffp F_2}{\diffp v} 
    \end{array} \right| = 
    \left| \begin{array}{c c}
      z & zyv \\
      -zuv^2 & -zu^2v+2x
    \end{array}\right| \overset{(1,1,1)}{=}
    \left| \begin{array}{c c}
      1 & 2 \\
      -2 & 0
    \end{array} \right| = -2-(-4) = 2 \neq 0 \checkmark
  \end{align*}
  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 4: } \text{Ableiten und berrechnen}&
  \end{flalign*}
  \vspace{-0.5cm}
  Gesucht; $\frac{\diffp v}{\diffp y}(1,1,1) \Rightarrow $ nach $y$ ableiten:\newline
  \begin{align*}
    &\;\;\Rightarrow \begin{cases}
     2x^2y +z \frac{\diffp u}{\diffp y} + v^2 + 2yv \frac{\diffp v}{\diffp y} = 0\\
     1 + 2x\frac{\diffp v}{\diffp y}-u^2 2v \frac{\diffp v}{\diffp y} - 2 u v^2 \frac{\diffp u}{\diffp y} = 0
    \end{cases} \\
    &\overset{(1,1,1)}{\Rightarrow} 
    \begin{cases}
      2 + \frac{\diffp u}{\diffp y} + 2y \frac{\diffp v}{\diffp y} + 1 \\
      1 + 2 \frac{\diffp v}{\diffp y} - 2 \frac{\diffp v}{\diffp y} - 2 \frac{\diffp u}{\diffp y} = 0
    \end{cases} 
    \Rightarrow \begin{cases}
      \frac{\diffp u}{\diffp y} + 2 \frac{\diffp v}{\diffp y} = -3 \quad[I] \\
      2\frac{\diffp u}{\diffp y} = 1 \qquad \qquad[II]
    \end{cases}
  \end{align*}
  \vspace{-0.5cm}
  \begin{flalign*}
    &\textbf{Schritt 4.1: } \text{System lösen}&
  \end{flalign*}
  \vspace{-0.5cm}
  \begin{align*}
    [II] \Rightarrow \frac{\diffp u}{\diffp y} = \frac{1}{2} \overset{[I]}{\Rightarrow} \frac{\diffp v}{\diffp y} = \frac{1}{2} (-3 -\frac{1}{2}) = - \frac{7}{4}
  \end{align*}
  
  
  \subsection{Fixpunktsatz}
  \begin{satz}
	  Banachscher Fixpunktsatz: Sei $(M,d)$ ein vollständig metrischer Raum und $F:M\rightarrow M$ eine Kontraktion, d.h. $\exists \kappa \in (0,1)$, sodass 
	  \begin{equation*}
	    d(F(x),F(y)) \leq \kappa d(x,y) \qquad \forall x,y \in M
	  \end{equation*}
	  Dann hat $F$ einen eindeutigen Fixpunkt $x^* \in M$, d.h. $x^*=F(x^*)$.
  \end{satz}
  \begin{bem}
    Das Verfahren konvergiert im Allgemeinen nur linear, d.h. $|y_{n+1} - y_{lim} \leq \tilde{\kappa}|y_n - y_{lim}|$ mit $\tilde{\kappa}$ klein. \newline
    Quadratische Konvergenz mit Newton Verfahren.
  \end{bem}
  
  \subsection{Extremwertaufgaben mit Nebenbedingungen}
  Suche Extremstellen von $f(x,y)$ unter der Nebenbedingung $g(x,y) = 0$.
  D.h. zum Beispiel: Gesucht sind diejenigen Punkte einer Parabel $y = x^2+1$, welche vom Punkt $(1,1)$ den minimalen Abstand haben. Es gilt $f(x,y) = \sqrt{(x-1)^2+(y-1)^2}$ und $g(x,y) = -y+x^2+1 = 0$.\newline
  Das Ziel ist eine Methode zu finden, mit der die Extremstellen berrechnet werden können, ohne vorher $g$ aufzulösen.
  \subsubsection{Lagrangemultiplikatoren}
  Betrachte $F:\R^2 \rightarrow \R$, $g:\R^2 \rightarrow \R$.\newline
  Annahme: Es existiert eine Auflösung $y = h(x)$ mit $g(x,h(x)) = 0$.\newline
  \begin{align*}
    \text{Neues Optimierungsproblem: } F'(x) &= 0 \\
    \Rightarrow F'(x) &= \frac{\diffp f}{\diffp x} + \frac{\diffp f}{\diffp y} h'(x) = 0\\
    \text{Aus } g(x,h(x)) &= 0 \text{ folgt: } \frac{\diffp g}{\diffp x} + \frac{\diffp g}{\diffp y} h'(x) = 0 \\
    \Rightarrow h'(x) &= -\left(\frac{\diffp g}{\diffp y}\right)^{-1} \left(\frac{\diffp g}{\diffp x}\right)\\
    \Rightarrow F'(x) &= 0 = \frac{\diffp f}{\diffp x} - \underbrace{\frac{\diffp f}{\diffp y}\left(\frac{\diffp g}{\diffp y}\right)^{-1}}_{=: \lambda} \left(\frac{\diffp g}{\diffp x}\right) = 0 \qquad (*1)\\
    &\overset{(*1)}{=} \frac{\diffp f}{\diffp x} + \lambda \frac{\diffp g}{\diffp x} = 0\\
    \text{Aus der Definition von } \lambda &= \frac{\diffp f}{\diffp y}\left(\frac{\diffp g}{\diffp y}\right)^{-1} \text{ folgt} \\
    \frac{\diffp f}{\diffp y} + \lambda \frac{\diffp g}{\diffp y} &= 0 \\
    \Rightarrow \text{ notwendige Bedingung für Extremum} \\
    \frac{\diffp H}{\diffp x} &= \frac{\diffp f}{\diffp x} + \lambda \frac{\diffp g}{\diffp x} = 0\\
    \frac{\diffp H}{\diffp y} &= \frac{\diffp f}{\diffp y} + \lambda \frac{\diffp g}{\diffp y} = 0\\
    \frac{\diffp H}{\diffp \lambda} &= g = 0
  \end{align*}
  Wobei $H = f + \lambda g$ die sogenannte Lagrangefunktion ist.
  
  \begin{satz}
    Allgemeiner Fall: Seien $f,g_1,...,g_n : \R^d \rightarrow \R$ stetig diffbar und $Z^*$ sei ein lokales Extremum von $f$ unter den Nebenbedingungen $g_1 = g_2 = ... = g_n = 0$. Weiter sei $Rang\;Jg = n$. Dann existieren $\lambda_1,...,\lambda_n$, sodass $\nabla H(z^*) = 0$ für 
    \begin{equation}
      H = f + \lambda_1g_1 + ... + \lambda_n g_n
    \end{equation}\label{ax:lagrangemult}
  \end{satz}
  \begin{bem}
    Bemerkung zu \eqref{ax:lagrangemult}: Voraussetzungen des Satzes über implizite Funktionen sind erfüllt, denn:
    \begin{itemize}
      \item[V1) ] $g_1(z^*) = g_2(z^*) = ... = g_n(z^*) = 0$
      \item[V2) ] folgt aus $Rang\; jg(z^*) = n$
    \end{itemize}
    Es gibt Koordinaten $z = (x,y)$ mit $Rang \; \frac{\diffp g}{\diffp y} = n$ mit $y \in \R^n$ und $x \in \R^{d-n}$. $Rang \; \frac{\diffp g}{\diffp y} = n$ bedeutet $\frac{\diffp g}{\diffp y} = n$ invertierbar. \newline
    $\Rightarrow \; \exists$ Auflösung $y = y(x)$.
  \end{bem}
  
  \subsection{Kurven und Bogenlängen}
	  \subsubsection{Kurve}
	  \begin{definition}
	    \begin{equation}
	      \R^d: x: t\rightarrow x(t), [a,b] \rightarrow \R^d
	    \end{equation}
	      \begin{itemize}
	        \item[a) ] Eine stetige Funktion $c:[a,b]\rightarrow \R^d$ heißt eine Kurve im $\R^d$. $c(a)$ heißt der Anfangspunkt und $c(b)$ heißt der Endpunkt. Eine Kurve heißt geschlossen, falls $c(a) = c(b)$.
	        \item[b) ] Eine differenzierbare Kurve heißt glatt, falls 
	        \begin{equation*}
	          \dot{c}(t) = \left( \dot{c_1}(t), ..., \dot{c_d}(t)\right) \neq 0
	        \end{equation*}
	        für alle $t \in [a,b]$. $\dot{c}(t)$ heißt der Tangentenvektor.
	      \end{itemize}
	  \end{definition}
	  Beispiele:
	  \begin{itemize}
	    \item[i)  ] $c(t) = (\cos t, \sin t)$ beschreibt einen Kreis mit $t \in [0, 2\pi]$.
	    \item[ii) ] $c(t) = (rt - a \sin t, r - a \cos t)$ ist eine Zykloide.
	    \item[iii)] $c(t) = (\cos t, sin t, t)$ beschreibt eine Schraubenlinie. 
	    \item[iv) ] $c(t) = (\Phi(t), r(t)) = (t, a(1+\cos t))$ beschreibt eine Herzkurve bzw. Kardiode. 
	    \item[v)  ] $c(t) = (a t \cos t, a t \sin t)$ beschreibt eine archimedische Spirale. 
	  \end{itemize}
	  \subsubsection{Länge von Kurven}
	  \begin{definition}
	    Ist die Menge $\lbrace L(Z): Z Zerlegung von [a,b] \rbrace$ nach oben beschränkt, so heißt die Kurve rektifizierbar und $L(c) = \sup \lbrace L(Z): Z Zerlegung von [a,b]\rbrace$ heißt die Länge der Kurve $c$.
	  \end{definition}
	  \begin{bem}
	    Ein Beispiel für eine nicht rektifizierbare Kurve ist eine Kochsche Schneeflocke.
	  \end{bem}
	  \begin{satz}
	    Jede stetige diffbare Kurve ist rektifizierbar und es gilt 
	    \begin{equation}
	      L(c) = \int\limits_a^b ||\dot{c}(t)|| dt
	    \end{equation}
	    Weiter gilt
	    \begin{equation}
	      L(c) \leq \sup\limits_{t \in [a,b]} || \dot{c}(t)|| (b-a)
	    \end{equation}
	  \end{satz}
	  \begin{bem}$\;$\newline
	    \begin{itemize}
	      \item[a) ] Gilt $||\dot{c}(t) = 1 \quad \forall  t \in [a,b]$, so heißt $c$ nach der Bogenlänge parametrisiert (verwende dann s statt t).
	      \item[b) ] Dann gilt, dass $\dot{c}(t) \perp \ddot{c}(t)$
	      \item[c) ] In diesem Fall heißt $\ddot{c}$ die Krümmung.
	    \end{itemize}
	  \end{bem}
	  \begin{bem}
	    Parametrisiert man den Graphen mit $(x, y(x))$, also in der $x-y$ Ebene, so ergibt sich:
	    \begin{equation}
	      L = \int\limits_a^b \sqrt{1 + (y'(x))^2}dx
	    \end{equation}
	    bzw. allgemein im $\R^2$
	    \begin{align*}
	      \text{Kurvenlänge: } \int ds &= \int \sqrt{(dx)^2 + (dy)^2} = \int \sqrt{(dx)^2 \left( 1 + \left(\frac{dy}{dx}\right)^2\right)} \\
	      &= \int\limits_a^b \sqrt{1 + \left(\frac{dy}{dx}\right)^2 }dx = \int\limits_a^b \sqrt{1 + (f'(x))^2}dx
	    \end{align*}
	    \begin{figure}[H] 
				\centering
			  \includegraphics[width=0.4\linewidth]{kurvenlaenge.png}
			  \caption{Kurvenlänge}
			  \label{fig:lkurvenlaenge}
      \end{figure}
	  \end{bem}
	  
	  \subsubsection{Flächen geschlossener ebener Kurven}
	  \begin{equation}
	    F(c) = \frac{1}{2} \int\limits_a^b \big|\big(x(t)\dot{y}(t) - y(t)\dot{x}(t)\big)\big|dt
	  \end{equation}
  \subsection{Wegintegrale}
	  \subsubsection{Wegintegral erster Art}
	  \begin{definition}
	    Das Wegintegral erster Art ist definiert durch:
	    \begin{equation}
	      \int\limits_c \varrho \; dx = \int\limits_x \varrho \; ds = \int\limits_a^b \varrho(c(t)) \quad || \dot{c}(t)|| dt \label{eq:wegintegral_1}
	    \end{equation}
	  \end{definition}
	  \begin{bem}
	    Aus \eqref{fig:wegintegral_erster_2} geht
	    \begin{equation*}
	      ds = \sqrt{dx^2 + dy^2}
	    \end{equation*}
	    hervor. Damit folgt:
	    \begin{align*}
	      ds &= \sqrt{dx^2 + dy^2} = \frac{dt}{dt}  \sqrt{dx^2 + dy^2} = \frac{1}{dt} \sqrt{dx^2 + dy^2}dt \\
	      &=  \sqrt{\frac{1}{dt^2}(dx^2 + dy^2)} =  \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2}
	    \end{align*}
	    Überträgt man das bekannte Integral aus dem $\R^2$, das mit $\int\limits_a^b f(x) dx$ gegeben ist, und  obigen Zusammenhang ein, so erhält man:
	    \begin{align*}
	      \int\limits_{t=a}^{t=b} f(x,y) ds = \int\limits_{t=a}^{t=b} f(x,y) \sqrt{dx^2 + dy^2}\\
	      = \int\limits_{t=a}^{t=b} \underbrace{f(x(t),y(t))}_{\text{Höhe}} \underbrace{\sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right))^2} dt}_{ds}
	    \end{align*}
	  \end{bem}
	  \begin{figure}[H] 
		\centering
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.9\linewidth]{wegintegral_erster_1.png}
		  \caption{Graphische Interpretation}
		  \label{fig:wegintegral_erster_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
		  \centering
		  \includegraphics[width=0.8\linewidth]{wegintegral_erster_2.png}
		  \caption{Interpretation von \protect\eqref{eq:wegintegral_1}}
		  \label{fig:wegintegral_erster_2}
		\end{minipage}
		\end{figure}
		\begin{bem}
			\begin{itemize}
			  \item[a) ] Integrale sind unabhängig von der gewählten Parametrisierung.
			  \item[b) ] Falls $c$ geschlossen ist so schreibt man 
				  \begin{equation}
				    \oint\limits_c \varrho \; ds
				  \end{equation}
			\end{itemize}
		\end{bem}
		\subsubsection{Wegintegrale zweiter Art}
		\begin{definition}
		  Sei $f: D\rightarrow \R^d$ ein stetiges Vektorfreld mit $D\subset \R^d$ und sei $c:[a,b] \rightarrow D$ eine stückweise $C^1$-Kurve, dann heißt 
		  \begin{equation}
		    \int\limits_c <f(x), dx> = \int\limits_a^b <f\left(c(t)\right), \dot{c}(t)>dt
		  \end{equation}
		  das Wegintegral 2-ter Art. Falls $c$ geschlossen ist schreibt man
		  \begin{equation}
		    \oint\limits_c <f(x), dx>
		  \end{equation}
		\end{definition}
		\begin{bem}
		  Das Wegintegral ist unabhängig von der gewählten Parametrisierung.
		\end{bem}
		\begin{bem}
		  Eine Alternative ältere Schreibweise ist
		  \begin{equation*}
		    \int\limits_c <f(X),dX>
		  \end{equation*}
		  Achtung, es handelt sich nur um eine Schreibweise. Nicht das Skalarprodukt aus $f(X)$ und $dX$ bilden!
		\end{bem}
		\begin{definition}
  		Ein stetiges Vektorfeld $f$ heißt wirbelfrei, falls die Kurvenintegrale längs aller stückweise stetig diffbaren Kurven verschwinden, d.h.
  		\begin{equation}
  		  \oint\limits_c <f(x), dx> = 0
  		\end{equation}
  		gilt.
		\end{definition}
		Als Konsequenz daraus folgt die Wegunabhängigkeit der Kurvenintegrale für den Fall das $f$ wirbelfrei ist. Das heißt, ist $f$ wirbelfrei, so gilt:
		\begin{equation}
		  \int\limits_{c_1} <f(x),dx> = \int\limits_{c_2} <f(x),dx>
		\end{equation}
		für beliebige Wege $c_1$ und $c_2$ mit gleichen Anfangs- und Endpunkten.
		\begin{definition}
		  Eine Teilmenge $D \subset \R^d$ heißt (weg)-zusammenhängend, falls je zwei Punkte $x,y \in D$ durch eine stückweise $C^1$-Kurve in $D$ verbunden werden können.
		  \begin{figure}[H] 
			  \centering
			  \includegraphics[width=0.8\linewidth]{zusammenhaengend.png}
			  \caption{Visualisierung zusammenhängend \protect\cite{HM12}}
			  \label{fig:zusammenhängend}
		  \end{figure}
		\end{definition}
		\subsubsection{Potential}
	  \begin{definition}
	    Sei $f:D\rightarrow \R^d$ ein Vektorfeld auf $D\subset \R^d$. Wir sagen $f$ ist ein gradientenfeld, falls es eine skalare $C^1$-Funktion $\varphi: D\rightarrow\R$ gibt, mit 
	    \begin{equation}
	    \nabla \varphi(x) = f(x)
	    \end{equation}
	    $\varphi$ heißt das Potential von $f$.
	  \end{definition}
    \begin{satz}
      Sei $D \subset \R^n$ offen und zusammenhängend und $f$ ein stetiges Vektorfeld auf $D$. 
      \begin{itemize}
        \item[a) ] Besitzt $f$ ein Potential $\varphi$, so gilt für alle stückweisen $C^1$-Kurven $c$, dass 
        \begin{equation}
          \int\limits_c <f(x),dx> = \varphi(c(b)) - \varphi(c(a))
        \end{equation}
        wobei $c:[a,b]\rightarrow \R^d$. D.h. das Wegintegral ist damit wegunabhängig und $f$ wirbelfrei.
        \item[b) ] Ist $f$ wirbelfrei, so besitzt $f$ ein Potential $\varphi$ mit der Darstellung 
        \begin{equation}
          \varphi(x) = \int\limits_{c_x} f(\tilde{x}),d\tilde{x}>
        \end{equation}
        wobei $c_x$ ein Weg nach $x$ mit fest gewähltem Startpunkt $x^*$ sein soll.
      \end{itemize}
    \end{satz}	  
	  \subsubsubsection{Berechnung von Potentialen}
	  Die notwendige (aber nicht hinreichende) Bedingung für die Existenz eines Potentials ist:
	  \begin{equation}
	    rot(\nabla \varphi) = 0 \Rightarrow rot(f) = 0 \Rightarrow Potential\;ex.  
	  \end{equation}
	  \begin{definition}
	    Ein Gebiet $G$ heißt einfach zusammenhängend, falls jeder geschlossene Weg in $G$ auf einen Punkt im Gebiet zusammen gezogen werden kann.
	    \begin{figure}[H] 
			  \centering
			  \includegraphics[width=0.8\linewidth]{einfach_zusammenhaengend.png}
			  \caption{Visualisierung einfach zusammenhängend \protect\cite{HM12}}
			  \label{fig:einfach_zusammenhängend}
		  \end{figure}
	  \end{definition}
	  
  \newpage
\section{Nachwort}
Dieses Dokument versteht sich einzig als Zusammenfassung der Vorlesungsunterlagen aus der HM1-2 Vorlesung von Prof. Dr. Guido Schneider mit einigen zusätzlichen Beispielen. Der Sinn ist einzig mir selbst und meinen Kommilitonen das studieren der Mathematik zu erleichtern. In diesem Sinne erhebe ich keinerlei Anspruch auf das hier dargestellte Wissen, da es sich in großen Teilen nur um Neuformulierungen aus der Vorlesung und aus dem Begleitkurs vom Mint Kolleg handelt, in dem Frau Dr. Monika Schulz den Stoff bereits hervorragend zusammengefasst hat. Sollten sich einige Fehler eingeschlichen haben (was sehr wahrscheinlich ist) würde ich mich freuen, wenn man mich per Email (f.leuze@outlook.de) kontaktieren und entsprechende Fehler mitteilen würde.
\bibliography{lit}

\end{document}